{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f75ab403",
   "metadata": {},
   "source": [
    "# User data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eafd6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>treatment</th>\n",
       "      <th>source</th>\n",
       "      <th>ops_type_merged</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-12-22</td>\n",
       "      <td>5145040</td>\n",
       "      <td>15x2元1張</td>\n",
       "      <td>隨機組</td>\n",
       "      <td>14天在其他尖峰預估車資</td>\n",
       "      <td>臺北市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>302812</td>\n",
       "      <td>15x2元1張</td>\n",
       "      <td>隨機組</td>\n",
       "      <td>14天在其他尖峰預估車資</td>\n",
       "      <td>新北市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-12-01</td>\n",
       "      <td>4375821</td>\n",
       "      <td>15x2元1張</td>\n",
       "      <td>隨機組</td>\n",
       "      <td>14天在其他尖峰預估車資</td>\n",
       "      <td>新北市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-24</td>\n",
       "      <td>2273154</td>\n",
       "      <td>15x2元1張</td>\n",
       "      <td>隨機組</td>\n",
       "      <td>14天在其他尖峰預估車資</td>\n",
       "      <td>臺北市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-12-22</td>\n",
       "      <td>433188</td>\n",
       "      <td>15x2元1張</td>\n",
       "      <td>隨機組</td>\n",
       "      <td>14天在其他尖峰預估車資</td>\n",
       "      <td>臺北市</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  experiment_date  user_id treatment source ops_type_merged city\n",
       "0      2025-12-22  5145040   15x2元1張    隨機組    14天在其他尖峰預估車資  臺北市\n",
       "1      2025-11-17   302812   15x2元1張    隨機組    14天在其他尖峰預估車資  新北市\n",
       "2      2025-12-01  4375821   15x2元1張    隨機組    14天在其他尖峰預估車資  新北市\n",
       "3      2025-11-24  2273154   15x2元1張    隨機組    14天在其他尖峰預估車資  臺北市\n",
       "4      2025-12-22   433188   15x2元1張    隨機組    14天在其他尖峰預估車資  臺北市"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "user = pd.read_csv(\"../../cleaned_data/user_cleaned.csv\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "user.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9660d6",
   "metadata": {},
   "source": [
    "# Trip data\n",
    "## RDS trip (nonrepeat_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a843d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[chunk 0] kept_rows=981,933 groups=2,972\n",
      "[chunk 1] kept_rows=1,684,483 groups=16,252\n",
      "[chunk 2] kept_rows=1,651,557 groups=29,406\n",
      "[chunk 3] kept_rows=210,530 groups=8,091\n",
      "Saved: ../data/cleaned_v1.csv rows: 44969\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------\n",
    "# Config (改這裡)\n",
    "# -----------------------\n",
    "USER_PATH = \"../../cleaned_data/user_cleaned.csv\"\n",
    "TRIP_PATH = \"../../merged_data/RDS_trip_merged.csv\"\n",
    "TEST_TRIP_PATH = \"../../data/test_trip.csv\"\n",
    "OUT_PATH = \"../data/cleaned_v1.csv\"\n",
    "\n",
    "START_DATE = pd.Timestamp(\"2025-07-28\")\n",
    "END_DATE   = pd.Timestamp(\"2026-01-11\")  # 含 1/11\n",
    "CHUNKSIZE = 2_000_000  # 依你機器記憶體調整：50萬~300萬都常見\n",
    "\n",
    "# -----------------------\n",
    "# Helper\n",
    "# -----------------------\n",
    "def week_monday(d: pd.Series) -> pd.Series:\n",
    "    \"\"\"把日期對齊到週一作為 experiment_date（週起始=週一）\"\"\"\n",
    "    return d - pd.to_timedelta(d.dt.weekday, unit=\"D\")\n",
    "\n",
    "# -----------------------\n",
    "# 1) Load user_cleaned (週 -> 類別)\n",
    "# -----------------------\n",
    "user = pd.read_csv(\n",
    "    USER_PATH,\n",
    "    usecols=[\"experiment_date\", \"user_id\", \"treatment\", \"source\", \"ops_type_merged\", \"city\"],\n",
    "    dtype={\"user_id\": \"int64\"},\n",
    ")\n",
    "user[\"experiment_date\"] = pd.to_datetime(user[\"experiment_date\"])\n",
    "# 建索引方便 merge（其實 merge 已夠用，但保持乾淨）\n",
    "user = user.drop_duplicates([\"experiment_date\", \"user_id\"])\n",
    "\n",
    "# -----------------------\n",
    "# 2) Load test_trip ids (排除清單)\n",
    "# -----------------------\n",
    "test_ids = pd.read_csv(TEST_TRIP_PATH, usecols=[\"trip_id\"])\n",
    "# 用 Index 省記憶體 + isin 也快\n",
    "test_id_index = pd.Index(test_ids[\"trip_id\"].astype(\"int64\", errors=\"ignore\"))\n",
    "\n",
    "# -----------------------\n",
    "# 3) Chunk read RDS_trip and aggregate\n",
    "# -----------------------\n",
    "agg_parts = []  # 也可以改成 dict 累加；先用 concat 再 groupby 方式較直覺\n",
    "\n",
    "usecols = [\"trip_id\", \"user_id\", \"year\", \"month\", \"day\", \"duplicate_id\"]\n",
    "dtypes = {\n",
    "    \"trip_id\": \"int64\",\n",
    "    \"user_id\": \"int64\",\n",
    "    \"year\": \"int16\",\n",
    "    \"month\": \"int8\",\n",
    "    \"day\": \"int8\",\n",
    "    \"duplicate_id\": \"int8\",\n",
    "}\n",
    "\n",
    "# END_DATE 要含當天，所以用 < END_DATE+1\n",
    "end_exclusive = END_DATE + pd.Timedelta(days=1)\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(TRIP_PATH, usecols=usecols, dtype=dtypes, chunksize=CHUNKSIZE)):\n",
    "    # 組出日期（用 year/month/day 避免解析 request_time）\n",
    "    chunk[\"day_date\"] = pd.to_datetime(\n",
    "        dict(year=chunk[\"year\"], month=chunk[\"month\"], day=chunk[\"day\"]),\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "    # 篩日期區間\n",
    "    chunk = chunk[(chunk[\"day_date\"] >= START_DATE) & (chunk[\"day_date\"] < end_exclusive)]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # 條件：duplicate_id = 0\n",
    "    chunk = chunk[chunk[\"duplicate_id\"].fillna(1).astype(\"int64\") == 0]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # 條件：trip_id 不在 test_trip\n",
    "    # （注意：如果 trip_id 有缺失或型別怪，先 coerce）\n",
    "    chunk = chunk[~chunk[\"trip_id\"].isin(test_id_index)]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # 轉成週（週一）\n",
    "    chunk[\"experiment_date\"] = week_monday(chunk[\"day_date\"])\n",
    "\n",
    "    # merge 出類別\n",
    "    merged = chunk.merge(\n",
    "        user,\n",
    "        on=[\"experiment_date\", \"user_id\"],\n",
    "        how=\"left\",\n",
    "        validate=\"many_to_one\",  # 一個 user 在一個 experiment_date 應該只有一筆類別\n",
    "    )\n",
    "\n",
    "    # groupby 計數 trip_id（每日每組 nonrepeat_cnt）\n",
    "    g = (\n",
    "        merged.groupby([\"day_date\", \"treatment\", \"source\", \"ops_type_merged\", \"city\"], dropna=False)[\"trip_id\"]\n",
    "        .nunique()   # trip_id 通常唯一；nunique 更保險\n",
    "        .reset_index(name=\"nonrepeat_cnt\")\n",
    "    )\n",
    "    agg_parts.append(g)\n",
    "\n",
    "    print(f\"[chunk {i}] kept_rows={len(merged):,} groups={len(g):,}\")\n",
    "\n",
    "# -----------------------\n",
    "# 4) Final aggregate (合併各 chunk 結果)\n",
    "# -----------------------\n",
    "if len(agg_parts) == 0:\n",
    "    out = pd.DataFrame(columns=[\"day\", \"treatment\", \"source\", \"ops_type_merged\", \"city\", \"nonrepeat_cnt\"])\n",
    "else:\n",
    "    out = pd.concat(agg_parts, ignore_index=True)\n",
    "    out = (\n",
    "        out.groupby([\"day_date\", \"treatment\", \"source\", \"ops_type_merged\", \"city\"], dropna=False)[\"nonrepeat_cnt\"]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"day_date\": \"day\"})\n",
    "        .sort_values([\"day\", \"treatment\", \"source\", \"ops_type_merged\", \"city\"])\n",
    "    )\n",
    "\n",
    "out.to_csv(OUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved:\", OUT_PATH, \"rows:\", len(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c53e39e",
   "metadata": {},
   "source": [
    "## trip (is_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d561ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------\n",
    "# Config（改路徑）\n",
    "# -----------------------\n",
    "USER_PATH = \"../../cleaned_data/user_cleaned.csv\"\n",
    "TRIP_MERGED_PATH = \"../../merged_data/trip_merged.csv\"\n",
    "RDS_TRIP_MERGED_PATH = \"../../merged_data/RDS_trip_merged.csv\"  # 用來取 duplicate_id\n",
    "TEST_TRIP_PATH = \"../../data/test_trip.csv\"\n",
    "\n",
    "OUT_PATH = \"../data/cleaned_v2.csv\"\n",
    "TMP_PATH = \"_tmp_filtered_trip_merged.csv\"  # 暫存 Step A 篩完的最小資料\n",
    "\n",
    "START_DATE = pd.Timestamp(\"2025-07-28\")\n",
    "END_DATE   = pd.Timestamp(\"2026-01-11\")  # 含 1/11\n",
    "REQ_LOOKBACK_DAYS = 60\n",
    "\n",
    "CHUNKSIZE = 2_000_000\n",
    "TZ = \"Asia/Taipei\"\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def to_taipei(ts: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(ts, utc=True, errors=\"coerce\").dt.tz_convert(TZ)\n",
    "\n",
    "def week_monday(d: pd.Series) -> pd.Series:\n",
    "    return d - pd.to_timedelta(d.dt.weekday, unit=\"D\")\n",
    "\n",
    "def is_nonempty_str(s: pd.Series) -> pd.Series:\n",
    "    ss = s.astype(\"string\")\n",
    "    return ss.notna() & (ss.str.strip() != \"\")\n",
    "\n",
    "# ---- City group mapping（三區）\n",
    "north = {'臺北市','新北市','基隆市','桃園市','宜蘭縣','花蓮縣','新竹縣','新竹市'}\n",
    "central = {'苗栗縣','臺中市','南投縣','彰化縣','雲林縣','嘉義縣','嘉義市'}\n",
    "south = {'臺南市','高雄市','屏東縣','臺東縣','澎湖縣','金門縣','連江縣'}\n",
    "\n",
    "def map_city_group(city):\n",
    "    if pd.isna(city) or city == \"\":\n",
    "        city = \"臺北市\"\n",
    "    if city in north:\n",
    "        return \"北區\"\n",
    "    if city in central:\n",
    "        return \"中區\"\n",
    "    if city in south:\n",
    "        return \"南區\"\n",
    "    return \"北區\"\n",
    "\n",
    "# -----------------------\n",
    "# 0) 時間窗（台北時區）\n",
    "# -----------------------\n",
    "reserve_start = pd.Timestamp(START_DATE.date(), tz=TZ)\n",
    "reserve_end_excl = pd.Timestamp((END_DATE + pd.Timedelta(days=1)).date(), tz=TZ)\n",
    "req_min = pd.Timestamp((START_DATE - pd.Timedelta(days=REQ_LOOKBACK_DAYS)).date(), tz=TZ)\n",
    "\n",
    "# 針對「最後輸出」的 day 下限（拿掉 7/28 以前）\n",
    "DAY_MIN = pd.Timestamp(\"2025-07-28\")\n",
    "\n",
    "# -----------------------\n",
    "# 1) Load user_cleaned（週 -> 類別）\n",
    "# -----------------------\n",
    "user = pd.read_csv(\n",
    "    USER_PATH,\n",
    "    usecols=[\"experiment_date\", \"user_id\", \"treatment\", \"source\", \"ops_type_merged\", \"city\"],\n",
    "    dtype={\"user_id\": \"int64\"},\n",
    ")\n",
    "user[\"experiment_date\"] = pd.to_datetime(user[\"experiment_date\"])\n",
    "user = user.drop_duplicates([\"experiment_date\", \"user_id\"])\n",
    "\n",
    "# （可選）省記憶體\n",
    "for c in [\"treatment\", \"source\", \"ops_type_merged\", \"city\"]:\n",
    "    user[c] = user[c].astype(\"category\")\n",
    "\n",
    "# -----------------------\n",
    "# 2) Load test_trip ids（排除）\n",
    "# -----------------------\n",
    "test_ids = pd.read_csv(TEST_TRIP_PATH, usecols=[\"trip_id\"])\n",
    "test_ids[\"trip_id\"] = pd.to_numeric(test_ids[\"trip_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "exclude_trip = set(test_ids[\"trip_id\"].dropna().astype(\"int64\").tolist())\n",
    "\n",
    "# -----------------------\n",
    "# Step A) 掃 trip_merged.csv：時間條件 + 排除 test_trip，存最小暫存檔\n",
    "# -----------------------\n",
    "if os.path.exists(TMP_PATH):\n",
    "    os.remove(TMP_PATH)\n",
    "\n",
    "usecols_a = [\"trip_id\", \"user_id\", \"driver_id\", \"request_time\", \"reserve_time\"]\n",
    "dtype_a = {\"trip_id\": \"int64\", \"user_id\": \"int64\", \"driver_id\": \"string\"}\n",
    "\n",
    "kept_trip_ids = set()\n",
    "\n",
    "for i, ch in enumerate(pd.read_csv(TRIP_MERGED_PATH, usecols=usecols_a, dtype=dtype_a, chunksize=CHUNKSIZE)):\n",
    "    # 排除 test_trip（先做，最快降量）\n",
    "    ch = ch[~ch[\"trip_id\"].isin(exclude_trip)]\n",
    "    if ch.empty:\n",
    "        continue\n",
    "\n",
    "    # 轉台北時區\n",
    "    ch[\"request_local\"] = to_taipei(ch[\"request_time\"])\n",
    "    ch[\"reserve_local\"] = to_taipei(ch[\"reserve_time\"])\n",
    "    ch = ch.dropna(subset=[\"request_local\", \"reserve_local\"])\n",
    "    if ch.empty:\n",
    "        continue\n",
    "\n",
    "    # request_time >= START-60d\n",
    "    ch = ch[ch[\"request_local\"] >= req_min]\n",
    "    if ch.empty:\n",
    "        continue\n",
    "\n",
    "    # reserve_time in [START, END]\n",
    "    ch = ch[(ch[\"reserve_local\"] >= reserve_start) & (ch[\"reserve_local\"] < reserve_end_excl)]\n",
    "    if ch.empty:\n",
    "        continue\n",
    "\n",
    "    # day = request_time(台北) 的日期\n",
    "    ch[\"day\"] = ch[\"request_local\"].dt.floor(\"D\")\n",
    "\n",
    "    # experiment_date（週一），用 day 的 naive 算週\n",
    "    day_naive = ch[\"day\"].dt.tz_localize(None)\n",
    "    ch[\"experiment_date\"] = week_monday(day_naive)\n",
    "\n",
    "    # 只留最小欄位\n",
    "    out_ch = ch[[\"trip_id\", \"user_id\", \"driver_id\", \"day\", \"experiment_date\"]].copy()\n",
    "\n",
    "    # append 寫入暫存\n",
    "    out_ch.to_csv(TMP_PATH, mode=\"a\", header=not os.path.exists(TMP_PATH), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    kept_trip_ids.update(out_ch[\"trip_id\"].unique().tolist())\n",
    "    print(f\"[A chunk {i}] kept_rows={len(out_ch):,} unique_trip_ids={len(kept_trip_ids):,}\")\n",
    "\n",
    "print(\"Step A done. filtered trips:\", len(kept_trip_ids))\n",
    "\n",
    "# -----------------------\n",
    "# Step B) 掃 RDS_trip_merged.csv：建立 trip_id -> duplicate_id mapping\n",
    "# -----------------------\n",
    "dup_map = {}\n",
    "\n",
    "usecols_b = [\"trip_id\", \"duplicate_id\"]\n",
    "dtype_b = {\"trip_id\": \"int64\", \"duplicate_id\": \"int8\"}\n",
    "\n",
    "for j, ch in enumerate(pd.read_csv(RDS_TRIP_MERGED_PATH, usecols=usecols_b, dtype=dtype_b, chunksize=CHUNKSIZE)):\n",
    "    ch = ch[ch[\"trip_id\"].isin(kept_trip_ids)]\n",
    "    if ch.empty:\n",
    "        continue\n",
    "\n",
    "    for tid, did in zip(ch[\"trip_id\"].tolist(), ch[\"duplicate_id\"].tolist()):\n",
    "        if tid not in dup_map:\n",
    "            dup_map[tid] = int(did)\n",
    "\n",
    "    print(f\"[B chunk {j}] found={len(ch):,} mapped={len(dup_map):,}\")\n",
    "\n",
    "print(\"Step B done. dup_map size:\", len(dup_map))\n",
    "\n",
    "# -----------------------\n",
    "# Step C) 讀暫存檔：補 duplicate_id -> filter duplicate_id=0 -> merge user -> 算媒合率\n",
    "# -----------------------\n",
    "parts = []\n",
    "\n",
    "usecols_c = [\"trip_id\", \"user_id\", \"driver_id\", \"day\", \"experiment_date\"]\n",
    "dtype_c = {\"trip_id\": \"int64\", \"user_id\": \"int64\", \"driver_id\": \"string\", \"day\": \"string\", \"experiment_date\": \"string\"}\n",
    "\n",
    "for k, ch in enumerate(pd.read_csv(TMP_PATH, usecols=usecols_c, dtype=dtype_c, chunksize=CHUNKSIZE)):\n",
    "\n",
    "    # day：統一解析成 UTC 再轉台北，最後取當日 00:00\n",
    "    ch[\"day\"] = pd.to_datetime(ch[\"day\"], errors=\"coerce\", utc=True).dt.tz_convert(TZ).dt.floor(\"D\")\n",
    "\n",
    "    # experiment_date：naive datetime\n",
    "    ch[\"experiment_date\"] = pd.to_datetime(ch[\"experiment_date\"], errors=\"coerce\")\n",
    "\n",
    "    ch = ch.dropna(subset=[\"day\", \"experiment_date\"])\n",
    "    if ch.empty:\n",
    "        continue\n",
    "\n",
    "    # duplicate_id\n",
    "    ch[\"duplicate_id\"] = ch[\"trip_id\"].map(dup_map)\n",
    "    ch = ch.dropna(subset=[\"duplicate_id\"])\n",
    "    ch = ch[ch[\"duplicate_id\"].astype(\"int64\") == 0]\n",
    "    if ch.empty:\n",
    "        continue\n",
    "\n",
    "    # merge 類別\n",
    "    m = ch.merge(user, on=[\"user_id\", \"experiment_date\"], how=\"left\", validate=\"many_to_one\")\n",
    "\n",
    "    # success\n",
    "    m[\"is_success\"] = is_nonempty_str(m[\"driver_id\"])\n",
    "\n",
    "    # 城市分區（三區）\n",
    "    m[\"city_group\"] = m[\"city\"].map(map_city_group)\n",
    "\n",
    "    # groupby 計數\n",
    "    g = (\n",
    "        m.groupby([\"day\", \"treatment\", \"source\", \"ops_type_merged\", \"city_group\"], dropna=False)\n",
    "         .agg(total_cnt=(\"trip_id\", \"size\"), success_cnt=(\"is_success\", \"sum\"))\n",
    "         .reset_index()\n",
    "    )\n",
    "    parts.append(g)\n",
    "    print(f\"[C chunk {k}] kept_rows={len(m):,} groups={len(g):,}\")\n",
    "\n",
    "# -----------------------\n",
    "# Final aggregate & compute rate\n",
    "# -----------------------\n",
    "if not parts:\n",
    "    out = pd.DataFrame(columns=[\"day\", \"treatment\", \"source\", \"ops_type_merged\", \"city_group\", \"is_match_rate\"])\n",
    "else:\n",
    "    out = pd.concat(parts, ignore_index=True)\n",
    "    out = (\n",
    "        out.groupby([\"day\", \"treatment\", \"source\", \"ops_type_merged\", \"city_group\"], dropna=False)[[\"total_cnt\", \"success_cnt\"]]\n",
    "           .sum()\n",
    "           .reset_index()\n",
    "    )\n",
    "    out[\"is_match_rate\"] = np.where(out[\"total_cnt\"] > 0, out[\"success_cnt\"] / out[\"total_cnt\"], np.nan)\n",
    "\n",
    "    # 轉 day 成 YYYY-MM-DD 字串\n",
    "    out[\"day\"] = pd.to_datetime(out[\"day\"], utc=True, errors=\"coerce\").dt.tz_convert(TZ).dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # 只保留輸出欄位\n",
    "    out = out[[\"day\", \"treatment\", \"source\", \"ops_type_merged\", \"city_group\", \"is_match_rate\"]].copy()\n",
    "\n",
    "# -----------------------\n",
    "# FINAL FILTER BEFORE SAVING（主動把不合條件的拿掉 + 拿掉 7/28 以前）\n",
    "# -----------------------\n",
    "# 乾淨字串\n",
    "for c in [\"day\", \"treatment\", \"source\", \"ops_type_merged\", \"city_group\"]:\n",
    "    out[c] = out[c].astype(\"string\").str.strip()\n",
    "\n",
    "# (4) keys 不可為空\n",
    "key_ok = (\n",
    "    out[\"day\"].notna() & (out[\"day\"] != \"\") &\n",
    "    out[\"treatment\"].notna() & (out[\"treatment\"] != \"\") &\n",
    "    out[\"source\"].notna() & (out[\"source\"] != \"\") &\n",
    "    out[\"ops_type_merged\"].notna() & (out[\"ops_type_merged\"] != \"\") &\n",
    "    out[\"city_group\"].notna() & (out[\"city_group\"] != \"\")\n",
    ")\n",
    "\n",
    "# 拿掉 2025-07-28 以前\n",
    "day_dt = pd.to_datetime(out[\"day\"], errors=\"coerce\")\n",
    "rule_day_min = day_dt >= DAY_MIN\n",
    "\n",
    "src = out[\"source\"]\n",
    "trt = out[\"treatment\"]\n",
    "\n",
    "# 用 contains 比較穩（避免 \"控制組 \" 或 \"隨機組-xxx\"）\n",
    "is_control = src.str.contains(\"控制\", na=False)\n",
    "is_random  = src.str.contains(\"隨機\", na=False)\n",
    "\n",
    "# (1) 控制組 -> treatment 必須是 不發\n",
    "rule1 = (~is_control) | (trt == \"不發\")\n",
    "\n",
    "# (2) 隨機組 -> treatment 一定不能是 不發\n",
    "rule2 = (~is_random) | (trt != \"不發\")\n",
    "\n",
    "# (3) 隨機組日期門檻\n",
    "has_x2 = trt.str.contains(\"x2元\", na=False)\n",
    "cut_no_x2 = pd.Timestamp(\"2025-10-06\")\n",
    "cut_has_x2 = pd.Timestamp(\"2025-11-03\")\n",
    "\n",
    "rule3 = (\n",
    "    (~is_random) |\n",
    "    ((~has_x2) & (day_dt >= cut_no_x2)) |\n",
    "    (has_x2 & (day_dt >= cut_has_x2))\n",
    ")\n",
    "\n",
    "out = out[key_ok & rule_day_min & rule1 & rule2 & rule3].copy()\n",
    "\n",
    "# 排序 & 輸出\n",
    "out = out.sort_values([\"day\", \"treatment\", \"source\", \"ops_type_merged\", \"city_group\"]).reset_index(drop=True)\n",
    "out.to_csv(OUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Saved:\", OUT_PATH, \"rows:\", len(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b3d548",
   "metadata": {},
   "source": [
    "## trip_label (trip_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07bf0352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[chunk 0] kept_rows=1,578,262 groups=10,849\n",
      "[chunk 1] kept_rows=1,864,410 groups=31,181\n",
      "[chunk 2] kept_rows=61,235 groups=2,858\n",
      "Saved: ../data/cleaned_v3.csv rows: 39334\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------\n",
    "# Config (改這裡)\n",
    "# -----------------------\n",
    "USER_PATH = \"../../cleaned_data/user_cleaned.csv\"\n",
    "TRIP_LABEL_PATH = \"../../merged_data/trip_label_merged.csv\"\n",
    "OUT_PATH = \"../data/cleaned_v3.csv\"\n",
    "\n",
    "START_DATE = pd.Timestamp(\"2025-07-28\")\n",
    "END_DATE   = pd.Timestamp(\"2026-01-11\")  # 含 1/11\n",
    "CHUNKSIZE = 2_000_000\n",
    "\n",
    "# -----------------------\n",
    "# Helper\n",
    "# -----------------------\n",
    "def week_monday(d: pd.Series) -> pd.Series:\n",
    "    \"\"\"把日期對齊到週一作為 experiment_date（週起始=週一）\"\"\"\n",
    "    return d - pd.to_timedelta(d.dt.weekday, unit=\"D\")\n",
    "\n",
    "# -----------------------\n",
    "# 1) Load user_cleaned (週 -> 類別)\n",
    "# -----------------------\n",
    "user = pd.read_csv(\n",
    "    USER_PATH,\n",
    "    usecols=[\"experiment_date\", \"user_id\", \"treatment\", \"source\", \"ops_type_merged\", \"city\"],\n",
    "    dtype={\"user_id\": \"int64\"},\n",
    ")\n",
    "user[\"experiment_date\"] = pd.to_datetime(user[\"experiment_date\"], errors=\"coerce\")\n",
    "user = user.dropna(subset=[\"experiment_date\"])\n",
    "user = user.drop_duplicates([\"experiment_date\", \"user_id\"])\n",
    "\n",
    "# -----------------------\n",
    "# 2) Chunk read trip_label_merged and aggregate trip_cnt\n",
    "# -----------------------\n",
    "agg_parts = []\n",
    "\n",
    "usecols = [\"trip_id\", \"user_id\", \"trip_date\"]\n",
    "dtypes = {\"trip_id\": \"int64\", \"user_id\": \"int64\"}\n",
    "\n",
    "end_exclusive = END_DATE + pd.Timedelta(days=1)\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(TRIP_LABEL_PATH, usecols=usecols, dtype=dtypes, chunksize=CHUNKSIZE)):\n",
    "    # trip_date -> day_date\n",
    "    chunk[\"day_date\"] = pd.to_datetime(chunk[\"trip_date\"], errors=\"coerce\")\n",
    "    chunk = chunk.dropna(subset=[\"day_date\"])\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # 篩日期區間\n",
    "    chunk = chunk[(chunk[\"day_date\"] >= START_DATE) & (chunk[\"day_date\"] < end_exclusive)]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # 轉成週（週一）用來 merge user_cleaned\n",
    "    chunk[\"experiment_date\"] = week_monday(chunk[\"day_date\"])\n",
    "\n",
    "    # merge 出類別\n",
    "    merged = chunk.merge(\n",
    "        user,\n",
    "        on=[\"experiment_date\", \"user_id\"],\n",
    "        how=\"left\",\n",
    "        validate=\"many_to_one\",\n",
    "    )\n",
    "\n",
    "    # groupby 計數 trip_id（每日每組 trip_cnt）\n",
    "    g = (\n",
    "        merged.groupby([\"day_date\", \"treatment\", \"source\", \"ops_type_merged\", \"city\"], dropna=False)[\"trip_id\"]\n",
    "        .nunique()  # trip_id 通常唯一；nunique 更保險\n",
    "        .reset_index(name=\"trip_cnt\")\n",
    "    )\n",
    "    agg_parts.append(g)\n",
    "\n",
    "    print(f\"[chunk {i}] kept_rows={len(merged):,} groups={len(g):,}\")\n",
    "\n",
    "# -----------------------\n",
    "# 3) Final aggregate (合併各 chunk 結果)\n",
    "# -----------------------\n",
    "if len(agg_parts) == 0:\n",
    "    out = pd.DataFrame(columns=[\"day\", \"treatment\", \"source\", \"ops_type_merged\", \"city\", \"trip_cnt\"])\n",
    "else:\n",
    "    out = pd.concat(agg_parts, ignore_index=True)\n",
    "    out = (\n",
    "        out.groupby([\"day_date\", \"treatment\", \"source\", \"ops_type_merged\", \"city\"], dropna=False)[\"trip_cnt\"]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"day_date\": \"day\"})\n",
    "        .sort_values([\"day\", \"treatment\", \"source\", \"ops_type_merged\", \"city\"])\n",
    "    )\n",
    "\n",
    "out.to_csv(OUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved:\", OUT_PATH, \"rows:\", len(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba324ae8",
   "metadata": {},
   "source": [
    "## merged trip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1b8cba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/daily_trip_merged.csv rows: 20160\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------\n",
    "# Config (改路徑)\n",
    "# -----------------------\n",
    "V1_PATH = \"../data/cleaned_v1.csv\"  # nonrepeat_cnt (city)\n",
    "V2_PATH = \"../data/cleaned_v2.csv\"  # is_match_rate (city_group)\n",
    "V3_PATH = \"../data/cleaned_v3.csv\"  # trip_cnt (city)\n",
    "\n",
    "OUT_PATH = \"../data/daily_trip_merged.csv\"\n",
    "\n",
    "# -----------------------\n",
    "# City -> City_group mapping（三區）\n",
    "# -----------------------\n",
    "north = {'臺北市','新北市','基隆市','桃園市','宜蘭縣','花蓮縣','新竹縣','新竹市'}\n",
    "central = {'苗栗縣','臺中市','南投縣','彰化縣','雲林縣','嘉義縣','嘉義市'}\n",
    "south = {'臺南市','高雄市','屏東縣','臺東縣','澎湖縣','金門縣','連江縣'}\n",
    "\n",
    "def map_city_group(city):\n",
    "    if pd.isna(city) or city == \"\":\n",
    "        city = \"臺北市\"\n",
    "    if city in north:\n",
    "        return \"北區\"\n",
    "    if city in central:\n",
    "        return \"中區\"\n",
    "    if city in south:\n",
    "        return \"南區\"\n",
    "    return \"北區\"\n",
    "\n",
    "def strip_cols(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(\"string\").str.strip()\n",
    "    return df\n",
    "\n",
    "def nonempty(s: pd.Series) -> pd.Series:\n",
    "    ss = s.astype(\"string\").str.strip()\n",
    "    return ss.notna() & (ss != \"\")\n",
    "\n",
    "# -----------------------\n",
    "# 1) Load\n",
    "# -----------------------\n",
    "v1 = pd.read_csv(V1_PATH)\n",
    "v2 = pd.read_csv(V2_PATH)\n",
    "v3 = pd.read_csv(V3_PATH)\n",
    "\n",
    "key_base = [\"day\", \"treatment\", \"source\", \"ops_type_merged\"]\n",
    "\n",
    "v1 = strip_cols(v1, key_base + [\"city\"])\n",
    "v2 = strip_cols(v2, key_base + [\"city_group\"])\n",
    "v3 = strip_cols(v3, key_base + [\"city\"])\n",
    "\n",
    "v1[\"day\"] = pd.to_datetime(v1[\"day\"], errors=\"coerce\")\n",
    "v2[\"day\"] = pd.to_datetime(v2[\"day\"], errors=\"coerce\")\n",
    "v3[\"day\"] = pd.to_datetime(v3[\"day\"], errors=\"coerce\")\n",
    "\n",
    "# -----------------------\n",
    "# 2) v1/v3: city -> city_group，再聚合到 city_group 層級\n",
    "# -----------------------\n",
    "v1[\"city_group\"] = v1[\"city\"].map(map_city_group)\n",
    "v3[\"city_group\"] = v3[\"city\"].map(map_city_group)\n",
    "\n",
    "v1_g = (\n",
    "    v1.groupby(key_base + [\"city_group\"], dropna=False, as_index=False)[\"nonrepeat_cnt\"]\n",
    "      .sum()\n",
    ")\n",
    "\n",
    "v3_g = (\n",
    "    v3.groupby(key_base + [\"city_group\"], dropna=False, as_index=False)[\"trip_cnt\"]\n",
    "      .sum()\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 3) merge: v1_g + v3_g + v2\n",
    "# -----------------------\n",
    "m = v3_g.merge(v1_g, on=key_base + [\"city_group\"], how=\"outer\")\n",
    "m = m.merge(\n",
    "    v2.rename(columns={\"is_match_rate\": \"match_rate\"}),\n",
    "    on=key_base + [\"city_group\"],\n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 4) format + drop empty keys + save\n",
    "# -----------------------\n",
    "m = m[[\"day\",\"treatment\",\"source\",\"ops_type_merged\",\"city_group\",\"trip_cnt\",\"nonrepeat_cnt\",\"match_rate\"]].copy()\n",
    "\n",
    "# 轉 day 回字串（保留排序前先不轉也可以；這裡先轉，方便輸出）\n",
    "m[\"day\"] = pd.to_datetime(m[\"day\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# 數值欄位轉 numeric\n",
    "for c in [\"trip_cnt\", \"nonrepeat_cnt\", \"match_rate\"]:\n",
    "    m[c] = pd.to_numeric(m[c], errors=\"coerce\")\n",
    "\n",
    "# ✅ 拿掉 treatment/source/ops_type_merged/city_group 任何空值或空字串\n",
    "m = strip_cols(m, [\"treatment\",\"source\",\"ops_type_merged\",\"city_group\"])\n",
    "mask_keep = (\n",
    "    nonempty(m[\"treatment\"]) &\n",
    "    nonempty(m[\"source\"]) &\n",
    "    nonempty(m[\"ops_type_merged\"]) &\n",
    "    nonempty(m[\"city_group\"])\n",
    ")\n",
    "m = m[mask_keep].copy()\n",
    "\n",
    "# 排序\n",
    "m = m.sort_values([\"day\",\"treatment\",\"source\",\"ops_type_merged\",\"city_group\"], na_position=\"last\").reset_index(drop=True)\n",
    "\n",
    "m.to_csv(OUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved:\", OUT_PATH, \"rows:\", len(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0ae3ce",
   "metadata": {},
   "source": [
    "# Weekend/Holiday data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5834f952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/daily_trip_merged_with_weekend.csv rows: 20160\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -----------------------\n",
    "# Config (改路徑)\n",
    "# -----------------------\n",
    "IN_PATH  = \"../data/daily_trip_merged.csv\"\n",
    "OUT_PATH = \"../data/daily_trip_merged_with_weekend.csv\"\n",
    "\n",
    "# -----------------------\n",
    "# Holidays (固定日 + 指定國定假日)\n",
    "# -----------------------\n",
    "HOLIDAYS = {\n",
    "    \"2025-09-29\",\n",
    "    \"2025-10-06\",\n",
    "    \"2025-10-10\",\n",
    "    \"2025-10-24\",\n",
    "    \"2025-12-25\",\n",
    "    \"2026-01-01\",\n",
    "}\n",
    "\n",
    "# -----------------------\n",
    "# Load\n",
    "# -----------------------\n",
    "df = pd.read_csv(IN_PATH)\n",
    "\n",
    "# day -> datetime\n",
    "df[\"day_dt\"] = pd.to_datetime(df[\"day\"], errors=\"coerce\")\n",
    "\n",
    "# 週末：Saturday(5) / Sunday(6)\n",
    "is_weekend = df[\"day_dt\"].dt.dayofweek.isin([5, 6])\n",
    "\n",
    "# 指定國定假日\n",
    "is_holiday = df[\"day\"].astype(str).isin(HOLIDAYS)\n",
    "\n",
    "# weekend OR holiday -> 1 else 0\n",
    "df[\"is_weekend_holiday\"] = (is_weekend | is_holiday).astype(\"int8\")\n",
    "\n",
    "# 清掉暫存欄位\n",
    "df = df.drop(columns=[\"day_dt\"])\n",
    "\n",
    "# Save\n",
    "df.to_csv(OUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved:\", OUT_PATH, \"rows:\", len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab69b183",
   "metadata": {},
   "source": [
    "# Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df046c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/daily_trip_merged_with_weekend_and_weather.csv rows: 20160\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------\n",
    "# Config (改路徑)\n",
    "# -----------------------\n",
    "BASE_PATH = \"../data/daily_trip_merged_with_weekend.csv\"\n",
    "WEATHER_PATH = \"../../data/weather.csv\"             \n",
    "OUT_PATH = \"../data/daily_trip_merged_with_weekend_and_weather.csv\"\n",
    "\n",
    "# -----------------------\n",
    "# City group mapping（三區，固定分母用）\n",
    "# -----------------------\n",
    "north = {'臺北市','新北市','基隆市','桃園市','宜蘭縣','花蓮縣','新竹縣','新竹市'}\n",
    "central = {'苗栗縣','臺中市','南投縣','彰化縣','雲林縣','嘉義縣','嘉義市'}\n",
    "south = {'臺南市','高雄市','屏東縣','臺東縣','澎湖縣','金門縣','連江縣'}\n",
    "\n",
    "def map_city_group(city):\n",
    "    if pd.isna(city) or city == \"\":\n",
    "        city = \"臺北市\"\n",
    "    if city in north:\n",
    "        return \"北區\"\n",
    "    if city in central:\n",
    "        return \"中區\"\n",
    "    if city in south:\n",
    "        return \"南區\"\n",
    "    return \"北區\"\n",
    "\n",
    "GROUP_CITIES = {\n",
    "    \"北區\": sorted(list(north)),\n",
    "    \"中區\": sorted(list(central)),\n",
    "    \"南區\": sorted(list(south)),\n",
    "}\n",
    "GROUP_DENOM = {g: len(cities) for g, cities in GROUP_CITIES.items()}\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def strip_col(df, c):\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(\"string\").str.strip()\n",
    "    return df\n",
    "\n",
    "# -----------------------\n",
    "# 1) Load base\n",
    "# -----------------------\n",
    "base = pd.read_csv(BASE_PATH)\n",
    "base = strip_col(base, \"day\")\n",
    "base = strip_col(base, \"city_group\")\n",
    "base[\"day_dt\"] = pd.to_datetime(base[\"day\"], errors=\"coerce\")\n",
    "# 保險：若 city_group 有空/怪值，也先丟掉（你前面應該已清過）\n",
    "base = base.dropna(subset=[\"day_dt\", \"city_group\"]).copy()\n",
    "\n",
    "# -----------------------\n",
    "# 2) Load weather\n",
    "# -----------------------\n",
    "w = pd.read_csv(WEATHER_PATH)\n",
    "\n",
    "# 只留需要欄位\n",
    "need_cols = [\"cityName\", \"forecastDate\", \"publishDate\", \"precipChance\"]\n",
    "w = w[need_cols].copy()\n",
    "\n",
    "# 清理字串\n",
    "w = strip_col(w, \"cityName\")\n",
    "w = strip_col(w, \"forecastDate\")\n",
    "w = strip_col(w, \"publishDate\")\n",
    "\n",
    "# 轉日期\n",
    "w[\"forecast_dt\"] = pd.to_datetime(w[\"forecastDate\"], errors=\"coerce\")\n",
    "w[\"publish_dt\"]  = pd.to_datetime(w[\"publishDate\"], errors=\"coerce\")\n",
    "\n",
    "# precipChance 轉數字\n",
    "w[\"precipChance\"] = pd.to_numeric(w[\"precipChance\"], errors=\"coerce\")\n",
    "\n",
    "# 丟掉關鍵缺失\n",
    "w = w.dropna(subset=[\"cityName\", \"forecast_dt\", \"publish_dt\", \"precipChance\"]).copy()\n",
    "\n",
    "# -----------------------\n",
    "# 3) 套 publishDate == forecastDate - 7 天 的規則\n",
    "# -----------------------\n",
    "w[\"publish_expected\"] = w[\"forecast_dt\"] - pd.Timedelta(days=7)\n",
    "w = w[w[\"publish_dt\"].dt.date == w[\"publish_expected\"].dt.date].copy()\n",
    "\n",
    "# 如果你的 publishDate 是日期(無時間)也OK，上面用 date 對齊最穩\n",
    "\n",
    "# -----------------------\n",
    "# 4) city/day 是否下雨：同一天(早晚兩筆)只要任一筆 precipChance>=60\n",
    "# -----------------------\n",
    "w[\"is_rainy_city\"] = (w[\"precipChance\"] >= 60).astype(\"int8\")\n",
    "\n",
    "city_day = (\n",
    "    w.groupby([\"cityName\", \"forecast_dt\"], as_index=False)[\"is_rainy_city\"]\n",
    "     .max()\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 5) city -> city_group，算每區每日 is_rainy = rainy_city_cnt / 該區城市總數(固定分母)\n",
    "# -----------------------\n",
    "city_day[\"city_group\"] = city_day[\"cityName\"].map(map_city_group)\n",
    "\n",
    "group_day = (\n",
    "    city_day.groupby([\"forecast_dt\", \"city_group\"], as_index=False)[\"is_rainy_city\"]\n",
    "            .sum()\n",
    "            .rename(columns={\"is_rainy_city\": \"rainy_city_cnt\"})\n",
    ")\n",
    "\n",
    "group_day[\"denom_city_cnt\"] = group_day[\"city_group\"].map(GROUP_DENOM).astype(\"int16\")\n",
    "group_day[\"is_rainy\"] = np.where(\n",
    "    group_day[\"denom_city_cnt\"] > 0,\n",
    "    group_day[\"rainy_city_cnt\"] / group_day[\"denom_city_cnt\"],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# 只留要 merge 的欄位\n",
    "group_day = group_day.rename(columns={\"forecast_dt\": \"day_dt\"})[[\"day_dt\", \"city_group\", \"is_rainy\"]]\n",
    "\n",
    "# -----------------------\n",
    "# 6) Merge 回主檔（by day + city_group）\n",
    "# -----------------------\n",
    "out = base.merge(group_day, on=[\"day_dt\", \"city_group\"], how=\"left\")\n",
    "\n",
    "# 還原 day 字串、移除 day_dt\n",
    "out[\"day\"] = out[\"day_dt\"].dt.strftime(\"%Y-%m-%d\")\n",
    "out = out.drop(columns=[\"day_dt\"])\n",
    "\n",
    "# -----------------------\n",
    "# 7) Save\n",
    "# -----------------------\n",
    "out.to_csv(OUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved:\", OUT_PATH, \"rows:\", len(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32dfa60",
   "metadata": {},
   "source": [
    "# Coupon data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5182fc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5k/wdr6l04n13xbrtz6l93pdqmc0000gn/T/ipykernel_63244/1046544406.py:105: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (~coupon_category[\"promo_title\"].str.contains(pattern, case=False, regex=True, na=False)),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed chunks: 5, acc daily-groups: 17,196\n",
      "processed chunks: 10, acc daily-groups: 21,130\n",
      "processed chunks: 15, acc daily-groups: 21,504\n",
      "✅ Finished reading coupons. Total daily groups aggregated: 21,504\n",
      "Saved merged output to: /Users/shotime/Desktop/2026winter_project/day_model/data/daily_trip_merged_with_weekend_and_weather_with_coupon.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# =========================================================\n",
    "# Paths\n",
    "# =========================================================\n",
    "USER_PATH  = Path(\"../../cleaned_data/user_cleaned.csv\")\n",
    "COUPON_PATH = Path(\"../../merged_data/coupon_merged.csv\")\n",
    "CAT_PATH   = Path(\"../../data/coupon_category.csv\")\n",
    "\n",
    "DAILY_TRIP_PATH = Path(\"../data/daily_trip_merged_with_weekend_and_weather.csv\")\n",
    "OUT_PATH = Path(\"../data/daily_trip_merged_with_weekend_and_weather_with_coupon.csv\")\n",
    "\n",
    "# =========================================================\n",
    "# City group mapping (your definition)\n",
    "# =========================================================\n",
    "north = {'臺北市','新北市','基隆市','桃園市','宜蘭縣','花蓮縣','新竹縣','新竹市'}\n",
    "central = {'苗栗縣','臺中市','南投縣','彰化縣','雲林縣','嘉義縣','嘉義市'}\n",
    "south = {'臺南市','高雄市','屏東縣','臺東縣','澎湖縣','金門縣','連江縣'}\n",
    "\n",
    "def map_city_group(city):\n",
    "    if pd.isna(city) or city == \"\":\n",
    "        city = \"臺北市\"\n",
    "    if city in north:\n",
    "        return \"北區\"\n",
    "    if city in central:\n",
    "        return \"中區\"\n",
    "    if city in south:\n",
    "        return \"南區\"\n",
    "    return \"北區\"\n",
    "\n",
    "GROUP_CITIES = {\n",
    "    \"北區\": sorted(list(north)),\n",
    "    \"中區\": sorted(list(central)),\n",
    "    \"南區\": sorted(list(south)),\n",
    "}\n",
    "\n",
    "# =========================================================\n",
    "# Coupon type buckets (8)\n",
    "# =========================================================\n",
    "coupon_cols = [\n",
    "    \"coupon_BD\", \"coupon_CDP\", \"coupon_folk\", \"coupon_growth_other\",\n",
    "    \"coupon_MGM\", \"coupon_MKT\", \"coupon_register\", \"coupon_daily\"\n",
    "]\n",
    "coupon_total_cols = [f\"{c}_total\" for c in coupon_cols]\n",
    "\n",
    "# =========================================================\n",
    "# Helper: UTC string -> Asia/Taipei date\n",
    "# =========================================================\n",
    "def utcstr_to_taipei_date(series: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(series, utc=True, errors=\"coerce\")\n",
    "    return dt.dt.tz_convert(\"Asia/Taipei\").dt.date\n",
    "\n",
    "# =========================================================\n",
    "# 1) Load daily trip dataset (target)\n",
    "# =========================================================\n",
    "daily = pd.read_csv(DAILY_TRIP_PATH)\n",
    "daily[\"day\"] = pd.to_datetime(daily[\"day\"], errors=\"coerce\").dt.date\n",
    "\n",
    "need_cols = [\"day\", \"treatment\", \"source\", \"ops_type_merged\", \"city_group\"]\n",
    "missing = [c for c in need_cols if c not in daily.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"[daily_trip] Missing required columns: {missing}\")\n",
    "\n",
    "# Normalize city_group to your labels (in case any whitespace)\n",
    "daily[\"city_group\"] = daily[\"city_group\"].astype(str).str.strip()\n",
    "\n",
    "# =========================================================\n",
    "# 2) Load user_cleaned (weekly assignment; experiment_date is Monday)\n",
    "# =========================================================\n",
    "user = pd.read_csv(USER_PATH)\n",
    "\n",
    "user[\"experiment_date\"] = pd.to_datetime(user[\"experiment_date\"], errors=\"coerce\").dt.date\n",
    "user[\"user_id\"] = pd.to_numeric(user[\"user_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Build city_group using YOUR mapping\n",
    "if \"city\" not in user.columns:\n",
    "    raise ValueError(\"[user_cleaned] Expected a 'city' column to map into city_group.\")\n",
    "\n",
    "user[\"city\"] = user[\"city\"].astype(str).str.strip().replace({\"nan\": \"\"})\n",
    "user[\"city_group\"] = user[\"city\"].apply(map_city_group)\n",
    "\n",
    "user_map = user[[\n",
    "    \"user_id\", \"experiment_date\", \"treatment\", \"source\", \"ops_type_merged\", \"city_group\"\n",
    "]].drop_duplicates()\n",
    "\n",
    "# week_end = Sunday\n",
    "user_map[\"week_end\"] = user_map[\"experiment_date\"].apply(lambda d: d + timedelta(days=6))\n",
    "\n",
    "# =========================================================\n",
    "# 3) Load coupon_category + filters\n",
    "# =========================================================\n",
    "coupon_category = pd.read_csv(CAT_PATH)\n",
    "\n",
    "coupon_category[\"promo_id\"] = pd.to_numeric(coupon_category[\"promo_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "coupon_category[\"enable_date\"] = pd.to_datetime(coupon_category[\"enable_date\"], errors=\"coerce\").dt.date\n",
    "\n",
    "pattern = r\"(機場|機接|接機|送機|出國|test|測試|租車券|旅遊)\"\n",
    "coupon_category[\"promo_title\"] = coupon_category[\"promo_title\"].astype(str)\n",
    "\n",
    "coupon_category_f = coupon_category.loc[\n",
    "    (coupon_category[\"promo_id\"].fillna(0) >= 10508458) &\n",
    "    (~coupon_category[\"promo_title\"].str.contains(pattern, case=False, regex=True, na=False)),\n",
    "    [\"promo_id\", \"coupon_category\", \"coupon_type\", \"enable_date\"]\n",
    "].copy()\n",
    "\n",
    "# =========================================================\n",
    "# 4) Chunk read coupon_merged and aggregate DAILY group totals (active coupons)\n",
    "# Grain: day + treatment + source + ops_type_merged + city_group\n",
    "# =========================================================\n",
    "usecols = [\"user_id\", \"promo_id\", \"expiry_date\", \"redeem_time\"]\n",
    "dtype = {\n",
    "    \"user_id\": \"Int64\",\n",
    "    \"promo_id\": \"Int64\",\n",
    "    \"expiry_date\": \"string\",\n",
    "    \"redeem_time\": \"string\",\n",
    "}\n",
    "\n",
    "acc = {}  # key=(day, treatment, source, ops_type_merged, city_group) -> np.array(8)\n",
    "\n",
    "chunksize = 1_000_000\n",
    "reader = pd.read_csv(\n",
    "    COUPON_PATH,\n",
    "    usecols=usecols,\n",
    "    dtype=dtype,\n",
    "    chunksize=chunksize,\n",
    "    low_memory=True\n",
    ")\n",
    "\n",
    "group_cols_day = [\"day\", \"treatment\", \"source\", \"ops_type_merged\", \"city_group\"]\n",
    "\n",
    "for i, chunk in enumerate(reader, start=1):\n",
    "    chunk[\"user_id\"] = pd.to_numeric(chunk[\"user_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    chunk[\"promo_id\"] = pd.to_numeric(chunk[\"promo_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # Apply promo filter via inner join\n",
    "    c = chunk.merge(coupon_category_f, on=\"promo_id\", how=\"inner\")\n",
    "    if c.empty:\n",
    "        continue\n",
    "\n",
    "    # expiry_date_eff = redeem_time (Taipei date) if present else expiry_date (Taipei date)\n",
    "    c[\"expiry_date_eff\"] = utcstr_to_taipei_date(c[\"redeem_time\"])\n",
    "    m = c[\"expiry_date_eff\"].isna()\n",
    "    if m.any():\n",
    "        c.loc[m, \"expiry_date_eff\"] = utcstr_to_taipei_date(c.loc[m, \"expiry_date\"])\n",
    "\n",
    "    # Drop missing enable/expiry\n",
    "    c = c.loc[c[\"enable_date\"].notna() & c[\"expiry_date_eff\"].notna()].copy()\n",
    "    if c.empty:\n",
    "        continue\n",
    "\n",
    "    # Join weekly group assignment\n",
    "    cu = c.merge(user_map, on=\"user_id\", how=\"inner\")\n",
    "    if cu.empty:\n",
    "        continue\n",
    "\n",
    "    # Overlap between coupon active window and the user's week window [Mon..Sun]\n",
    "    cu[\"overlap_start\"] = cu[[\"enable_date\", \"experiment_date\"]].max(axis=1)\n",
    "    cu[\"overlap_end\"]   = cu[[\"expiry_date_eff\", \"week_end\"]].min(axis=1)\n",
    "\n",
    "    cu = cu.loc[cu[\"overlap_start\"] <= cu[\"overlap_end\"]].copy()\n",
    "    if cu.empty:\n",
    "        continue\n",
    "\n",
    "    # Bucketize (0/1 per row)\n",
    "    cu[\"coupon_BD\"] = (cu[\"coupon_type\"].isin([\"BD\", \"異業合作\"])).astype(np.int32)\n",
    "    cu[\"coupon_CDP\"] = ((cu[\"coupon_type\"] == \"Folksonomy\") & (cu[\"coupon_category\"] == \"Event-Triggered\")).astype(np.int32)\n",
    "    cu[\"coupon_folk\"] = ((cu[\"coupon_type\"] == \"Folksonomy\") & (cu[\"coupon_category\"] == \"Gene\")).astype(np.int32)\n",
    "    cu[\"coupon_growth_other\"] = (\n",
    "        (cu[\"coupon_type\"] == \"Folksonomy\") &\n",
    "        (~cu[\"coupon_category\"].isin([\"Event-Triggered\", \"Gene\"]))\n",
    "    ).astype(np.int32)\n",
    "    cu[\"coupon_MGM\"] = (cu[\"coupon_type\"] == \"MGM\").astype(np.int32)\n",
    "    cu[\"coupon_MKT\"] = (cu[\"coupon_type\"] == \"MKT\").astype(np.int32)\n",
    "    cu[\"coupon_register\"] = (cu[\"coupon_type\"] == \"新註冊\").astype(np.int32)\n",
    "    cu[\"coupon_daily\"] = (cu[\"coupon_type\"] == \"天天領\").astype(np.int32)\n",
    "\n",
    "    # Expand to daily within overlap (max 7 days per row)\n",
    "    cu[\"day\"] = cu.apply(\n",
    "        lambda r: pd.date_range(r[\"overlap_start\"], r[\"overlap_end\"], freq=\"D\").date.tolist(),\n",
    "        axis=1\n",
    "    )\n",
    "    cu = cu.explode(\"day\", ignore_index=True)\n",
    "\n",
    "    # Daily totals in this chunk\n",
    "    g = cu.groupby(group_cols_day, as_index=False)[coupon_cols].sum()\n",
    "\n",
    "    # Accumulate into dict\n",
    "    for row in g.itertuples(index=False):\n",
    "        key = (row.day, row.treatment, row.source, row.ops_type_merged, row.city_group)\n",
    "        vec = np.array([getattr(row, col) for col in coupon_cols], dtype=np.int64)\n",
    "        if key in acc:\n",
    "            acc[key] += vec\n",
    "        else:\n",
    "            acc[key] = vec\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        print(f\"processed chunks: {i}, acc daily-groups: {len(acc):,}\")\n",
    "\n",
    "print(f\"✅ Finished reading coupons. Total daily groups aggregated: {len(acc):,}\")\n",
    "\n",
    "# =========================================================\n",
    "# 5) acc -> DataFrame (coupon_daily_totals)\n",
    "# =========================================================\n",
    "if len(acc) == 0:\n",
    "    coupon_daily_totals = daily[need_cols].drop_duplicates().copy()\n",
    "    for c in coupon_total_cols:\n",
    "        coupon_daily_totals[c] = 0\n",
    "else:\n",
    "    rows = []\n",
    "    for k, v in acc.items():\n",
    "        rows.append(list(k) + list(v))\n",
    "    coupon_daily_totals = pd.DataFrame(rows, columns=group_cols_day + coupon_cols)\n",
    "    coupon_daily_totals = coupon_daily_totals.rename(columns={c: f\"{c}_total\" for c in coupon_cols})\n",
    "\n",
    "# =========================================================\n",
    "# 6) Merge into daily trip data and save\n",
    "# =========================================================\n",
    "out = daily.merge(\n",
    "    coupon_daily_totals,\n",
    "    on=[\"day\", \"treatment\", \"source\", \"ops_type_merged\", \"city_group\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "for c in coupon_total_cols:\n",
    "    if c not in out.columns:\n",
    "        out[c] = 0\n",
    "    out[c] = out[c].fillna(0).astype(np.int64)\n",
    "\n",
    "out.to_csv(OUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Saved merged output to: {OUT_PATH.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv) 2026winter_project",
   "language": "python",
   "name": "2026winter_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
