{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93e62385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== nonrepeat_cnt_per_user ====================\n",
      "[2025-12-15] train_rows=456 test_rows=24  RMSE=0.026213  MAE=0.017293\n",
      "[2025-12-22] train_rows=480 test_rows=24  RMSE=0.044824  MAE=0.031992\n",
      "[Overall 2 weeks] test_rows=48  RMSE=0.036718  MAE=0.024643\n",
      "\n",
      "==================== trip_cnt_per_user ====================\n",
      "[2025-12-15] train_rows=456 test_rows=24  RMSE=0.017717  MAE=0.013171\n",
      "[2025-12-22] train_rows=480 test_rows=24  RMSE=0.031629  MAE=0.023037\n",
      "[Overall 2 weeks] test_rows=48  RMSE=0.025635  MAE=0.018104\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "DATA_PATH = \"../../final_data/data_260125_control.csv\"\n",
    "DATE_COL = \"experiment_date\"\n",
    "\n",
    "TARGET_WEEKS = [pd.Timestamp(\"2025-12-15\"), pd.Timestamp(\"2025-12-22\")]\n",
    "\n",
    "Y_COLS = [\"nonrepeat_cnt_per_user\", \"trip_cnt_per_user\"]\n",
    "\n",
    "CAT_COLS = [\"ops_type_merged\", \"city_group\"]\n",
    "\n",
    "NUM_FEATURES = [\n",
    "    \"avg_rainy_day\",\n",
    "    \"avg_rainy_weekday\",\n",
    "    \"avg_rainy_weekend\",\n",
    "    \"mgm_day\",\n",
    "    \"nonrepeat_cnt_per_user_lag2\",\n",
    "    \"nonrepeat_cnt_per_user_roll4\",\n",
    "    \"trip_cnt_per_user_lag2\",\n",
    "    \"trip_cnt_per_user_roll4\",\n",
    "    \"weekday_nonrepeat_cnt_per_user_lag2\",\n",
    "    \"weekday_nonrepeat_cnt_per_user_roll4\",\n",
    "    \"weekday_trip_cnt_per_user_lag2\",\n",
    "    \"weekday_trip_cnt_per_user_roll4\",\n",
    "    \"weekday_match_rate_lag2\",\n",
    "    \"weekday_match_rate_roll4\",\n",
    "    \"weekend_nonrepeat_cnt_per_user_lag2\",\n",
    "    \"weekend_nonrepeat_cnt_per_user_roll4\",\n",
    "    \"weekend_trip_cnt_per_user_lag2\",\n",
    "    \"weekend_trip_cnt_per_user_roll4\",\n",
    "    \"weekend_match_rate_lag2\",\n",
    "    \"weekend_match_rate_roll4\",\n",
    "    \"has_national_holiday\",\n",
    "    \"coupon_BD_per_user_log1p_lag2\",\n",
    "    \"coupon_BD_per_user_log1p_roll4\",\n",
    "    \"coupon_CDP_per_user_log1p_lag2\",\n",
    "    \"coupon_CDP_per_user_log1p_roll4\",\n",
    "    \"coupon_folk_per_user_log1p_lag2\",\n",
    "    \"coupon_folk_per_user_log1p_roll4\",\n",
    "    \"coupon_growth_other_per_user_log1p_lag2\",\n",
    "    \"coupon_growth_other_per_user_log1p_roll4\",\n",
    "    \"coupon_MGM_per_user_log1p_lag2\",\n",
    "    \"coupon_MGM_per_user_log1p_roll4\",\n",
    "    \"coupon_MKT_per_user_log1p_lag2\",\n",
    "    \"coupon_MKT_per_user_log1p_roll4\",\n",
    "    \"coupon_register_per_user_log1p_lag2\",\n",
    "    \"coupon_register_per_user_log1p_roll4\",\n",
    "    \"coupon_daily_per_user_log1p_lag2\",\n",
    "    \"coupon_daily_per_user_log1p_roll4\",\n",
    "    \"delta_trip_per_user\",\n",
    "    \"delta_nonrepeat_per_user\",\n",
    "]\n",
    "\n",
    "# --------------------------\n",
    "# Load\n",
    "# --------------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "df = df[df[DATE_COL].notna()].copy()\n",
    "\n",
    "# 只保留到最大 target week 為止（後面不會用到）\n",
    "df = df[df[DATE_COL] <= max(TARGET_WEEKS)].copy()\n",
    "\n",
    "# categorical cast\n",
    "for c in CAT_COLS:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(\"category\")\n",
    "\n",
    "# feature availability check\n",
    "missing_num = [c for c in NUM_FEATURES if c not in df.columns]\n",
    "missing_cat = [c for c in CAT_COLS if c not in df.columns]\n",
    "if missing_cat:\n",
    "    print(\"[WARN] Missing categorical cols (will be dropped):\", missing_cat)\n",
    "if missing_num:\n",
    "    print(\"[WARN] Missing numeric features (will be dropped):\", missing_num)\n",
    "\n",
    "X_COLS = [c for c in CAT_COLS if c in df.columns] + [c for c in NUM_FEATURES if c in df.columns]\n",
    "CAT_IN_X = [c for c in CAT_COLS if c in df.columns]\n",
    "\n",
    "# --------------------------\n",
    "# Helpers\n",
    "# --------------------------\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def mae(y_true, y_pred) -> float:\n",
    "    return float(mean_absolute_error(y_true, y_pred))\n",
    "\n",
    "def fit_predict_one_week(df_all: pd.DataFrame, target_week: pd.Timestamp, y_col: str):\n",
    "    # 嚴格遵守：只能拿 target_week 之前的資料訓練\n",
    "    train_df = df_all[df_all[DATE_COL] < target_week].copy()\n",
    "    test_df  = df_all[df_all[DATE_COL] == target_week].copy()\n",
    "\n",
    "    # drop rows with missing target\n",
    "    train_df = train_df[train_df[y_col].notna()].copy()\n",
    "    test_df  = test_df[test_df[y_col].notna()].copy()\n",
    "\n",
    "    if len(train_df) == 0 or len(test_df) == 0:\n",
    "        return None, None, len(train_df), len(test_df)\n",
    "\n",
    "    X_train = train_df[X_COLS]\n",
    "    y_train = train_df[y_col].astype(float)\n",
    "\n",
    "    X_test = test_df[X_COLS]\n",
    "    y_test = test_df[y_col].astype(float)\n",
    "\n",
    "    model = lgb.LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        n_estimators=3600,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=35,\n",
    "        min_child_samples=20,\n",
    "        subsample=1.0,\n",
    "        colsample_bytree=0.7,\n",
    "        bagging_freq=0,\n",
    "        reg_alpha=0.4,\n",
    "        reg_lambda=7.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1,\n",
    "    )\n",
    "\n",
    "    # categorical_feature: 用欄名即可\n",
    "    model.fit(X_train, y_train, categorical_feature=[c for c in CAT_IN_X if c in X_train.columns])\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_test.values, y_pred, len(train_df), len(test_df)\n",
    "\n",
    "# --------------------------\n",
    "# Run + Print metrics\n",
    "# --------------------------\n",
    "for y_col in Y_COLS:\n",
    "    print(f\"\\n==================== {y_col} ====================\")\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    for tw in TARGET_WEEKS:\n",
    "        y_true, y_pred, n_tr, n_te = fit_predict_one_week(df, tw, y_col)\n",
    "        if y_true is None:\n",
    "            print(f\"[{tw.date()}] SKIP (train_rows={n_tr:,}, test_rows={n_te:,})\")\n",
    "            continue\n",
    "\n",
    "        r = rmse(y_true, y_pred)\n",
    "        m = mae(y_true, y_pred)\n",
    "        print(f\"[{tw.date()}] train_rows={n_tr:,} test_rows={n_te:,}  RMSE={r:.6f}  MAE={m:.6f}\")\n",
    "\n",
    "        all_y_true.append(y_true)\n",
    "        all_y_pred.append(y_pred)\n",
    "\n",
    "    if all_y_true:\n",
    "        all_y_true = np.concatenate(all_y_true)\n",
    "        all_y_pred = np.concatenate(all_y_pred)\n",
    "        print(f\"[Overall 2 weeks] test_rows={len(all_y_true):,}  RMSE={rmse(all_y_true, all_y_pred):.6f}  MAE={mae(all_y_true, all_y_pred):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99366d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a2778d7ba146fdadfe0881a03d5deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Optuna Best (trip_cnt_per_user) =====\n",
      "Best RMSE (avg over 2 weeks): 0.0246632502224929\n",
      "Best params: {'learning_rate': 0.06737402776597771, 'n_estimators': 3600, 'num_leaves': 35, 'min_child_samples': 20, 'subsample': 1.0, 'colsample_bytree': 0.7, 'bagging_freq': 0, 'reg_alpha': 0.3627746654415558, 'reg_lambda': 7.1919990636928235}\n",
      "\n",
      "==================== trip_cnt_per_user (BEST params) ====================\n",
      "[2025-12-15] train_rows=456 test_rows=24 RMSE=0.018084 MAE=0.012919\n",
      "[2025-12-22] train_rows=480 test_rows=24 RMSE=0.031242 MAE=0.022657\n",
      "[Overall 2 weeks] test_rows=48 RMSE=0.025526 MAE=0.017788\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cc088795bd4eb0a68387f72243be55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Optuna Best (nonrepeat_cnt_per_user) =====\n",
      "Best RMSE (avg over 2 weeks): 0.034817604824950235\n",
      "Best params: {'learning_rate': 0.035706588601484346, 'n_estimators': 1800, 'num_leaves': 55, 'min_child_samples': 60, 'subsample': 0.75, 'colsample_bytree': 0.65, 'bagging_freq': 2, 'reg_alpha': 1.0707585889530296, 'reg_lambda': 6.881170955936629}\n",
      "\n",
      "==================== nonrepeat_cnt_per_user (BEST params) ====================\n",
      "[2025-12-15] train_rows=456 test_rows=24 RMSE=0.024273 MAE=0.015087\n",
      "[2025-12-22] train_rows=480 test_rows=24 RMSE=0.045362 MAE=0.032614\n",
      "[Overall 2 weeks] test_rows=48 RMSE=0.036379 MAE=0.023850\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Optuna tuning for CONTROL\n",
    "# - Tune separately for each target in Y_COLS\n",
    "# - Objective: avg RMSE over 2025-12-15 & 2025-12-22\n",
    "# - Training constraint: ONLY use data strictly before each target week\n",
    "# - No saving files, just print best + per-week metrics\n",
    "# ==========================\n",
    "\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# ---- baseline params (center of search)\n",
    "BASE_PARAMS = dict(\n",
    "    objective=\"regression\",\n",
    "    n_estimators=3000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    min_child_samples=80,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.8,\n",
    "    bagging_freq=1,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=2.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1,\n",
    ")\n",
    "\n",
    "def fit_predict_one_week_with_params(df_all: pd.DataFrame, target_week: pd.Timestamp, y_col: str, model_params: dict):\n",
    "    train_df = df_all[df_all[DATE_COL] < target_week].copy()\n",
    "    test_df  = df_all[df_all[DATE_COL] == target_week].copy()\n",
    "\n",
    "    train_df = train_df[train_df[y_col].notna()].copy()\n",
    "    test_df  = test_df[test_df[y_col].notna()].copy()\n",
    "\n",
    "    if len(train_df) == 0 or len(test_df) == 0:\n",
    "        return None, None, len(train_df), len(test_df)\n",
    "\n",
    "    X_train = train_df[X_COLS]\n",
    "    y_train = train_df[y_col].astype(float)\n",
    "\n",
    "    X_test = test_df[X_COLS]\n",
    "    y_test = test_df[y_col].astype(float)\n",
    "\n",
    "    model = lgb.LGBMRegressor(**model_params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        categorical_feature=[c for c in CAT_IN_X if c in X_train.columns],\n",
    "    )\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_test.values, y_pred, len(train_df), len(test_df)\n",
    "\n",
    "def tune_one_target(y_col: str, n_trials: int = 60):\n",
    "    def objective(trial: optuna.Trial) -> float:\n",
    "        params = dict(BASE_PARAMS)\n",
    "\n",
    "        # --- search space near your baseline (and sensible)\n",
    "        params[\"learning_rate\"] = trial.suggest_float(\"learning_rate\", 0.02, 0.08, log=True)\n",
    "        params[\"n_estimators\"]  = trial.suggest_int(\"n_estimators\", 1200, 4500, step=300)\n",
    "\n",
    "        params[\"num_leaves\"] = trial.suggest_int(\"num_leaves\", 15, 127, step=4)\n",
    "        params[\"min_child_samples\"] = trial.suggest_int(\"min_child_samples\", 20, 200, step=10)\n",
    "\n",
    "        params[\"subsample\"] = trial.suggest_float(\"subsample\", 0.65, 1.0, step=0.05)\n",
    "        params[\"colsample_bytree\"] = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0, step=0.05)\n",
    "\n",
    "        # bagging_freq: keep small (0/1/2/5) - your baseline uses 1\n",
    "        params[\"bagging_freq\"] = trial.suggest_categorical(\"bagging_freq\", [0, 1, 2, 5])\n",
    "\n",
    "        # regularization near baseline\n",
    "        params[\"reg_alpha\"] = trial.suggest_float(\"reg_alpha\", 0.0, 3.0)\n",
    "        params[\"reg_lambda\"] = trial.suggest_float(\"reg_lambda\", 0.0, 8.0)\n",
    "\n",
    "        rmses = []\n",
    "        for i, tw in enumerate(TARGET_WEEKS, start=1):\n",
    "            y_true, y_pred, n_tr, n_te = fit_predict_one_week_with_params(df, tw, y_col, params)\n",
    "            if y_true is None or len(y_true) == 0:\n",
    "                return 1e9\n",
    "            rmses.append(rmse(y_true, y_pred))\n",
    "\n",
    "            trial.report(float(np.mean(rmses)), step=i)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        return float(np.mean(rmses))\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=42)\n",
    "    pruner = optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps=0)\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner)\n",
    "\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "    print(f\"\\n===== Optuna Best ({y_col}) =====\")\n",
    "    print(\"Best RMSE (avg over 2 weeks):\", study.best_value)\n",
    "    print(\"Best params:\", study.best_params)\n",
    "\n",
    "    best_params = dict(BASE_PARAMS)\n",
    "    best_params.update(study.best_params)\n",
    "\n",
    "    # ---- evaluate best params with RMSE + MAE per-week + overall\n",
    "    print(f\"\\n==================== {y_col} (BEST params) ====================\")\n",
    "    all_y_true, all_y_pred = [], []\n",
    "    for tw in TARGET_WEEKS:\n",
    "        y_true, y_pred, n_tr, n_te = fit_predict_one_week_with_params(df, tw, y_col, best_params)\n",
    "        if y_true is None:\n",
    "            print(f\"[{tw.date()}] SKIP (train_rows={n_tr:,}, test_rows={n_te:,})\")\n",
    "            continue\n",
    "        print(f\"[{tw.date()}] train_rows={n_tr:,} test_rows={n_te:,} RMSE={rmse(y_true, y_pred):.6f} MAE={mae(y_true, y_pred):.6f}\")\n",
    "        all_y_true.append(y_true)\n",
    "        all_y_pred.append(y_pred)\n",
    "\n",
    "    if all_y_true:\n",
    "        all_y_true = np.concatenate(all_y_true)\n",
    "        all_y_pred = np.concatenate(all_y_pred)\n",
    "        print(f\"[Overall 2 weeks] test_rows={len(all_y_true):,} RMSE={rmse(all_y_true, all_y_pred):.6f} MAE={mae(all_y_true, all_y_pred):.6f}\")\n",
    "\n",
    "    return study\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Run tuning separately for two targets\n",
    "# --------------------------\n",
    "study_trip = tune_one_target(\"trip_cnt_per_user\", n_trials=60)\n",
    "study_nonrepeat = tune_one_target(\"nonrepeat_cnt_per_user\", n_trials=60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
