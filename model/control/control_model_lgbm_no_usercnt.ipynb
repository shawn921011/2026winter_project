{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3159a791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Target week === 2026-01-05\n",
      "Train rows: 528 | Test rows: 24\n",
      "Train weeks: 22 | Test weeks: 1\n",
      "Test ops_type_merged: 8 | Test city_group: 3\n",
      "\n",
      "[Info] Dropped constant features in train: ['treatment', 'source', 'trip_cnt_per_user_roll4_isna', 'nonrepeat_cnt_per_user_roll4_isna', 'weekday_nonrepeat_cnt_per_user_roll4_isna', 'weekend_nonrepeat_cnt_per_user_roll4_isna', 'weekday_trip_cnt_per_user_roll4_isna', 'weekend_trip_cnt_per_user_roll4_isna', 'weekday_match_rate_roll4_isna', 'weekend_match_rate_roll4_isna', 'log_coupon_BD_total_per_user_roll4_isna', 'log_coupon_CDP_total_per_user_roll4_isna', 'log_coupon_folk_total_per_user_roll4_isna', 'log_coupon_growth_other_total_per_user_roll4_isna', 'log_coupon_MGM_total_per_user_roll4_isna', 'log_coupon_MKT_total_per_user_roll4_isna', 'log_coupon_register_total_per_user_roll4_isna', 'log_coupon_daily_total_per_user_roll4_isna', 'log_coupon_total_per_user_roll4_isna']\n",
      "[LightGBM] [Info] Total Bins 6398\n",
      "[LightGBM] [Info] Number of data points in the train set: 432, number of used features: 95\n",
      "[LightGBM] [Info] Start training from score 0.096434\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Total Bins 6398\n",
      "[LightGBM] [Info] Number of data points in the train set: 432, number of used features: 95\n",
      "[LightGBM] [Info] Start training from score 0.130375\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "=== Overall metrics (target week) ===\n",
      "                target          model      MAE     RMSE\n",
      "nonrepeat_cnt_per_user  baseline_lag1 0.050654 0.061245\n",
      "nonrepeat_cnt_per_user baseline_roll4 0.043487 0.052395\n",
      "nonrepeat_cnt_per_user    lgbm_global 0.026321 0.032260\n",
      "     trip_cnt_per_user  baseline_lag1 0.023878 0.029743\n",
      "     trip_cnt_per_user baseline_roll4 0.022243 0.026756\n",
      "     trip_cnt_per_user    lgbm_global 0.022053 0.025706\n",
      "\n",
      "=== Metrics by ops_type_merged (target week) ===\n",
      "ops_type_merged                 target          model      MAE     RMSE  n\n",
      "   14天在其他尖峰預估車資 nonrepeat_cnt_per_user  baseline_lag1 0.058491 0.069610  3\n",
      "   14天在其他尖峰預估車資 nonrepeat_cnt_per_user baseline_roll4 0.035716 0.047553  3\n",
      "   14天在其他尖峰預估車資 nonrepeat_cnt_per_user    lgbm_global 0.014265 0.014314  3\n",
      "    14天在晚尖峰預估車資 nonrepeat_cnt_per_user  baseline_lag1 0.052384 0.065279  3\n",
      "    14天在晚尖峰預估車資 nonrepeat_cnt_per_user baseline_roll4 0.051107 0.067293  3\n",
      "    14天在晚尖峰預估車資 nonrepeat_cnt_per_user    lgbm_global 0.030276 0.036739  3\n",
      "     90天在尖峰預估車資 nonrepeat_cnt_per_user  baseline_lag1 0.023160 0.025283  3\n",
      "     90天在尖峰預估車資 nonrepeat_cnt_per_user baseline_roll4 0.019414 0.021412  3\n",
      "     90天在尖峰預估車資 nonrepeat_cnt_per_user    lgbm_global 0.013593 0.014182  3\n",
      "          喚回-其他 nonrepeat_cnt_per_user  baseline_lag1 0.018568 0.019912  3\n",
      "          喚回-其他 nonrepeat_cnt_per_user baseline_roll4 0.010418 0.014716  3\n",
      "          喚回-其他 nonrepeat_cnt_per_user    lgbm_global 0.008483 0.010422  3\n",
      "       喚回-高優惠敏感 nonrepeat_cnt_per_user  baseline_lag1 0.075408 0.078426  3\n",
      "       喚回-高優惠敏感 nonrepeat_cnt_per_user baseline_roll4 0.060948 0.061523  3\n",
      "       喚回-高優惠敏感 nonrepeat_cnt_per_user    lgbm_global 0.032606 0.034815  3\n",
      "    既有regular鞏固 nonrepeat_cnt_per_user  baseline_lag1 0.067614 0.079491  3\n",
      "    既有regular鞏固 nonrepeat_cnt_per_user baseline_roll4 0.063584 0.067308  3\n",
      "    既有regular鞏固 nonrepeat_cnt_per_user    lgbm_global 0.021236 0.022980  3\n",
      "   養成Regular-其他 nonrepeat_cnt_per_user  baseline_lag1 0.042694 0.050759  3\n",
      "   養成Regular-其他 nonrepeat_cnt_per_user baseline_roll4 0.041738 0.044119  3\n",
      "   養成Regular-其他 nonrepeat_cnt_per_user    lgbm_global 0.033185 0.035184  3\n",
      "養成Regular-高優惠敏感 nonrepeat_cnt_per_user  baseline_lag1 0.066915 0.069418  3\n",
      "養成Regular-高優惠敏感 nonrepeat_cnt_per_user baseline_roll4 0.064973 0.065082  3\n",
      "養成Regular-高優惠敏感 nonrepeat_cnt_per_user    lgbm_global 0.056928 0.059016  3\n",
      "   14天在其他尖峰預估車資      trip_cnt_per_user  baseline_lag1 0.031740 0.031908  3\n",
      "   14天在其他尖峰預估車資      trip_cnt_per_user baseline_roll4 0.024116 0.025804  3\n",
      "   14天在其他尖峰預估車資      trip_cnt_per_user    lgbm_global 0.018784 0.020217  3\n",
      "    14天在晚尖峰預估車資      trip_cnt_per_user  baseline_lag1 0.024723 0.026458  3\n",
      "    14天在晚尖峰預估車資      trip_cnt_per_user baseline_roll4 0.020512 0.028653  3\n",
      "    14天在晚尖峰預估車資      trip_cnt_per_user    lgbm_global 0.024104 0.028585  3\n",
      "     90天在尖峰預估車資      trip_cnt_per_user  baseline_lag1 0.007964 0.009046  3\n",
      "     90天在尖峰預估車資      trip_cnt_per_user baseline_roll4 0.007700 0.009225  3\n",
      "     90天在尖峰預估車資      trip_cnt_per_user    lgbm_global 0.010761 0.011926  3\n",
      "          喚回-其他      trip_cnt_per_user  baseline_lag1 0.004875 0.005989  3\n",
      "          喚回-其他      trip_cnt_per_user baseline_roll4 0.008602 0.008686  3\n",
      "          喚回-其他      trip_cnt_per_user    lgbm_global 0.007267 0.007592  3\n",
      "       喚回-高優惠敏感      trip_cnt_per_user  baseline_lag1 0.031721 0.034201  3\n",
      "       喚回-高優惠敏感      trip_cnt_per_user baseline_roll4 0.031731 0.032140  3\n",
      "       喚回-高優惠敏感      trip_cnt_per_user    lgbm_global 0.028343 0.034016  3\n",
      "    既有regular鞏固      trip_cnt_per_user  baseline_lag1 0.024870 0.030727  3\n",
      "    既有regular鞏固      trip_cnt_per_user baseline_roll4 0.020014 0.023621  3\n",
      "    既有regular鞏固      trip_cnt_per_user    lgbm_global 0.023939 0.024051  3\n",
      "   養成Regular-其他      trip_cnt_per_user  baseline_lag1 0.024834 0.038224  3\n",
      "   養成Regular-其他      trip_cnt_per_user baseline_roll4 0.026231 0.030932  3\n",
      "   養成Regular-其他      trip_cnt_per_user    lgbm_global 0.028165 0.029720  3\n",
      "養成Regular-高優惠敏感      trip_cnt_per_user  baseline_lag1 0.040299 0.040825  3\n",
      "養成Regular-高優惠敏感      trip_cnt_per_user baseline_roll4 0.039035 0.039141  3\n",
      "養成Regular-高優惠敏感      trip_cnt_per_user    lgbm_global 0.035063 0.035243  3\n",
      "\n",
      "=== Metrics by city_group (target week) ===\n",
      "city_group                 target          model      MAE     RMSE  n\n",
      "        中區 nonrepeat_cnt_per_user  baseline_lag1 0.042349 0.051023  8\n",
      "        中區 nonrepeat_cnt_per_user baseline_roll4 0.036780 0.043242  8\n",
      "        中區 nonrepeat_cnt_per_user    lgbm_global 0.025815 0.033608  8\n",
      "        北區 nonrepeat_cnt_per_user  baseline_lag1 0.072779 0.080802  8\n",
      "        北區 nonrepeat_cnt_per_user baseline_roll4 0.060654 0.067262  8\n",
      "        北區 nonrepeat_cnt_per_user    lgbm_global 0.025619 0.028425  8\n",
      "        南區 nonrepeat_cnt_per_user  baseline_lag1 0.036835 0.046048  8\n",
      "        南區 nonrepeat_cnt_per_user baseline_roll4 0.033027 0.042916  8\n",
      "        南區 nonrepeat_cnt_per_user    lgbm_global 0.027530 0.034417  8\n",
      "        中區      trip_cnt_per_user  baseline_lag1 0.024562 0.032883  8\n",
      "        中區      trip_cnt_per_user baseline_roll4 0.018999 0.025663  8\n",
      "        中區      trip_cnt_per_user    lgbm_global 0.021266 0.026898  8\n",
      "        北區      trip_cnt_per_user  baseline_lag1 0.027227 0.031335  8\n",
      "        北區      trip_cnt_per_user baseline_roll4 0.028028 0.030978  8\n",
      "        北區      trip_cnt_per_user    lgbm_global 0.018676 0.020715  8\n",
      "        南區      trip_cnt_per_user  baseline_lag1 0.019846 0.024308  8\n",
      "        南區      trip_cnt_per_user baseline_roll4 0.019701 0.023010  8\n",
      "        南區      trip_cnt_per_user    lgbm_global 0.026218 0.028808  8\n",
      "\n",
      "=== Prediction preview (first 30 rows) ===\n",
      "experiment_date ops_type_merged city_group  trip_cnt_per_user  pred_y1_lag1  pred_y1_roll4  pred_y1_lgbm  nonrepeat_cnt_per_user  pred_y2_lag1  pred_y2_roll4  pred_y2_lgbm\n",
      "     2026-01-05    14天在其他尖峰預估車資         中區           0.111377      0.147170       0.122789      0.125544                0.146108      0.211321       0.173127      0.159321\n",
      "     2026-01-05    14天在其他尖峰預估車資         北區           0.143668      0.171447       0.176450      0.156523                0.197730      0.298714       0.275500      0.211383\n",
      "     2026-01-05    14天在其他尖峰預估車資         南區           0.140374      0.108727       0.112220      0.111045                0.173797      0.164521       0.171438      0.157869\n",
      "     2026-01-05     14天在晚尖峰預估車資         中區           0.164804      0.153285       0.159300      0.119007                0.198324      0.204380       0.225825      0.160552\n",
      "     2026-01-05     14天在晚尖峰預估車資         北區           0.121813      0.151575       0.170600      0.133988                0.156516      0.257874       0.268992      0.207692\n",
      "     2026-01-05     14天在晚尖峰預估車資         南區           0.115079      0.082192       0.107835      0.100741                0.154762      0.105023       0.141419      0.156643\n",
      "     2026-01-05      90天在尖峰預估車資         中區           0.044901      0.043003       0.045577      0.048586                0.056317      0.068513       0.067363      0.069227\n",
      "     2026-01-05      90天在尖峰預估車資         北區           0.038262      0.049357       0.050789      0.051105                0.054354      0.091003       0.086311      0.073205\n",
      "     2026-01-05      90天在尖峰預估車資         南區           0.031026      0.041924       0.040922      0.046781                0.045346      0.065979       0.060584      0.054365\n",
      "     2026-01-05           喚回-其他         中區           0.052469      0.052531       0.044086      0.047506                0.060700      0.074499       0.064134      0.060521\n",
      "     2026-01-05           喚回-其他         北區           0.047662      0.055826       0.057834      0.054278                0.063450      0.092184       0.088561      0.077897\n",
      "     2026-01-05           喚回-其他         南區           0.048938      0.055336       0.041688      0.059161                0.060942      0.074111       0.063650      0.071764\n",
      "     2026-01-05        喚回-高優惠敏感         中區           0.091398      0.077320       0.119035      0.088247                0.096774      0.144330       0.154255      0.113899\n",
      "     2026-01-05        喚回-高優惠敏感         北區           0.059105      0.103074       0.087719      0.092651                0.075080      0.153707       0.127937      0.108819\n",
      "     2026-01-05        喚回-高優惠敏感         南區           0.045161      0.082278       0.084103      0.093495                0.064516      0.164557       0.137024      0.111470\n",
      "     2026-01-05     既有regular鞏固         中區           0.202265      0.183422       0.195579      0.181393                0.223301      0.257496       0.258895      0.212578\n",
      "     2026-01-05     既有regular鞏固         北區           0.159231      0.208589       0.196051      0.183680                0.202692      0.329243       0.292252      0.223460\n",
      "     2026-01-05     既有regular鞏固         南區           0.141199      0.134791       0.157734      0.167693                0.152805      0.194900       0.218402      0.185022\n",
      "     2026-01-05    養成Regular-其他         中區           0.034286      0.100000       0.083695      0.074378                0.048571      0.129730       0.110508      0.093748\n",
      "     2026-01-05    養成Regular-其他         北區           0.065965      0.073976       0.080961      0.082843                0.084912      0.112946       0.117373      0.102243\n",
      "     2026-01-05    養成Regular-其他         南區           0.038724      0.039501       0.053013      0.066250                0.045558      0.064449       0.076374      0.082605\n",
      "     2026-01-05 養成Regular-高優惠敏感         中區           0.053743      0.102334       0.096024      0.091142                0.061420      0.140036       0.131645      0.130840\n",
      "     2026-01-05 養成Regular-高優惠敏感         北區           0.060421      0.100095       0.099947      0.090465                0.083149      0.164442       0.146191      0.118138\n",
      "     2026-01-05 養成Regular-高優惠敏感         南區           0.039832      0.072464       0.075131      0.077578                0.046122      0.086957       0.107773      0.112496\n",
      "\n",
      "=== Quick aggregates (means) ===\n",
      "         metric  trip_cnt_per_user  nonrepeat_cnt_per_user\n",
      "      mean_true           0.085488                0.106385\n",
      " mean_pred_lag1           0.099592                0.152121\n",
      "mean_pred_roll4           0.102462                0.148564\n",
      " mean_pred_lgbm           0.097670                0.127323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minhsiang.chang\\AppData\\Local\\Temp\\ipykernel_26560\\1295071023.py:244: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for ops, g in test.groupby(\"ops_type_merged\"):\n",
      "C:\\Users\\minhsiang.chang\\AppData\\Local\\Temp\\ipykernel_26560\\1295071023.py:266: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for cg, g in test.groupby(\"city_group\"):\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Control model (LGBM) + Baselines (NO FILE OUTPUT)\n",
    "# Target week: 2026-01-05\n",
    "# Predict:\n",
    "#   - trip_cnt_per_user\n",
    "#   - nonrepeat_cnt_per_user\n",
    "# ==========================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "DATA_PATH = \"../../final_data/data_260119_control.csv\"\n",
    "TARGET_WEEK = \"2026-01-05\"\n",
    "\n",
    "# 控制組：只關心 ops_type_merged, city_group\n",
    "CAT_COLS = [\"ops_type_merged\", \"city_group\"]\n",
    "DATE_COL = \"experiment_date\"\n",
    "\n",
    "Y1 = \"trip_cnt_per_user\"\n",
    "Y2 = \"nonrepeat_cnt_per_user\"\n",
    "\n",
    "# Baselines (必須在資料裡存在)\n",
    "Y1_LAG1 = \"trip_cnt_per_user_lag1\"\n",
    "Y1_ROLL4 = \"trip_cnt_per_user_roll4\"\n",
    "Y2_LAG1 = \"nonrepeat_cnt_per_user_lag1\"\n",
    "Y2_ROLL4 = \"nonrepeat_cnt_per_user_roll4\"\n",
    "\n",
    "# ✅ 你不想放入模型的變數\n",
    "DROP_FEATURES = {\"log1p_user_cnt\"}\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --------------------------\n",
    "# Helpers\n",
    "# --------------------------\n",
    "def rmse(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    return float(np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "def make_time_val_split(train_df, date_col, n_val_weeks=4):\n",
    "    \"\"\"\n",
    "    用 train 資料中最後 n_val_weeks 個 unique 週當 validation（時間切分）\n",
    "    \"\"\"\n",
    "    uniq_dates = np.array(sorted(pd.to_datetime(train_df[date_col]).dt.date.unique()))\n",
    "    if len(uniq_dates) <= 2:\n",
    "        return train_df.copy(), None\n",
    "\n",
    "    n_val_weeks = min(n_val_weeks, max(1, len(uniq_dates) // 5))\n",
    "    val_dates = set(uniq_dates[-n_val_weeks:])\n",
    "\n",
    "    tr = train_df[~pd.to_datetime(train_df[date_col]).dt.date.isin(val_dates)].copy()\n",
    "    va = train_df[pd.to_datetime(train_df[date_col]).dt.date.isin(val_dates)].copy()\n",
    "\n",
    "    # fallback：若切不到就改 80/20 by date\n",
    "    if tr.empty or va.empty:\n",
    "        cut = int(len(uniq_dates) * 0.8)\n",
    "        cut = max(1, min(cut, len(uniq_dates) - 1))\n",
    "        tr_dates = set(uniq_dates[:cut])\n",
    "        va_dates = set(uniq_dates[cut:])\n",
    "        tr = train_df[pd.to_datetime(train_df[date_col]).dt.date.isin(tr_dates)].copy()\n",
    "        va = train_df[pd.to_datetime(train_df[date_col]).dt.date.isin(va_dates)].copy()\n",
    "        if tr.empty or va.empty:\n",
    "            return train_df.copy(), None\n",
    "\n",
    "    return tr, va\n",
    "\n",
    "def drop_constant_features(train_df, feature_cols):\n",
    "    \"\"\"\n",
    "    移除在 train 期間完全沒有變化的欄位（控制組常見：treatment/source 全常數）\n",
    "    \"\"\"\n",
    "    keep = []\n",
    "    dropped = []\n",
    "    for c in feature_cols:\n",
    "        nun = train_df[c].nunique(dropna=False)\n",
    "        if nun <= 1:\n",
    "            dropped.append(c)\n",
    "        else:\n",
    "            keep.append(c)\n",
    "    return keep, dropped\n",
    "\n",
    "# --------------------------\n",
    "# Load & preprocess\n",
    "# --------------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "\n",
    "if df[DATE_COL].isna().any():\n",
    "    bad = df[df[DATE_COL].isna()].head(10)\n",
    "    raise ValueError(f\"Found unparsable dates in {DATE_COL}. Examples:\\n{bad}\")\n",
    "\n",
    "required_cols = CAT_COLS + [DATE_COL, Y1, Y2, Y1_LAG1, Y1_ROLL4, Y2_LAG1, Y2_ROLL4]\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "for c in CAT_COLS:\n",
    "    df[c] = df[c].astype(\"category\")\n",
    "\n",
    "# 用 week_idx 取代把日期直接丟進模型（比較穩）\n",
    "min_date = df[DATE_COL].min()\n",
    "df[\"week_idx\"] = ((df[DATE_COL] - min_date).dt.days // 7).astype(int)\n",
    "\n",
    "target_week = pd.to_datetime(TARGET_WEEK)\n",
    "\n",
    "# --------------------------\n",
    "# Split train / test (time-based)\n",
    "# --------------------------\n",
    "train = df[df[DATE_COL] < target_week].copy()\n",
    "test = df[df[DATE_COL] == target_week].copy()\n",
    "\n",
    "if train.empty:\n",
    "    raise ValueError(\"Train set is empty. TARGET_WEEK may be too early.\")\n",
    "if test.empty:\n",
    "    raise ValueError(f\"Test set is empty for week {TARGET_WEEK}. Check experiment_date values.\")\n",
    "\n",
    "print(\"=== Target week ===\", TARGET_WEEK)\n",
    "print(\"Train rows:\", len(train), \"| Test rows:\", len(test))\n",
    "print(\"Train weeks:\", train[DATE_COL].dt.date.nunique(), \"| Test weeks:\", test[DATE_COL].dt.date.nunique())\n",
    "print(\"Test ops_type_merged:\", test[\"ops_type_merged\"].nunique(), \"| Test city_group:\", test[\"city_group\"].nunique())\n",
    "\n",
    "# --------------------------\n",
    "# Baseline predictions (直接用 lag/roll 當預測)\n",
    "# --------------------------\n",
    "test[\"pred_y1_lag1\"] = test[Y1_LAG1].astype(float)\n",
    "test[\"pred_y1_roll4\"] = test[Y1_ROLL4].astype(float)\n",
    "test[\"pred_y2_lag1\"] = test[Y2_LAG1].astype(float)\n",
    "test[\"pred_y2_roll4\"] = test[Y2_ROLL4].astype(float)\n",
    "\n",
    "# --------------------------\n",
    "# Feature matrix: 用「除了 y 以外」全部欄位當特徵\n",
    "# 並把 experiment_date 拿掉（保留 week_idx）\n",
    "# ✅ 同時移除 log1p_user_cnt\n",
    "# ✅ 控制組額外：自動移除 train 期間的常數欄位\n",
    "# --------------------------\n",
    "exclude = {Y1, Y2} | set(DROP_FEATURES)\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in exclude]\n",
    "if DATE_COL in feature_cols:\n",
    "    feature_cols.remove(DATE_COL)\n",
    "\n",
    "# 控制組常見：treatment/source 之類可能全常數，先自動移掉\n",
    "feature_cols, dropped_const = drop_constant_features(train, feature_cols)\n",
    "if dropped_const:\n",
    "    print(\"\\n[Info] Dropped constant features in train:\", dropped_const)\n",
    "\n",
    "cat_features = [c for c in CAT_COLS if c in feature_cols]\n",
    "\n",
    "X_train = train[feature_cols].copy()\n",
    "X_test = test[feature_cols].copy()\n",
    "\n",
    "y1_train = train[Y1].astype(float).values\n",
    "y2_train = train[Y2].astype(float).values\n",
    "\n",
    "y1_true = test[Y1].astype(float).values\n",
    "y2_true = test[Y2].astype(float).values\n",
    "\n",
    "# --------------------------\n",
    "# Time-based validation for early stopping\n",
    "# --------------------------\n",
    "tr_df, va_df = make_time_val_split(train, DATE_COL, n_val_weeks=4)\n",
    "X_tr = tr_df[feature_cols].copy()\n",
    "y1_tr = tr_df[Y1].astype(float).values\n",
    "y2_tr = tr_df[Y2].astype(float).values\n",
    "\n",
    "X_va = y1_va = y2_va = None\n",
    "if va_df is not None:\n",
    "    X_va = va_df[feature_cols].copy()\n",
    "    y1_va = va_df[Y1].astype(float).values\n",
    "    y2_va = va_df[Y2].astype(float).values\n",
    "\n",
    "# --------------------------\n",
    "# Train LGBM (global) — 兩個 target 各一個模型\n",
    "# --------------------------\n",
    "import lightgbm as lgb\n",
    "\n",
    "common_params = dict(\n",
    "    objective=\"regression\",\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=RANDOM_STATE,\n",
    "    force_row_wise=True,\n",
    ")\n",
    "\n",
    "m_y1 = lgb.LGBMRegressor(**common_params)\n",
    "m_y2 = lgb.LGBMRegressor(**common_params)\n",
    "\n",
    "fit_kwargs = dict(categorical_feature=cat_features)\n",
    "\n",
    "if X_va is not None:\n",
    "    m_y1.fit(\n",
    "        X_tr, y1_tr,\n",
    "        eval_set=[(X_va, y1_va)],\n",
    "        eval_metric=\"l2\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)],\n",
    "        **fit_kwargs\n",
    "    )\n",
    "    m_y2.fit(\n",
    "        X_tr, y2_tr,\n",
    "        eval_set=[(X_va, y2_va)],\n",
    "        eval_metric=\"l2\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)],\n",
    "        **fit_kwargs\n",
    "    )\n",
    "else:\n",
    "    m_y1.fit(X_train, y1_train, **fit_kwargs)\n",
    "    m_y2.fit(X_train, y2_train, **fit_kwargs)\n",
    "\n",
    "# Predict on target week\n",
    "test[\"pred_y1_lgbm\"] = m_y1.predict(X_test)\n",
    "test[\"pred_y2_lgbm\"] = m_y2.predict(X_test)\n",
    "\n",
    "# --------------------------\n",
    "# Overall metrics\n",
    "# --------------------------\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\"target\": Y1, \"model\": \"baseline_lag1\",  \"MAE\": mae(y1_true, test[\"pred_y1_lag1\"]),   \"RMSE\": rmse(y1_true, test[\"pred_y1_lag1\"])},\n",
    "    {\"target\": Y1, \"model\": \"baseline_roll4\", \"MAE\": mae(y1_true, test[\"pred_y1_roll4\"]),  \"RMSE\": rmse(y1_true, test[\"pred_y1_roll4\"])},\n",
    "    {\"target\": Y1, \"model\": \"lgbm_global\",    \"MAE\": mae(y1_true, test[\"pred_y1_lgbm\"]),   \"RMSE\": rmse(y1_true, test[\"pred_y1_lgbm\"])},\n",
    "\n",
    "    {\"target\": Y2, \"model\": \"baseline_lag1\",  \"MAE\": mae(y2_true, test[\"pred_y2_lag1\"]),   \"RMSE\": rmse(y2_true, test[\"pred_y2_lag1\"])},\n",
    "    {\"target\": Y2, \"model\": \"baseline_roll4\", \"MAE\": mae(y2_true, test[\"pred_y2_roll4\"]),  \"RMSE\": rmse(y2_true, test[\"pred_y2_roll4\"])},\n",
    "    {\"target\": Y2, \"model\": \"lgbm_global\",    \"MAE\": mae(y2_true, test[\"pred_y2_lgbm\"]),   \"RMSE\": rmse(y2_true, test[\"pred_y2_lgbm\"])},\n",
    "]).sort_values([\"target\", \"model\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== Overall metrics (target week) ===\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# --------------------------\n",
    "# Metrics by ops_type_merged (diagnostic)\n",
    "# --------------------------\n",
    "rows = []\n",
    "for ops, g in test.groupby(\"ops_type_merged\"):\n",
    "    y1g = g[Y1].astype(float).values\n",
    "    y2g = g[Y2].astype(float).values\n",
    "    rows += [\n",
    "        {\"ops_type_merged\": ops, \"target\": Y1, \"model\": \"baseline_lag1\",  \"MAE\": mae(y1g, g[\"pred_y1_lag1\"]),   \"RMSE\": rmse(y1g, g[\"pred_y1_lag1\"]),   \"n\": len(g)},\n",
    "        {\"ops_type_merged\": ops, \"target\": Y1, \"model\": \"baseline_roll4\", \"MAE\": mae(y1g, g[\"pred_y1_roll4\"]),  \"RMSE\": rmse(y1g, g[\"pred_y1_roll4\"]),  \"n\": len(g)},\n",
    "        {\"ops_type_merged\": ops, \"target\": Y1, \"model\": \"lgbm_global\",    \"MAE\": mae(y1g, g[\"pred_y1_lgbm\"]),   \"RMSE\": rmse(y1g, g[\"pred_y1_lgbm\"]),   \"n\": len(g)},\n",
    "\n",
    "        {\"ops_type_merged\": ops, \"target\": Y2, \"model\": \"baseline_lag1\",  \"MAE\": mae(y2g, g[\"pred_y2_lag1\"]),   \"RMSE\": rmse(y2g, g[\"pred_y2_lag1\"]),   \"n\": len(g)},\n",
    "        {\"ops_type_merged\": ops, \"target\": Y2, \"model\": \"baseline_roll4\", \"MAE\": mae(y2g, g[\"pred_y2_roll4\"]),  \"RMSE\": rmse(y2g, g[\"pred_y2_roll4\"]),  \"n\": len(g)},\n",
    "        {\"ops_type_merged\": ops, \"target\": Y2, \"model\": \"lgbm_global\",    \"MAE\": mae(y2g, g[\"pred_y2_lgbm\"]),   \"RMSE\": rmse(y2g, g[\"pred_y2_lgbm\"]),   \"n\": len(g)},\n",
    "    ]\n",
    "\n",
    "metrics_by_ops_df = pd.DataFrame(rows).sort_values([\"target\",\"ops_type_merged\",\"model\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== Metrics by ops_type_merged (target week) ===\")\n",
    "print(metrics_by_ops_df.to_string(index=False))\n",
    "\n",
    "# --------------------------\n",
    "# Metrics by city_group (diagnostic) — 控制組很重要\n",
    "# --------------------------\n",
    "rows = []\n",
    "for cg, g in test.groupby(\"city_group\"):\n",
    "    y1g = g[Y1].astype(float).values\n",
    "    y2g = g[Y2].astype(float).values\n",
    "    rows += [\n",
    "        {\"city_group\": cg, \"target\": Y1, \"model\": \"baseline_lag1\",  \"MAE\": mae(y1g, g[\"pred_y1_lag1\"]),  \"RMSE\": rmse(y1g, g[\"pred_y1_lag1\"]),  \"n\": len(g)},\n",
    "        {\"city_group\": cg, \"target\": Y1, \"model\": \"baseline_roll4\", \"MAE\": mae(y1g, g[\"pred_y1_roll4\"]), \"RMSE\": rmse(y1g, g[\"pred_y1_roll4\"]), \"n\": len(g)},\n",
    "        {\"city_group\": cg, \"target\": Y1, \"model\": \"lgbm_global\",    \"MAE\": mae(y1g, g[\"pred_y1_lgbm\"]),  \"RMSE\": rmse(y1g, g[\"pred_y1_lgbm\"]),  \"n\": len(g)},\n",
    "\n",
    "        {\"city_group\": cg, \"target\": Y2, \"model\": \"baseline_lag1\",  \"MAE\": mae(y2g, g[\"pred_y2_lag1\"]),  \"RMSE\": rmse(y2g, g[\"pred_y2_lag1\"]),  \"n\": len(g)},\n",
    "        {\"city_group\": cg, \"target\": Y2, \"model\": \"baseline_roll4\", \"MAE\": mae(y2g, g[\"pred_y2_roll4\"]), \"RMSE\": rmse(y2g, g[\"pred_y2_roll4\"]), \"n\": len(g)},\n",
    "        {\"city_group\": cg, \"target\": Y2, \"model\": \"lgbm_global\",    \"MAE\": mae(y2g, g[\"pred_y2_lgbm\"]),  \"RMSE\": rmse(y2g, g[\"pred_y2_lgbm\"]),  \"n\": len(g)},\n",
    "    ]\n",
    "\n",
    "metrics_by_city_df = pd.DataFrame(rows).sort_values([\"target\",\"city_group\",\"model\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== Metrics by city_group (target week) ===\")\n",
    "print(metrics_by_city_df.to_string(index=False))\n",
    "\n",
    "# --------------------------\n",
    "# Prediction preview (first 30 rows)\n",
    "# --------------------------\n",
    "id_cols = [DATE_COL, \"ops_type_merged\", \"city_group\"]\n",
    "preview_cols = id_cols + [\n",
    "    Y1, \"pred_y1_lag1\", \"pred_y1_roll4\", \"pred_y1_lgbm\",\n",
    "    Y2, \"pred_y2_lag1\", \"pred_y2_roll4\", \"pred_y2_lgbm\",\n",
    "]\n",
    "print(\"\\n=== Prediction preview (first 30 rows) ===\")\n",
    "print(test[preview_cols].head(30).to_string(index=False))\n",
    "\n",
    "# --------------------------\n",
    "# Quick aggregates (means)\n",
    "# --------------------------\n",
    "agg = pd.DataFrame({\n",
    "    \"metric\": [\"mean_true\", \"mean_pred_lag1\", \"mean_pred_roll4\", \"mean_pred_lgbm\"],\n",
    "    \"trip_cnt_per_user\": [\n",
    "        float(np.mean(test[Y1])),\n",
    "        float(np.mean(test[\"pred_y1_lag1\"])),\n",
    "        float(np.mean(test[\"pred_y1_roll4\"])),\n",
    "        float(np.mean(test[\"pred_y1_lgbm\"])),\n",
    "    ],\n",
    "    \"nonrepeat_cnt_per_user\": [\n",
    "        float(np.mean(test[Y2])),\n",
    "        float(np.mean(test[\"pred_y2_lag1\"])),\n",
    "        float(np.mean(test[\"pred_y2_roll4\"])),\n",
    "        float(np.mean(test[\"pred_y2_lgbm\"])),\n",
    "    ],\n",
    "})\n",
    "print(\"\\n=== Quick aggregates (means) ===\")\n",
    "print(agg.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d1b8d5",
   "metadata": {},
   "source": [
    "# 調整參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2fede15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total param sets: 144\n",
      "\n",
      "=== Stage 2 results (top 30 per target) ===\n",
      " param_id                 target  mean_RMSE  std_RMSE  median_RMSE  best_iter_mean                                                                    params\n",
      "       21 nonrepeat_cnt_per_user   0.036502  0.009671     0.037635           165.5  {'mcs': 50, 'min_split_gain': 0.01, 'reg_lambda': 5.0, 'subsample': 0.9}\n",
      "       24 nonrepeat_cnt_per_user   0.036505  0.009440     0.037174           178.8 {'mcs': 50, 'min_split_gain': 0.01, 'reg_lambda': 10.0, 'subsample': 0.9}\n",
      "       15 nonrepeat_cnt_per_user   0.036574  0.009464     0.037847           156.4  {'mcs': 50, 'min_split_gain': 0.01, 'reg_lambda': 1.0, 'subsample': 0.9}\n",
      "       18 nonrepeat_cnt_per_user   0.036592  0.009570     0.037392           174.8  {'mcs': 50, 'min_split_gain': 0.01, 'reg_lambda': 2.0, 'subsample': 0.9}\n",
      "       12 nonrepeat_cnt_per_user   0.036606  0.009733     0.038280           219.5  {'mcs': 50, 'min_split_gain': 0.0, 'reg_lambda': 10.0, 'subsample': 0.9}\n",
      "        6 nonrepeat_cnt_per_user   0.037026  0.010254     0.038874           164.0   {'mcs': 50, 'min_split_gain': 0.0, 'reg_lambda': 2.0, 'subsample': 0.9}\n",
      "        9 nonrepeat_cnt_per_user   0.037043  0.010122     0.038770           195.1   {'mcs': 50, 'min_split_gain': 0.0, 'reg_lambda': 5.0, 'subsample': 0.9}\n",
      "       14 nonrepeat_cnt_per_user   0.037093  0.009969     0.038436           160.9  {'mcs': 50, 'min_split_gain': 0.01, 'reg_lambda': 1.0, 'subsample': 0.8}\n",
      "       17 nonrepeat_cnt_per_user   0.037178  0.009909     0.037942           175.8  {'mcs': 50, 'min_split_gain': 0.01, 'reg_lambda': 2.0, 'subsample': 0.8}\n",
      "       11 nonrepeat_cnt_per_user   0.037304  0.010523     0.039243           230.6  {'mcs': 50, 'min_split_gain': 0.0, 'reg_lambda': 10.0, 'subsample': 0.8}\n",
      "       23 nonrepeat_cnt_per_user   0.037478  0.010096     0.037296           156.9 {'mcs': 50, 'min_split_gain': 0.01, 'reg_lambda': 10.0, 'subsample': 0.8}\n",
      "       20 nonrepeat_cnt_per_user   0.037488  0.009893     0.038584           177.4  {'mcs': 50, 'min_split_gain': 0.01, 'reg_lambda': 5.0, 'subsample': 0.8}\n",
      "       50 nonrepeat_cnt_per_user   0.037516  0.009659     0.036722           315.4   {'mcs': 80, 'min_split_gain': 0.0, 'reg_lambda': 1.0, 'subsample': 0.8}\n",
      "       51 nonrepeat_cnt_per_user   0.037579  0.009292     0.035723           289.8   {'mcs': 80, 'min_split_gain': 0.0, 'reg_lambda': 1.0, 'subsample': 0.9}\n",
      "       54 nonrepeat_cnt_per_user   0.037616  0.009481     0.036353           235.3   {'mcs': 80, 'min_split_gain': 0.0, 'reg_lambda': 2.0, 'subsample': 0.9}\n",
      "        5 nonrepeat_cnt_per_user   0.037624  0.010549     0.038695           170.4   {'mcs': 50, 'min_split_gain': 0.0, 'reg_lambda': 2.0, 'subsample': 0.8}\n",
      "       98 nonrepeat_cnt_per_user   0.037658  0.008745     0.036932           248.2  {'mcs': 100, 'min_split_gain': 0.0, 'reg_lambda': 1.0, 'subsample': 0.8}\n",
      "       57 nonrepeat_cnt_per_user   0.037658  0.009652     0.036160           263.6   {'mcs': 80, 'min_split_gain': 0.0, 'reg_lambda': 5.0, 'subsample': 0.9}\n",
      "        3 nonrepeat_cnt_per_user   0.037659  0.010453     0.039613           163.9   {'mcs': 50, 'min_split_gain': 0.0, 'reg_lambda': 1.0, 'subsample': 0.9}\n",
      "       53 nonrepeat_cnt_per_user   0.037717  0.009703     0.036789           311.1   {'mcs': 80, 'min_split_gain': 0.0, 'reg_lambda': 2.0, 'subsample': 0.8}\n",
      "       63 nonrepeat_cnt_per_user   0.037736  0.010156     0.038007           181.6  {'mcs': 80, 'min_split_gain': 0.01, 'reg_lambda': 1.0, 'subsample': 0.9}\n",
      "      107 nonrepeat_cnt_per_user   0.037749  0.008764     0.036838           308.6 {'mcs': 100, 'min_split_gain': 0.0, 'reg_lambda': 10.0, 'subsample': 0.8}\n",
      "       60 nonrepeat_cnt_per_user   0.037755  0.009748     0.036245           293.4  {'mcs': 80, 'min_split_gain': 0.0, 'reg_lambda': 10.0, 'subsample': 0.9}\n",
      "      101 nonrepeat_cnt_per_user   0.037759  0.008770     0.036629           242.3  {'mcs': 100, 'min_split_gain': 0.0, 'reg_lambda': 2.0, 'subsample': 0.8}\n",
      "       99 nonrepeat_cnt_per_user   0.037781  0.008617     0.037477           381.1  {'mcs': 100, 'min_split_gain': 0.0, 'reg_lambda': 1.0, 'subsample': 0.9}\n",
      "       66 nonrepeat_cnt_per_user   0.037808  0.010124     0.038440           152.0  {'mcs': 80, 'min_split_gain': 0.01, 'reg_lambda': 2.0, 'subsample': 0.9}\n",
      "       56 nonrepeat_cnt_per_user   0.037812  0.009667     0.037292           310.2   {'mcs': 80, 'min_split_gain': 0.0, 'reg_lambda': 5.0, 'subsample': 0.8}\n",
      "      104 nonrepeat_cnt_per_user   0.037815  0.008723     0.036724           264.2  {'mcs': 100, 'min_split_gain': 0.0, 'reg_lambda': 5.0, 'subsample': 0.8}\n",
      "      108 nonrepeat_cnt_per_user   0.037860  0.008782     0.037193           365.9 {'mcs': 100, 'min_split_gain': 0.0, 'reg_lambda': 10.0, 'subsample': 0.9}\n",
      "      102 nonrepeat_cnt_per_user   0.037866  0.008487     0.037517           389.9  {'mcs': 100, 'min_split_gain': 0.0, 'reg_lambda': 2.0, 'subsample': 0.9}\n",
      "       54      trip_cnt_per_user   0.025831  0.007816     0.023835           367.6   {'mcs': 80, 'min_split_gain': 0.0, 'reg_lambda': 2.0, 'subsample': 0.9}\n",
      "      107      trip_cnt_per_user   0.025875  0.007710     0.023585           272.5 {'mcs': 100, 'min_split_gain': 0.0, 'reg_lambda': 10.0, 'subsample': 0.8}\n",
      "      101      trip_cnt_per_user   0.025905  0.007756     0.023507           273.6  {'mcs': 100, 'min_split_gain': 0.0, 'reg_lambda': 2.0, 'subsample': 0.8}\n",
      "      104      trip_cnt_per_user   0.025921  0.007762     0.023603           280.3  {'mcs': 100, 'min_split_gain': 0.0, 'reg_lambda': 5.0, 'subsample': 0.8}\n",
      "       57      trip_cnt_per_user   0.025941  0.007787     0.024204           309.4   {'mcs': 80, 'min_split_gain': 0.0, 'reg_lambda': 5.0, 'subsample': 0.9}\n",
      "      108      trip_cnt_per_user   0.025981  0.008331     0.024410           297.2 {'mcs': 100, 'min_split_gain': 0.0, 'reg_lambda': 10.0, 'subsample': 0.9}\n",
      "       60      trip_cnt_per_user   0.026026  0.007752     0.024231           364.3  {'mcs': 80, 'min_split_gain': 0.0, 'reg_lambda': 10.0, 'subsample': 0.9}\n",
      "      102      trip_cnt_per_user   0.026038  0.008292     0.024621           332.0  {'mcs': 100, 'min_split_gain': 0.0, 'reg_lambda': 2.0, 'subsample': 0.9}\n",
      "       98      trip_cnt_per_user   0.026052  0.007776     0.023809           251.9  {'mcs': 100, 'min_split_gain': 0.0, 'reg_lambda': 1.0, 'subsample': 0.8}\n",
      "       51      trip_cnt_per_user   0.026180  0.007896     0.024563           269.9   {'mcs': 80, 'min_split_gain': 0.0, 'reg_lambda': 1.0, 'subsample': 0.9}\n",
      "       11      trip_cnt_per_user   0.026219  0.007840     0.024018           230.7  {'mcs': 50, 'min_split_gain': 0.0, 'reg_lambda': 10.0, 'subsample': 0.8}\n",
      "        3      trip_cnt_per_user   0.026232  0.007860     0.025492           200.5   {'mcs': 50, 'min_split_gain': 0.0, 'reg_lambda': 1.0, 'subsample': 0.9}\n",
      "        9      trip_cnt_per_user   0.026235  0.007800     0.024797           175.3   {'mcs': 50, 'min_split_gain': 0.0, 'reg_lambda': 5.0, 'subsample': 0.9}\n",
      "       12      trip_cnt_per_user   0.026303  0.008060     0.024123           205.6  {'mcs': 50, 'min_split_gain': 0.0, 'reg_lambda': 10.0, 'subsample': 0.9}\n",
      "       50      trip_cnt_per_user   0.026313  0.007987     0.024814           347.8   {'mcs': 80, 'min_split_gain': 0.0, 'reg_lambda': 1.0, 'subsample': 0.8}\n",
      "        6      trip_cnt_per_user   0.026318  0.007671     0.025289           225.2   {'mcs': 50, 'min_split_gain': 0.0, 'reg_lambda': 2.0, 'subsample': 0.9}\n",
      "      105      trip_cnt_per_user   0.026342  0.007873     0.024402           406.5  {'mcs': 100, 'min_split_gain': 0.0, 'reg_lambda': 5.0, 'subsample': 0.9}\n",
      "       56      trip_cnt_per_user   0.026347  0.007893     0.024234           361.9   {'mcs': 80, 'min_split_gain': 0.0, 'reg_lambda': 5.0, 'subsample': 0.8}\n",
      "       53      trip_cnt_per_user   0.026366  0.008148     0.024227           443.1   {'mcs': 80, 'min_split_gain': 0.0, 'reg_lambda': 2.0, 'subsample': 0.8}\n",
      "       59      trip_cnt_per_user   0.026385  0.007906     0.023930           403.9  {'mcs': 80, 'min_split_gain': 0.0, 'reg_lambda': 10.0, 'subsample': 0.8}\n",
      "        5      trip_cnt_per_user   0.026413  0.007420     0.024704           178.5   {'mcs': 50, 'min_split_gain': 0.0, 'reg_lambda': 2.0, 'subsample': 0.8}\n",
      "       18      trip_cnt_per_user   0.026438  0.008114     0.026382           181.8  {'mcs': 50, 'min_split_gain': 0.01, 'reg_lambda': 2.0, 'subsample': 0.9}\n",
      "       99      trip_cnt_per_user   0.026470  0.008005     0.024464           399.6  {'mcs': 100, 'min_split_gain': 0.0, 'reg_lambda': 1.0, 'subsample': 0.9}\n",
      "        8      trip_cnt_per_user   0.026560  0.007748     0.024868           219.8   {'mcs': 50, 'min_split_gain': 0.0, 'reg_lambda': 5.0, 'subsample': 0.8}\n",
      "       21      trip_cnt_per_user   0.026570  0.008139     0.026208           164.2  {'mcs': 50, 'min_split_gain': 0.01, 'reg_lambda': 5.0, 'subsample': 0.9}\n",
      "       14      trip_cnt_per_user   0.026590  0.008189     0.027441           210.0  {'mcs': 50, 'min_split_gain': 0.01, 'reg_lambda': 1.0, 'subsample': 0.8}\n",
      "       23      trip_cnt_per_user   0.026621  0.008299     0.026935           228.8 {'mcs': 50, 'min_split_gain': 0.01, 'reg_lambda': 10.0, 'subsample': 0.8}\n",
      "       20      trip_cnt_per_user   0.026641  0.008349     0.026947           204.0  {'mcs': 50, 'min_split_gain': 0.01, 'reg_lambda': 5.0, 'subsample': 0.8}\n",
      "       15      trip_cnt_per_user   0.026642  0.008109     0.026650           168.9  {'mcs': 50, 'min_split_gain': 0.01, 'reg_lambda': 1.0, 'subsample': 0.9}\n",
      "        2      trip_cnt_per_user   0.026684  0.007369     0.025035           204.2   {'mcs': 50, 'min_split_gain': 0.0, 'reg_lambda': 1.0, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from itertools import product\n",
    "\n",
    "# ============= Config =============\n",
    "DATA_PATH = \"../../final_data/data_260119_control.csv\"\n",
    "DATE_COL = \"experiment_date\"\n",
    "CAT_COLS = [\"ops_type_merged\", \"city_group\"]\n",
    "Y1 = \"trip_cnt_per_user\"\n",
    "Y2 = \"nonrepeat_cnt_per_user\"\n",
    "DROP_FEATURES = {\"log1p_user_cnt\"}\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_TEST_WEEKS = 10\n",
    "N_VAL_WEEKS = 4\n",
    "EARLY_STOP_ROUNDS = 100\n",
    "MIN_TRAIN_WEEKS = 12\n",
    "DROP_CONSTANT_FEATURES = True\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "def make_time_val_split(train_df, date_col, n_val_weeks=4):\n",
    "    uniq_dates = np.array(sorted(pd.to_datetime(train_df[date_col]).dt.date.unique()))\n",
    "    if len(uniq_dates) <= 2:\n",
    "        return train_df.copy(), None\n",
    "    n_val_weeks = min(n_val_weeks, max(1, len(uniq_dates) // 5))\n",
    "    val_dates = set(uniq_dates[-n_val_weeks:])\n",
    "    tr = train_df[~pd.to_datetime(train_df[date_col]).dt.date.isin(val_dates)].copy()\n",
    "    va = train_df[pd.to_datetime(train_df[date_col]).dt.date.isin(val_dates)].copy()\n",
    "    if tr.empty or va.empty:\n",
    "        return train_df.copy(), None\n",
    "    return tr, va\n",
    "\n",
    "def get_feature_cols(df):\n",
    "    exclude = {Y1, Y2} | set(DROP_FEATURES)\n",
    "    cols = [c for c in df.columns if c not in exclude]\n",
    "    if DATE_COL in cols:\n",
    "        cols.remove(DATE_COL)\n",
    "    return cols\n",
    "\n",
    "def drop_constant_features(train_df, feature_cols):\n",
    "    return [c for c in feature_cols if train_df[c].nunique(dropna=False) > 1]\n",
    "\n",
    "# ============= Load =============\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "for c in CAT_COLS:\n",
    "    df[c] = df[c].astype(\"category\")\n",
    "min_date = df[DATE_COL].min()\n",
    "df[\"week_idx\"] = ((df[DATE_COL] - min_date).dt.days // 7).astype(int)\n",
    "\n",
    "feature_cols_global = get_feature_cols(df)\n",
    "all_weeks = np.array(sorted(df[DATE_COL].dt.date.unique()))\n",
    "test_weeks = all_weeks[-N_TEST_WEEKS:]\n",
    "\n",
    "# ============= Stage 2 grid (controlled size) =============\n",
    "mcs_list = [50, 80, 100]                # 你也可加 30/120 當對照\n",
    "min_split_gain_list = [0.0, 0.01, 0.05, 0.1]\n",
    "reg_lambda_list = [1.0, 2.0, 5.0, 10.0]\n",
    "subsample_list = [0.7, 0.8, 0.9]        # 先只掃 subsample；colsample 固定 0.8\n",
    "\n",
    "param_grid = list(product(mcs_list, min_split_gain_list, reg_lambda_list, subsample_list))\n",
    "print(\"Total param sets:\", len(param_grid))\n",
    "\n",
    "base_params = dict(\n",
    "    objective=\"regression\",\n",
    "    metric=\"l2\",\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    max_depth=6,\n",
    "    colsample_bytree=0.8,\n",
    "    bagging_freq=1,\n",
    "    reg_alpha=0.0,\n",
    "    seed=RANDOM_STATE,\n",
    "    force_row_wise=True,\n",
    "    verbosity=-1,\n",
    "    num_iterations=3000,\n",
    ")\n",
    "\n",
    "results = []\n",
    "\n",
    "for pid, (mcs, msg, rl, subs) in enumerate(param_grid, start=1):\n",
    "    params = {\n",
    "        **base_params,\n",
    "        \"min_child_samples\": mcs,\n",
    "        \"min_split_gain\": msg,\n",
    "        \"reg_lambda\": rl,\n",
    "        \"subsample\": subs,\n",
    "    }\n",
    "\n",
    "    rmse_y1, rmse_y2 = [], []\n",
    "    bestit_y1, bestit_y2 = [], []\n",
    "\n",
    "    for tw in test_weeks:\n",
    "        tw = pd.to_datetime(tw)\n",
    "        train_df = df[df[DATE_COL] < tw].copy()\n",
    "        test_df  = df[df[DATE_COL] == tw].copy()\n",
    "        if train_df.empty or test_df.empty:\n",
    "            continue\n",
    "        if train_df[DATE_COL].dt.date.nunique() < MIN_TRAIN_WEEKS:\n",
    "            continue\n",
    "\n",
    "        cur_cols = feature_cols_global\n",
    "        if DROP_CONSTANT_FEATURES:\n",
    "            cur_cols = drop_constant_features(train_df, feature_cols_global)\n",
    "\n",
    "        tr_df, va_df = make_time_val_split(train_df, DATE_COL, n_val_weeks=N_VAL_WEEKS)\n",
    "\n",
    "        X_tr = tr_df[cur_cols]\n",
    "        X_te = test_df[cur_cols]\n",
    "        y1_tr = tr_df[Y1].astype(float).values\n",
    "        y2_tr = tr_df[Y2].astype(float).values\n",
    "        y1_te = test_df[Y1].astype(float).values\n",
    "        y2_te = test_df[Y2].astype(float).values\n",
    "\n",
    "        dtrain1 = lgb.Dataset(X_tr, label=y1_tr, free_raw_data=True)\n",
    "        dtrain2 = lgb.Dataset(X_tr, label=y2_tr, free_raw_data=True)\n",
    "\n",
    "        valid_sets1 = [dtrain1]\n",
    "        valid_sets2 = [dtrain2]\n",
    "\n",
    "        if va_df is not None and (not va_df.empty):\n",
    "            X_va = va_df[cur_cols]\n",
    "            y1_va = va_df[Y1].astype(float).values\n",
    "            y2_va = va_df[Y2].astype(float).values\n",
    "            dvalid1 = lgb.Dataset(X_va, label=y1_va, reference=dtrain1, free_raw_data=True)\n",
    "            dvalid2 = lgb.Dataset(X_va, label=y2_va, reference=dtrain2, free_raw_data=True)\n",
    "            valid_sets1 = [dtrain1, dvalid1]\n",
    "            valid_sets2 = [dtrain2, dvalid2]\n",
    "\n",
    "        booster1 = lgb.train(params, dtrain1, valid_sets=valid_sets1,\n",
    "                             callbacks=[lgb.early_stopping(EARLY_STOP_ROUNDS, verbose=False)])\n",
    "        booster2 = lgb.train(params, dtrain2, valid_sets=valid_sets2,\n",
    "                             callbacks=[lgb.early_stopping(EARLY_STOP_ROUNDS, verbose=False)])\n",
    "\n",
    "        p1 = booster1.predict(X_te, num_iteration=booster1.best_iteration)\n",
    "        p2 = booster2.predict(X_te, num_iteration=booster2.best_iteration)\n",
    "\n",
    "        rmse_y1.append(rmse(y1_te, p1))\n",
    "        rmse_y2.append(rmse(y2_te, p2))\n",
    "        bestit_y1.append(booster1.best_iteration)\n",
    "        bestit_y2.append(booster2.best_iteration)\n",
    "\n",
    "        del booster1, booster2, dtrain1, dtrain2\n",
    "        if \"dvalid1\" in locals(): del dvalid1\n",
    "        if \"dvalid2\" in locals(): del dvalid2\n",
    "        gc.collect()\n",
    "\n",
    "    def summarize(x):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        return float(np.mean(x)), float(np.std(x)), float(np.median(x))\n",
    "\n",
    "    y1_mean, y1_std, y1_med = summarize(rmse_y1) if rmse_y1 else (np.nan, np.nan, np.nan)\n",
    "    y2_mean, y2_std, y2_med = summarize(rmse_y2) if rmse_y2 else (np.nan, np.nan, np.nan)\n",
    "\n",
    "    results.append({\n",
    "        \"param_id\": pid,\n",
    "        \"target\": Y1,\n",
    "        \"mean_RMSE\": y1_mean,\n",
    "        \"std_RMSE\": y1_std,\n",
    "        \"median_RMSE\": y1_med,\n",
    "        \"best_iter_mean\": float(np.mean(bestit_y1)) if bestit_y1 else np.nan,\n",
    "        \"params\": {\"mcs\": mcs, \"min_split_gain\": msg, \"reg_lambda\": rl, \"subsample\": subs},\n",
    "    })\n",
    "    results.append({\n",
    "        \"param_id\": pid,\n",
    "        \"target\": Y2,\n",
    "        \"mean_RMSE\": y2_mean,\n",
    "        \"std_RMSE\": y2_std,\n",
    "        \"median_RMSE\": y2_med,\n",
    "        \"best_iter_mean\": float(np.mean(bestit_y2)) if bestit_y2 else np.nan,\n",
    "        \"params\": {\"mcs\": mcs, \"min_split_gain\": msg, \"reg_lambda\": rl, \"subsample\": subs},\n",
    "    })\n",
    "\n",
    "res_df = pd.DataFrame(results).sort_values([\"target\", \"mean_RMSE\"]).reset_index(drop=True)\n",
    "print(\"\\n=== Stage 2 results (top 30 per target) ===\")\n",
    "print(res_df.groupby(\"target\").head(30).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "164fd8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Rolling backtest test weeks ===\n",
      "['2025-11-03', '2025-11-10', '2025-11-17', '2025-11-24', '2025-12-01', '2025-12-08', '2025-12-15', '2025-12-22', '2025-12-29', '2026-01-05']\n",
      "\n",
      "=== Baseline summary over last 10 weeks ===\n",
      "                target          model     level_2     RMSE\n",
      "nonrepeat_cnt_per_user  baseline_lag1   mean_RMSE 0.046217\n",
      "nonrepeat_cnt_per_user  baseline_lag1    std_RMSE 0.012741\n",
      "nonrepeat_cnt_per_user  baseline_lag1 median_RMSE 0.044246\n",
      "nonrepeat_cnt_per_user baseline_roll4   mean_RMSE 0.036673\n",
      "nonrepeat_cnt_per_user baseline_roll4    std_RMSE 0.012511\n",
      "nonrepeat_cnt_per_user baseline_roll4 median_RMSE 0.035774\n",
      "     trip_cnt_per_user  baseline_lag1   mean_RMSE 0.031468\n",
      "     trip_cnt_per_user  baseline_lag1    std_RMSE 0.009583\n",
      "     trip_cnt_per_user  baseline_lag1 median_RMSE 0.029976\n",
      "     trip_cnt_per_user baseline_roll4   mean_RMSE 0.025142\n",
      "     trip_cnt_per_user baseline_roll4    std_RMSE 0.007925\n",
      "     trip_cnt_per_user baseline_roll4 median_RMSE 0.024099\n",
      "\n",
      "=== Candidate models: 10-week rolling backtest summary ===\n",
      "                target                model  mean_RMSE  std_RMSE  median_RMSE  best_iter_mean  n_weeks_used                                                                    params\n",
      "nonrepeat_cnt_per_user NR_best(Stage2_id21)   0.036502  0.009671     0.037635           165.5            10  {'mcs': 50, 'min_split_gain': 0.01, 'reg_lambda': 5.0, 'subsample': 0.9}\n",
      "nonrepeat_cnt_per_user   NR_close(lambda10)   0.036505  0.009440     0.037174           178.8            10 {'mcs': 50, 'min_split_gain': 0.01, 'reg_lambda': 10.0, 'subsample': 0.9}\n",
      "nonrepeat_cnt_per_user TR_best(Stage2_id54)   0.037616  0.009481     0.036353           235.3            10   {'mcs': 80, 'min_split_gain': 0.0, 'reg_lambda': 2.0, 'subsample': 0.9}\n",
      "nonrepeat_cnt_per_user       NR_stage1_base   0.037624  0.010549     0.038695           170.4            10   {'mcs': 50, 'min_split_gain': 0.0, 'reg_lambda': 2.0, 'subsample': 0.8}\n",
      "nonrepeat_cnt_per_user TR_close(mcs100_l10)   0.037749  0.008764     0.036838           308.6            10 {'mcs': 100, 'min_split_gain': 0.0, 'reg_lambda': 10.0, 'subsample': 0.8}\n",
      "nonrepeat_cnt_per_user       TR_stage1_base   0.037759  0.008770     0.036629           242.3            10  {'mcs': 100, 'min_split_gain': 0.0, 'reg_lambda': 2.0, 'subsample': 0.8}\n",
      "     trip_cnt_per_user TR_best(Stage2_id54)   0.025831  0.007816     0.023835           367.6            10   {'mcs': 80, 'min_split_gain': 0.0, 'reg_lambda': 2.0, 'subsample': 0.9}\n",
      "     trip_cnt_per_user TR_close(mcs100_l10)   0.025875  0.007710     0.023585           272.5            10 {'mcs': 100, 'min_split_gain': 0.0, 'reg_lambda': 10.0, 'subsample': 0.8}\n",
      "     trip_cnt_per_user       TR_stage1_base   0.025905  0.007756     0.023507           273.6            10  {'mcs': 100, 'min_split_gain': 0.0, 'reg_lambda': 2.0, 'subsample': 0.8}\n",
      "     trip_cnt_per_user       NR_stage1_base   0.026413  0.007420     0.024704           178.5            10   {'mcs': 50, 'min_split_gain': 0.0, 'reg_lambda': 2.0, 'subsample': 0.8}\n",
      "     trip_cnt_per_user NR_best(Stage2_id21)   0.026570  0.008139     0.026208           164.2            10  {'mcs': 50, 'min_split_gain': 0.01, 'reg_lambda': 5.0, 'subsample': 0.9}\n",
      "     trip_cnt_per_user   NR_close(lambda10)   0.026759  0.008226     0.026251           176.9            10 {'mcs': 50, 'min_split_gain': 0.01, 'reg_lambda': 10.0, 'subsample': 0.9}\n",
      "\n",
      "=== Candidate models: RMSE by week (for inspection) ===\n",
      "                target                model                                                                                                                                                                                                              rmse_by_week\n",
      "nonrepeat_cnt_per_user NR_best(Stage2_id21) [0.046582555848533866, 0.04270538216257974, 0.040877835200384356, 0.033716391912016065, 0.017943981143353986, 0.03917200822413135, 0.025968801418960706, 0.052501619752931675, 0.02945548465222832, 0.036097736074439606]\n",
      "nonrepeat_cnt_per_user   NR_close(lambda10)     [0.04798297986823923, 0.04301557776742862, 0.03945211640604233, 0.0346414317043357, 0.018705447088832507, 0.03847586382259898, 0.025651295553561333, 0.05139529114793069, 0.029858856391669755, 0.035872847558982957]\n",
      "nonrepeat_cnt_per_user TR_best(Stage2_id54)     [0.05289319102572289, 0.04442886553236798, 0.041230741267578294, 0.03169561781786236, 0.022002544970347596, 0.03412234181049054, 0.027171414112707147, 0.05113063682313031, 0.03289927899301944, 0.03858414780535847]\n",
      "nonrepeat_cnt_per_user       NR_stage1_base [0.04629426853082366, 0.048341017717533866, 0.04106561631987499, 0.031914219077775696, 0.01655481028789955, 0.041318464722230615, 0.025568338663331556, 0.053945747001116366, 0.034917797514375445, 0.036323716500926494]\n",
      "nonrepeat_cnt_per_user TR_close(mcs100_l10)        [0.05370242069780729, 0.0459228846954162, 0.0357807225525126, 0.031436397590391615, 0.02305741141955782, 0.03875842468348198, 0.028475835427917116, 0.04738918477486706, 0.03789466943139988, 0.03507269300498548]\n",
      "nonrepeat_cnt_per_user       TR_stage1_base  [0.053934707223567156, 0.045848899892363924, 0.03618246932125931, 0.03241905983638291, 0.023086081489043543, 0.039056439178629714, 0.028501192447292944, 0.047456632863805415, 0.03707464195470288, 0.03403010190668891]\n",
      "     trip_cnt_per_user TR_best(Stage2_id54)  [0.04051329377358938, 0.035377769102570504, 0.028287107792213984, 0.022975752734796066, 0.017599009491540425, 0.01844246176159953, 0.020280138003487148, 0.03326544087466949, 0.01687363968881557, 0.024694491115638335]\n",
      "     trip_cnt_per_user TR_close(mcs100_l10)  [0.04033685197334384, 0.03590874081347356, 0.02503582640789417, 0.024355944653407735, 0.018705554565563942, 0.022813756973927245, 0.018881060974862286, 0.03420223781830464, 0.016513335819873805, 0.022000262555631173]\n",
      "     trip_cnt_per_user       TR_stage1_base     [0.0404855286252132, 0.0359902435265084, 0.024951551913553912, 0.02446587489412297, 0.018416746538128543, 0.022547978448330862, 0.019139997281371317, 0.03428146071501138, 0.01655706467427538, 0.022215028732987585]\n",
      "     trip_cnt_per_user       NR_stage1_base   [0.036190412316181295, 0.03763304687355391, 0.028995214482693786, 0.023869751604398734, 0.017457616904125197, 0.02209083377373666, 0.01898628901816479, 0.03565709637066779, 0.017716374361143983, 0.02553780881660842]\n",
      "     trip_cnt_per_user NR_best(Stage2_id21)  [0.03677476309495631, 0.039112770743888234, 0.028105136200804633, 0.0270934756185252, 0.01643978216115614, 0.021625231559142564, 0.018788894040332866, 0.036439400034447685, 0.016001076167929267, 0.025322847777406537]\n",
      "     trip_cnt_per_user   NR_close(lambda10)    [0.03736155013967878, 0.03911377725098769, 0.028558860103372444, 0.027501580688383872, 0.016549931647659513, 0.021645250639377372, 0.019014337118550002, 0.03676708882769537, 0.0160729070832508, 0.02500116480822839]\n",
      "\n",
      "=== Ranking (lower mean_RMSE is better) ===\n",
      "\n",
      "Target = trip_cnt_per_user\n",
      "               model  mean_RMSE  std_RMSE  median_RMSE  best_iter_mean\n",
      "TR_best(Stage2_id54)   0.025831  0.007816     0.023835           367.6\n",
      "TR_close(mcs100_l10)   0.025875  0.007710     0.023585           272.5\n",
      "      TR_stage1_base   0.025905  0.007756     0.023507           273.6\n",
      "      NR_stage1_base   0.026413  0.007420     0.024704           178.5\n",
      "NR_best(Stage2_id21)   0.026570  0.008139     0.026208           164.2\n",
      "  NR_close(lambda10)   0.026759  0.008226     0.026251           176.9\n",
      "\n",
      "Target = nonrepeat_cnt_per_user\n",
      "               model  mean_RMSE  std_RMSE  median_RMSE  best_iter_mean\n",
      "NR_best(Stage2_id21)   0.036502  0.009671     0.037635           165.5\n",
      "  NR_close(lambda10)   0.036505  0.009440     0.037174           178.8\n",
      "TR_best(Stage2_id54)   0.037616  0.009481     0.036353           235.3\n",
      "      NR_stage1_base   0.037624  0.010549     0.038695           170.4\n",
      "TR_close(mcs100_l10)   0.037749  0.008764     0.036838           308.6\n",
      "      TR_stage1_base   0.037759  0.008770     0.036629           242.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "# =========================\n",
    "# Config (CONTROL)\n",
    "# =========================\n",
    "DATA_PATH = \"../../final_data/data_260119_control.csv\"\n",
    "DATE_COL = \"experiment_date\"\n",
    "CAT_COLS = [\"ops_type_merged\", \"city_group\"]\n",
    "\n",
    "Y1 = \"trip_cnt_per_user\"\n",
    "Y2 = \"nonrepeat_cnt_per_user\"\n",
    "\n",
    "# baselines (already in data)\n",
    "Y1_LAG1 = \"trip_cnt_per_user_lag1\"\n",
    "Y1_ROLL4 = \"trip_cnt_per_user_roll4\"\n",
    "Y2_LAG1 = \"nonrepeat_cnt_per_user_lag1\"\n",
    "Y2_ROLL4 = \"nonrepeat_cnt_per_user_roll4\"\n",
    "\n",
    "DROP_FEATURES = {\"log1p_user_cnt\"}\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_TEST_WEEKS = 10\n",
    "N_VAL_WEEKS = 4\n",
    "EARLY_STOP_ROUNDS = 100\n",
    "MIN_TRAIN_WEEKS = 12\n",
    "\n",
    "# training budget (keep modest to avoid memory blowups)\n",
    "NUM_ITER = 3000\n",
    "LR = 0.05\n",
    "\n",
    "DROP_CONSTANT_FEATURES = True\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def rmse(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "def summarize(arr):\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    return float(np.mean(arr)), float(np.std(arr)), float(np.median(arr))\n",
    "\n",
    "def make_time_val_split(train_df, date_col, n_val_weeks=4):\n",
    "    uniq_dates = np.array(sorted(pd.to_datetime(train_df[date_col]).dt.date.unique()))\n",
    "    if len(uniq_dates) <= 2:\n",
    "        return train_df.copy(), None\n",
    "    n_val_weeks = min(n_val_weeks, max(1, len(uniq_dates) // 5))\n",
    "    val_dates = set(uniq_dates[-n_val_weeks:])\n",
    "    tr = train_df[~pd.to_datetime(train_df[date_col]).dt.date.isin(val_dates)].copy()\n",
    "    va = train_df[pd.to_datetime(train_df[date_col]).dt.date.isin(val_dates)].copy()\n",
    "    if tr.empty or va.empty:\n",
    "        return train_df.copy(), None\n",
    "    return tr, va\n",
    "\n",
    "def get_feature_cols(df):\n",
    "    exclude = {Y1, Y2} | set(DROP_FEATURES)\n",
    "    cols = [c for c in df.columns if c not in exclude]\n",
    "    if DATE_COL in cols:\n",
    "        cols.remove(DATE_COL)\n",
    "    return cols\n",
    "\n",
    "def drop_constant_features(train_df, feature_cols):\n",
    "    return [c for c in feature_cols if train_df[c].nunique(dropna=False) > 1]\n",
    "\n",
    "# =========================\n",
    "# Load & preprocess\n",
    "# =========================\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "\n",
    "# category\n",
    "for c in CAT_COLS:\n",
    "    df[c] = df[c].astype(\"category\")\n",
    "\n",
    "# week_idx\n",
    "min_date = df[DATE_COL].min()\n",
    "df[\"week_idx\"] = ((df[DATE_COL] - min_date).dt.days // 7).astype(int)\n",
    "\n",
    "feature_cols_global = get_feature_cols(df)\n",
    "\n",
    "all_weeks = np.array(sorted(df[DATE_COL].dt.date.unique()))\n",
    "if len(all_weeks) < (N_TEST_WEEKS + 2):\n",
    "    raise ValueError(f\"Not enough weeks ({len(all_weeks)}) for N_TEST_WEEKS={N_TEST_WEEKS}\")\n",
    "test_weeks = all_weeks[-N_TEST_WEEKS:]\n",
    "\n",
    "print(\"=== Rolling backtest test weeks ===\")\n",
    "print([str(w) for w in test_weeks])\n",
    "\n",
    "# =========================\n",
    "# Candidate param sets (pick a few important ones)\n",
    "# From your Stage 2 top results + a couple of near-by controls\n",
    "# =========================\n",
    "CANDIDATES = [\n",
    "    # --- Nonrepeat winner (Stage2 top1)\n",
    "    dict(name=\"NR_best(Stage2_id21)\", mcs=50, min_split_gain=0.01, reg_lambda=5.0, subsample=0.9),\n",
    "\n",
    "    # --- Nonrepeat close runner-ups (very close means)\n",
    "    dict(name=\"NR_close(lambda10)\",    mcs=50, min_split_gain=0.01, reg_lambda=10.0, subsample=0.9),\n",
    "    dict(name=\"NR_stage1_base\",        mcs=50, min_split_gain=0.00, reg_lambda=2.0,  subsample=0.8),\n",
    "\n",
    "    # --- Trip winner (Stage2 top1)\n",
    "    dict(name=\"TR_best(Stage2_id54)\",  mcs=80,  min_split_gain=0.00, reg_lambda=2.0,  subsample=0.9),\n",
    "\n",
    "    # --- Trip close variants (very close means)\n",
    "    dict(name=\"TR_close(mcs100_l10)\",  mcs=100, min_split_gain=0.00, reg_lambda=10.0, subsample=0.8),\n",
    "    dict(name=\"TR_stage1_base\",        mcs=100, min_split_gain=0.00, reg_lambda=2.0,  subsample=0.8),\n",
    "]\n",
    "\n",
    "# Common (fixed) params\n",
    "base_params = dict(\n",
    "    objective=\"regression\",\n",
    "    metric=\"l2\",\n",
    "    learning_rate=LR,\n",
    "    num_leaves=31,\n",
    "    max_depth=6,\n",
    "    colsample_bytree=0.8,\n",
    "    bagging_freq=1,\n",
    "    reg_alpha=0.0,\n",
    "    seed=RANDOM_STATE,\n",
    "    force_row_wise=True,\n",
    "    verbosity=-1,\n",
    "    num_iterations=NUM_ITER,\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Baseline scoring (lag1/roll4) over the same 10 test weeks\n",
    "# =========================\n",
    "baseline_rows = []\n",
    "for tw in test_weeks:\n",
    "    tw = pd.to_datetime(tw)\n",
    "    test_df = df[df[DATE_COL] == tw].copy()\n",
    "    if test_df.empty:\n",
    "        continue\n",
    "    y1_true = test_df[Y1].astype(float).values\n",
    "    y2_true = test_df[Y2].astype(float).values\n",
    "\n",
    "    baseline_rows.append({\n",
    "        \"test_week\": str(tw.date()),\n",
    "        \"target\": Y1,\n",
    "        \"model\": \"baseline_lag1\",\n",
    "        \"RMSE\": rmse(y1_true, test_df[Y1_LAG1].astype(float).values),\n",
    "    })\n",
    "    baseline_rows.append({\n",
    "        \"test_week\": str(tw.date()),\n",
    "        \"target\": Y1,\n",
    "        \"model\": \"baseline_roll4\",\n",
    "        \"RMSE\": rmse(y1_true, test_df[Y1_ROLL4].astype(float).values),\n",
    "    })\n",
    "    baseline_rows.append({\n",
    "        \"test_week\": str(tw.date()),\n",
    "        \"target\": Y2,\n",
    "        \"model\": \"baseline_lag1\",\n",
    "        \"RMSE\": rmse(y2_true, test_df[Y2_LAG1].astype(float).values),\n",
    "    })\n",
    "    baseline_rows.append({\n",
    "        \"test_week\": str(tw.date()),\n",
    "        \"target\": Y2,\n",
    "        \"model\": \"baseline_roll4\",\n",
    "        \"RMSE\": rmse(y2_true, test_df[Y2_ROLL4].astype(float).values),\n",
    "    })\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_rows)\n",
    "baseline_sum = (\n",
    "    baseline_df.groupby([\"target\", \"model\"])[\"RMSE\"]\n",
    "    .apply(lambda s: pd.Series({\"mean_RMSE\": float(s.mean()), \"std_RMSE\": float(s.std()), \"median_RMSE\": float(s.median())}))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"\\n=== Baseline summary over last 10 weeks ===\")\n",
    "print(baseline_sum.to_string(index=False))\n",
    "\n",
    "# =========================\n",
    "# Rolling backtest for each candidate\n",
    "# =========================\n",
    "rows = []\n",
    "\n",
    "for cand in CANDIDATES:\n",
    "    name = cand[\"name\"]\n",
    "    params = {\n",
    "        **base_params,\n",
    "        \"min_child_samples\": cand[\"mcs\"],\n",
    "        \"min_split_gain\": cand[\"min_split_gain\"],\n",
    "        \"reg_lambda\": cand[\"reg_lambda\"],\n",
    "        \"subsample\": cand[\"subsample\"],\n",
    "    }\n",
    "\n",
    "    rmse_by_week_y1, rmse_by_week_y2 = [], []\n",
    "    bestit_y1, bestit_y2 = [], []\n",
    "\n",
    "    used_weeks = 0\n",
    "\n",
    "    for tw in test_weeks:\n",
    "        tw = pd.to_datetime(tw)\n",
    "\n",
    "        train_df = df[df[DATE_COL] < tw].copy()\n",
    "        test_df  = df[df[DATE_COL] == tw].copy()\n",
    "        if train_df.empty or test_df.empty:\n",
    "            continue\n",
    "\n",
    "        # ensure enough training weeks\n",
    "        if train_df[DATE_COL].dt.date.nunique() < MIN_TRAIN_WEEKS:\n",
    "            continue\n",
    "\n",
    "        used_weeks += 1\n",
    "\n",
    "        cur_cols = feature_cols_global\n",
    "        if DROP_CONSTANT_FEATURES:\n",
    "            cur_cols = drop_constant_features(train_df, feature_cols_global)\n",
    "\n",
    "        tr_df, va_df = make_time_val_split(train_df, DATE_COL, n_val_weeks=N_VAL_WEEKS)\n",
    "\n",
    "        X_tr = tr_df[cur_cols]\n",
    "        X_te = test_df[cur_cols]\n",
    "        y1_tr = tr_df[Y1].astype(float).values\n",
    "        y2_tr = tr_df[Y2].astype(float).values\n",
    "        y1_te = test_df[Y1].astype(float).values\n",
    "        y2_te = test_df[Y2].astype(float).values\n",
    "\n",
    "        dtrain1 = lgb.Dataset(X_tr, label=y1_tr, free_raw_data=True)\n",
    "        dtrain2 = lgb.Dataset(X_tr, label=y2_tr, free_raw_data=True)\n",
    "\n",
    "        valid_sets1 = [dtrain1]\n",
    "        valid_sets2 = [dtrain2]\n",
    "\n",
    "        if va_df is not None and (not va_df.empty):\n",
    "            X_va = va_df[cur_cols]\n",
    "            y1_va = va_df[Y1].astype(float).values\n",
    "            y2_va = va_df[Y2].astype(float).values\n",
    "            dvalid1 = lgb.Dataset(X_va, label=y1_va, reference=dtrain1, free_raw_data=True)\n",
    "            dvalid2 = lgb.Dataset(X_va, label=y2_va, reference=dtrain2, free_raw_data=True)\n",
    "            valid_sets1 = [dtrain1, dvalid1]\n",
    "            valid_sets2 = [dtrain2, dvalid2]\n",
    "\n",
    "        booster1 = lgb.train(\n",
    "            params, dtrain1, valid_sets=valid_sets1,\n",
    "            callbacks=[lgb.early_stopping(EARLY_STOP_ROUNDS, verbose=False)]\n",
    "        )\n",
    "        booster2 = lgb.train(\n",
    "            params, dtrain2, valid_sets=valid_sets2,\n",
    "            callbacks=[lgb.early_stopping(EARLY_STOP_ROUNDS, verbose=False)]\n",
    "        )\n",
    "\n",
    "        p1 = booster1.predict(X_te, num_iteration=booster1.best_iteration)\n",
    "        p2 = booster2.predict(X_te, num_iteration=booster2.best_iteration)\n",
    "\n",
    "        rmse_by_week_y1.append(rmse(y1_te, p1))\n",
    "        rmse_by_week_y2.append(rmse(y2_te, p2))\n",
    "        bestit_y1.append(booster1.best_iteration)\n",
    "        bestit_y2.append(booster2.best_iteration)\n",
    "\n",
    "        # free memory\n",
    "        del booster1, booster2, dtrain1, dtrain2\n",
    "        if \"dvalid1\" in locals(): del dvalid1\n",
    "        if \"dvalid2\" in locals(): del dvalid2\n",
    "        gc.collect()\n",
    "\n",
    "    y1_mean, y1_std, y1_med = summarize(rmse_by_week_y1) if rmse_by_week_y1 else (np.nan, np.nan, np.nan)\n",
    "    y2_mean, y2_std, y2_med = summarize(rmse_by_week_y2) if rmse_by_week_y2 else (np.nan, np.nan, np.nan)\n",
    "\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"target\": Y1,\n",
    "        \"mean_RMSE\": y1_mean,\n",
    "        \"std_RMSE\": y1_std,\n",
    "        \"median_RMSE\": y1_med,\n",
    "        \"best_iter_mean\": float(np.mean(bestit_y1)) if bestit_y1 else np.nan,\n",
    "        \"n_weeks_used\": used_weeks,\n",
    "        \"rmse_by_week\": rmse_by_week_y1,\n",
    "        \"params\": str({k:v for k,v in cand.items() if k!=\"name\"}),\n",
    "    })\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"target\": Y2,\n",
    "        \"mean_RMSE\": y2_mean,\n",
    "        \"std_RMSE\": y2_std,\n",
    "        \"median_RMSE\": y2_med,\n",
    "        \"best_iter_mean\": float(np.mean(bestit_y2)) if bestit_y2 else np.nan,\n",
    "        \"n_weeks_used\": used_weeks,\n",
    "        \"rmse_by_week\": rmse_by_week_y2,\n",
    "        \"params\": str({k:v for k,v in cand.items() if k!=\"name\"}),\n",
    "    })\n",
    "\n",
    "res_df = pd.DataFrame(rows).sort_values([\"target\", \"mean_RMSE\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== Candidate models: 10-week rolling backtest summary ===\")\n",
    "print(res_df[[\"target\",\"model\",\"mean_RMSE\",\"std_RMSE\",\"median_RMSE\",\"best_iter_mean\",\"n_weeks_used\",\"params\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Candidate models: RMSE by week (for inspection) ===\")\n",
    "print(res_df[[\"target\",\"model\",\"rmse_by_week\"]].to_string(index=False))\n",
    "\n",
    "# Optional: rank table per target (best to worst)\n",
    "print(\"\\n=== Ranking (lower mean_RMSE is better) ===\")\n",
    "for tgt in [Y1, Y2]:\n",
    "    sub = res_df[res_df[\"target\"]==tgt].copy().sort_values(\"mean_RMSE\")\n",
    "    print(f\"\\nTarget = {tgt}\")\n",
    "    print(sub[[\"model\",\"mean_RMSE\",\"std_RMSE\",\"median_RMSE\",\"best_iter_mean\"]].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b84e42",
   "metadata": {},
   "source": [
    "# 最終版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dae094e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Target week === 2026-01-05\n",
      "Train rows: 528 | Test rows: 24\n",
      "\n",
      "=== Best iterations (if early stopping used) ===\n",
      "Y1 best_iter: 112\n",
      "Y2 best_iter: 197\n",
      "\n",
      "=== Overall metrics (CONTROL, target week) ===\n",
      "                target          model      MAE     RMSE\n",
      "nonrepeat_cnt_per_user  baseline_lag1 0.050654 0.061245\n",
      "nonrepeat_cnt_per_user baseline_roll4 0.043487 0.052395\n",
      "nonrepeat_cnt_per_user     lgbm_tuned 0.030186 0.035428\n",
      "     trip_cnt_per_user  baseline_lag1 0.023878 0.029743\n",
      "     trip_cnt_per_user baseline_roll4 0.022243 0.026756\n",
      "     trip_cnt_per_user     lgbm_tuned 0.021460 0.025286\n",
      "Saved: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\control_overall_metrics.csv\n",
      "\n",
      "=== Metrics by ops_type_merged (CONTROL, target week) ===\n",
      "ops_type_merged                 target          model      MAE     RMSE  n\n",
      "   14天在其他尖峰預估車資 nonrepeat_cnt_per_user  baseline_lag1 0.058491 0.069610  3\n",
      "   14天在其他尖峰預估車資 nonrepeat_cnt_per_user baseline_roll4 0.035716 0.047553  3\n",
      "   14天在其他尖峰預估車資 nonrepeat_cnt_per_user     lgbm_tuned 0.026108 0.026935  3\n",
      "    14天在晚尖峰預估車資 nonrepeat_cnt_per_user  baseline_lag1 0.052384 0.065279  3\n",
      "    14天在晚尖峰預估車資 nonrepeat_cnt_per_user baseline_roll4 0.051107 0.067293  3\n",
      "    14天在晚尖峰預估車資 nonrepeat_cnt_per_user     lgbm_tuned 0.028360 0.036120  3\n",
      "     90天在尖峰預估車資 nonrepeat_cnt_per_user  baseline_lag1 0.023160 0.025283  3\n",
      "     90天在尖峰預估車資 nonrepeat_cnt_per_user baseline_roll4 0.019414 0.021412  3\n",
      "     90天在尖峰預估車資 nonrepeat_cnt_per_user     lgbm_tuned 0.016458 0.016901  3\n",
      "          喚回-其他 nonrepeat_cnt_per_user  baseline_lag1 0.018568 0.019912  3\n",
      "          喚回-其他 nonrepeat_cnt_per_user baseline_roll4 0.010418 0.014716  3\n",
      "          喚回-其他 nonrepeat_cnt_per_user     lgbm_tuned 0.007568 0.008432  3\n",
      "       喚回-高優惠敏感 nonrepeat_cnt_per_user  baseline_lag1 0.075408 0.078426  3\n",
      "       喚回-高優惠敏感 nonrepeat_cnt_per_user baseline_roll4 0.060948 0.061523  3\n",
      "       喚回-高優惠敏感 nonrepeat_cnt_per_user     lgbm_tuned 0.042702 0.043048  3\n",
      "    既有regular鞏固 nonrepeat_cnt_per_user  baseline_lag1 0.067614 0.079491  3\n",
      "    既有regular鞏固 nonrepeat_cnt_per_user baseline_roll4 0.063584 0.067308  3\n",
      "    既有regular鞏固 nonrepeat_cnt_per_user     lgbm_tuned 0.030400 0.035721  3\n",
      "   養成Regular-其他 nonrepeat_cnt_per_user  baseline_lag1 0.042694 0.050759  3\n",
      "   養成Regular-其他 nonrepeat_cnt_per_user baseline_roll4 0.041738 0.044119  3\n",
      "   養成Regular-其他 nonrepeat_cnt_per_user     lgbm_tuned 0.035520 0.039153  3\n",
      "養成Regular-高優惠敏感 nonrepeat_cnt_per_user  baseline_lag1 0.066915 0.069418  3\n",
      "養成Regular-高優惠敏感 nonrepeat_cnt_per_user baseline_roll4 0.064973 0.065082  3\n",
      "養成Regular-高優惠敏感 nonrepeat_cnt_per_user     lgbm_tuned 0.054371 0.054701  3\n",
      "   14天在其他尖峰預估車資      trip_cnt_per_user  baseline_lag1 0.031740 0.031908  3\n",
      "   14天在其他尖峰預估車資      trip_cnt_per_user baseline_roll4 0.024116 0.025804  3\n",
      "   14天在其他尖峰預估車資      trip_cnt_per_user     lgbm_tuned 0.019006 0.019029  3\n",
      "    14天在晚尖峰預估車資      trip_cnt_per_user  baseline_lag1 0.024723 0.026458  3\n",
      "    14天在晚尖峰預估車資      trip_cnt_per_user baseline_roll4 0.020512 0.028653  3\n",
      "    14天在晚尖峰預估車資      trip_cnt_per_user     lgbm_tuned 0.023008 0.023636  3\n",
      "     90天在尖峰預估車資      trip_cnt_per_user  baseline_lag1 0.007964 0.009046  3\n",
      "     90天在尖峰預估車資      trip_cnt_per_user baseline_roll4 0.007700 0.009225  3\n",
      "     90天在尖峰預估車資      trip_cnt_per_user     lgbm_tuned 0.009038 0.009773  3\n",
      "          喚回-其他      trip_cnt_per_user  baseline_lag1 0.004875 0.005989  3\n",
      "          喚回-其他      trip_cnt_per_user baseline_roll4 0.008602 0.008686  3\n",
      "          喚回-其他      trip_cnt_per_user     lgbm_tuned 0.004233 0.004238  3\n",
      "       喚回-高優惠敏感      trip_cnt_per_user  baseline_lag1 0.031721 0.034201  3\n",
      "       喚回-高優惠敏感      trip_cnt_per_user baseline_roll4 0.031731 0.032140  3\n",
      "       喚回-高優惠敏感      trip_cnt_per_user     lgbm_tuned 0.028753 0.032285  3\n",
      "    既有regular鞏固      trip_cnt_per_user  baseline_lag1 0.024870 0.030727  3\n",
      "    既有regular鞏固      trip_cnt_per_user baseline_roll4 0.020014 0.023621  3\n",
      "    既有regular鞏固      trip_cnt_per_user     lgbm_tuned 0.023701 0.024055  3\n",
      "   養成Regular-其他      trip_cnt_per_user  baseline_lag1 0.024834 0.038224  3\n",
      "   養成Regular-其他      trip_cnt_per_user baseline_roll4 0.026231 0.030932  3\n",
      "   養成Regular-其他      trip_cnt_per_user     lgbm_tuned 0.023748 0.028799  3\n",
      "養成Regular-高優惠敏感      trip_cnt_per_user  baseline_lag1 0.040299 0.040825  3\n",
      "養成Regular-高優惠敏感      trip_cnt_per_user baseline_roll4 0.039035 0.039141  3\n",
      "養成Regular-高優惠敏感      trip_cnt_per_user     lgbm_tuned 0.040190 0.040378  3\n",
      "Saved: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\control_metrics_by_ops_type_merged.csv\n",
      "\n",
      "=== Metrics by city_group (CONTROL, target week) ===\n",
      "city_group                 target          model      MAE     RMSE  n\n",
      "        中區 nonrepeat_cnt_per_user  baseline_lag1 0.042349 0.051023  8\n",
      "        中區 nonrepeat_cnt_per_user baseline_roll4 0.036780 0.043242  8\n",
      "        中區 nonrepeat_cnt_per_user     lgbm_tuned 0.027183 0.035365  8\n",
      "        北區 nonrepeat_cnt_per_user  baseline_lag1 0.072779 0.080802  8\n",
      "        北區 nonrepeat_cnt_per_user baseline_roll4 0.060654 0.067262  8\n",
      "        北區 nonrepeat_cnt_per_user     lgbm_tuned 0.032501 0.035568  8\n",
      "        南區 nonrepeat_cnt_per_user  baseline_lag1 0.036835 0.046048  8\n",
      "        南區 nonrepeat_cnt_per_user baseline_roll4 0.033027 0.042916  8\n",
      "        南區 nonrepeat_cnt_per_user     lgbm_tuned 0.030874 0.035350  8\n",
      "        中區      trip_cnt_per_user  baseline_lag1 0.024562 0.032883  8\n",
      "        中區      trip_cnt_per_user baseline_roll4 0.018999 0.025663  8\n",
      "        中區      trip_cnt_per_user     lgbm_tuned 0.021178 0.025249  8\n",
      "        北區      trip_cnt_per_user  baseline_lag1 0.027227 0.031335  8\n",
      "        北區      trip_cnt_per_user baseline_roll4 0.028028 0.030978  8\n",
      "        北區      trip_cnt_per_user     lgbm_tuned 0.021416 0.024554  8\n",
      "        南區      trip_cnt_per_user  baseline_lag1 0.019846 0.024308  8\n",
      "        南區      trip_cnt_per_user baseline_roll4 0.019701 0.023010  8\n",
      "        南區      trip_cnt_per_user     lgbm_tuned 0.021785 0.026032  8\n",
      "Saved: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\control_metrics_by_city_group.csv\n",
      "\n",
      "=== Predictions FULL (CONTROL; saved full; preview shown) ===\n",
      "experiment_date ops_type_merged city_group  trip_cnt_per_user  pred_y1_lag1  pred_y1_roll4  pred_y1_lgbm  nonrepeat_cnt_per_user  pred_y2_lag1  pred_y2_roll4  pred_y2_lgbm\n",
      "     2026-01-05    14天在其他尖峰預估車資         中區           0.111377      0.147170       0.122789      0.131383                0.146108      0.211321       0.173127      0.180505\n",
      "     2026-01-05    14天在其他尖峰預估車資         北區           0.143668      0.171447       0.176450      0.161442                0.197730      0.298714       0.275500      0.223468\n",
      "     2026-01-05    14天在其他尖峰預估車資         南區           0.140374      0.108727       0.112220      0.121137                0.173797      0.164521       0.171438      0.155609\n",
      "     2026-01-05     14天在晚尖峰預估車資         中區           0.164804      0.153285       0.159300      0.140290                0.198324      0.204380       0.225825      0.195761\n",
      "     2026-01-05     14天在晚尖峰預估車資         北區           0.121813      0.151575       0.170600      0.150566                0.156516      0.257874       0.268992      0.213631\n",
      "     2026-01-05     14天在晚尖峰預估車資         南區           0.115079      0.082192       0.107835      0.099323                0.154762      0.105023       0.141419      0.129360\n",
      "     2026-01-05      90天在尖峰預估車資         中區           0.044901      0.043003       0.045577      0.051292                0.056317      0.068513       0.067363      0.070527\n",
      "     2026-01-05      90天在尖峰預估車資         北區           0.038262      0.049357       0.050789      0.052561                0.054354      0.091003       0.086311      0.076225\n",
      "     2026-01-05      90天在尖峰預估車資         南區           0.031026      0.041924       0.040922      0.037449                0.045346      0.065979       0.060584      0.058638\n",
      "     2026-01-05           喚回-其他         中區           0.052469      0.052531       0.044086      0.048282                0.060700      0.074499       0.064134      0.064981\n",
      "     2026-01-05           喚回-其他         北區           0.047662      0.055826       0.057834      0.052171                0.063450      0.092184       0.088561      0.076213\n",
      "     2026-01-05           喚回-其他         南區           0.048938      0.055336       0.041688      0.044936                0.060942      0.074111       0.063650      0.066601\n",
      "     2026-01-05        喚回-高優惠敏感         中區           0.091398      0.077320       0.119035      0.100376                0.096774      0.144330       0.154255      0.134440\n",
      "     2026-01-05        喚回-高優惠敏感         北區           0.059105      0.103074       0.087719      0.092264                0.075080      0.153707       0.127937      0.115246\n",
      "     2026-01-05        喚回-高優惠敏感         南區           0.045161      0.082278       0.084103      0.089284                0.064516      0.164557       0.137024      0.114789\n",
      "     2026-01-05     既有regular鞏固         中區           0.202265      0.183422       0.195579      0.178380                0.223301      0.257496       0.258895      0.228122\n",
      "     2026-01-05     既有regular鞏固         北區           0.159231      0.208589       0.196051      0.177808                0.202692      0.329243       0.292252      0.239794\n",
      "     2026-01-05     既有regular鞏固         南區           0.141199      0.134791       0.157734      0.169842                0.152805      0.194900       0.218402      0.202082\n",
      "     2026-01-05    養成Regular-其他         中區           0.034286      0.100000       0.083695      0.081003                0.048571      0.129730       0.110508      0.106073\n",
      "     2026-01-05    養成Regular-其他         北區           0.065965      0.073976       0.080961      0.076676                0.084912      0.112946       0.117373      0.102767\n",
      "     2026-01-05    養成Regular-其他         南區           0.038724      0.039501       0.053013      0.052539                0.045558      0.064449       0.076374      0.076761\n",
      "     2026-01-05 養成Regular-高優惠敏感         中區           0.053743      0.102334       0.096024      0.088486                0.061420      0.140036       0.131645      0.123445\n",
      "     2026-01-05 養成Regular-高優惠敏感         北區           0.060421      0.100095       0.099947      0.103967                0.083149      0.164442       0.146191      0.130543\n",
      "     2026-01-05 養成Regular-高優惠敏感         南區           0.039832      0.072464       0.075131      0.082114                0.046122      0.086957       0.107773      0.099817\n",
      "Saved: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\control_predictions_full.csv\n",
      "\n",
      "=== Quick aggregates (means) (CONTROL) ===\n",
      "         metric  trip_cnt_per_user  nonrepeat_cnt_per_user\n",
      "      mean_true           0.085488                0.106385\n",
      " mean_pred_lag1           0.099592                0.152121\n",
      "mean_pred_roll4           0.102462                0.148564\n",
      " mean_pred_lgbm           0.099315                0.132725\n",
      "Saved: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\control_quick_aggregates_means.csv\n",
      "\n",
      "=== Group RMSE (LGBM) - trip_cnt_per_user (CONTROL, all groups) ===\n",
      "ops_type_merged city_group            target     RMSE      MAE  n\n",
      "          喚回-其他         南區 trip_cnt_per_user 0.004002 0.004002  1\n",
      "          喚回-其他         中區 trip_cnt_per_user 0.004187 0.004187  1\n",
      "          喚回-其他         北區 trip_cnt_per_user 0.004509 0.004509  1\n",
      "     90天在尖峰預估車資         中區 trip_cnt_per_user 0.006391 0.006391  1\n",
      "     90天在尖峰預估車資         南區 trip_cnt_per_user 0.006423 0.006423  1\n",
      "       喚回-高優惠敏感         中區 trip_cnt_per_user 0.008978 0.008978  1\n",
      "   養成Regular-其他         北區 trip_cnt_per_user 0.010711 0.010711  1\n",
      "   養成Regular-其他         南區 trip_cnt_per_user 0.013814 0.013814  1\n",
      "     90天在尖峰預估車資         北區 trip_cnt_per_user 0.014299 0.014299  1\n",
      "    14天在晚尖峰預估車資         南區 trip_cnt_per_user 0.015757 0.015757  1\n",
      "   14天在其他尖峰預估車資         北區 trip_cnt_per_user 0.017775 0.017775  1\n",
      "    既有regular鞏固         北區 trip_cnt_per_user 0.018577 0.018577  1\n",
      "   14天在其他尖峰預估車資         南區 trip_cnt_per_user 0.019238 0.019238  1\n",
      "   14天在其他尖峰預估車資         中區 trip_cnt_per_user 0.020006 0.020006  1\n",
      "    既有regular鞏固         中區 trip_cnt_per_user 0.023885 0.023885  1\n",
      "    14天在晚尖峰預估車資         中區 trip_cnt_per_user 0.024514 0.024514  1\n",
      "    既有regular鞏固         南區 trip_cnt_per_user 0.028642 0.028642  1\n",
      "    14天在晚尖峰預估車資         北區 trip_cnt_per_user 0.028753 0.028753  1\n",
      "       喚回-高優惠敏感         北區 trip_cnt_per_user 0.033159 0.033159  1\n",
      "養成Regular-高優惠敏感         中區 trip_cnt_per_user 0.034743 0.034743  1\n",
      "養成Regular-高優惠敏感         南區 trip_cnt_per_user 0.042282 0.042282  1\n",
      "養成Regular-高優惠敏感         北區 trip_cnt_per_user 0.043546 0.043546  1\n",
      "       喚回-高優惠敏感         南區 trip_cnt_per_user 0.044123 0.044123  1\n",
      "   養成Regular-其他         中區 trip_cnt_per_user 0.046717 0.046717  1\n",
      "Saved: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\control_group_rmse_lgbm_trip_all_groups.csv\n",
      "\n",
      "=== Group RMSE (LGBM) - nonrepeat_cnt_per_user (CONTROL, all groups) ===\n",
      "ops_type_merged city_group                 target     RMSE      MAE  n\n",
      "    14天在晚尖峰預估車資         中區 nonrepeat_cnt_per_user 0.002563 0.002563  1\n",
      "          喚回-其他         中區 nonrepeat_cnt_per_user 0.004282 0.004282  1\n",
      "    既有regular鞏固         中區 nonrepeat_cnt_per_user 0.004821 0.004821  1\n",
      "          喚回-其他         南區 nonrepeat_cnt_per_user 0.005659 0.005659  1\n",
      "          喚回-其他         北區 nonrepeat_cnt_per_user 0.012764 0.012764  1\n",
      "     90天在尖峰預估車資         南區 nonrepeat_cnt_per_user 0.013292 0.013292  1\n",
      "     90天在尖峰預估車資         中區 nonrepeat_cnt_per_user 0.014210 0.014210  1\n",
      "   養成Regular-其他         北區 nonrepeat_cnt_per_user 0.017854 0.017854  1\n",
      "   14天在其他尖峰預估車資         南區 nonrepeat_cnt_per_user 0.018188 0.018188  1\n",
      "     90天在尖峰預估車資         北區 nonrepeat_cnt_per_user 0.021871 0.021871  1\n",
      "    14天在晚尖峰預估車資         南區 nonrepeat_cnt_per_user 0.025402 0.025402  1\n",
      "   14天在其他尖峰預估車資         北區 nonrepeat_cnt_per_user 0.025738 0.025738  1\n",
      "   養成Regular-其他         南區 nonrepeat_cnt_per_user 0.031203 0.031203  1\n",
      "   14天在其他尖峰預估車資         中區 nonrepeat_cnt_per_user 0.034398 0.034398  1\n",
      "    既有regular鞏固         北區 nonrepeat_cnt_per_user 0.037102 0.037102  1\n",
      "       喚回-高優惠敏感         中區 nonrepeat_cnt_per_user 0.037665 0.037665  1\n",
      "       喚回-高優惠敏感         北區 nonrepeat_cnt_per_user 0.040166 0.040166  1\n",
      "養成Regular-高優惠敏感         北區 nonrepeat_cnt_per_user 0.047394 0.047394  1\n",
      "    既有regular鞏固         南區 nonrepeat_cnt_per_user 0.049277 0.049277  1\n",
      "       喚回-高優惠敏感         南區 nonrepeat_cnt_per_user 0.050273 0.050273  1\n",
      "養成Regular-高優惠敏感         南區 nonrepeat_cnt_per_user 0.053695 0.053695  1\n",
      "    14天在晚尖峰預估車資         北區 nonrepeat_cnt_per_user 0.057115 0.057115  1\n",
      "   養成Regular-其他         中區 nonrepeat_cnt_per_user 0.057502 0.057502  1\n",
      "養成Regular-高優惠敏感         中區 nonrepeat_cnt_per_user 0.062025 0.062025  1\n",
      "Saved: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\control_group_rmse_lgbm_nonrepeat_all_groups.csv\n",
      "\n",
      "=== Best 15 group RMSE (LGBM) - trip_cnt_per_user (CONTROL) ===\n",
      "ops_type_merged city_group            target     RMSE      MAE  n\n",
      "          喚回-其他         南區 trip_cnt_per_user 0.004002 0.004002  1\n",
      "          喚回-其他         中區 trip_cnt_per_user 0.004187 0.004187  1\n",
      "          喚回-其他         北區 trip_cnt_per_user 0.004509 0.004509  1\n",
      "     90天在尖峰預估車資         中區 trip_cnt_per_user 0.006391 0.006391  1\n",
      "     90天在尖峰預估車資         南區 trip_cnt_per_user 0.006423 0.006423  1\n",
      "       喚回-高優惠敏感         中區 trip_cnt_per_user 0.008978 0.008978  1\n",
      "   養成Regular-其他         北區 trip_cnt_per_user 0.010711 0.010711  1\n",
      "   養成Regular-其他         南區 trip_cnt_per_user 0.013814 0.013814  1\n",
      "     90天在尖峰預估車資         北區 trip_cnt_per_user 0.014299 0.014299  1\n",
      "    14天在晚尖峰預估車資         南區 trip_cnt_per_user 0.015757 0.015757  1\n",
      "   14天在其他尖峰預估車資         北區 trip_cnt_per_user 0.017775 0.017775  1\n",
      "    既有regular鞏固         北區 trip_cnt_per_user 0.018577 0.018577  1\n",
      "   14天在其他尖峰預估車資         南區 trip_cnt_per_user 0.019238 0.019238  1\n",
      "   14天在其他尖峰預估車資         中區 trip_cnt_per_user 0.020006 0.020006  1\n",
      "    既有regular鞏固         中區 trip_cnt_per_user 0.023885 0.023885  1\n",
      "Saved: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\control_best15_group_rmse_lgbm_trip.csv\n",
      "\n",
      "=== Worst 15 group RMSE (LGBM) - trip_cnt_per_user (CONTROL) ===\n",
      "ops_type_merged city_group            target     RMSE      MAE  n\n",
      "   養成Regular-其他         中區 trip_cnt_per_user 0.046717 0.046717  1\n",
      "       喚回-高優惠敏感         南區 trip_cnt_per_user 0.044123 0.044123  1\n",
      "養成Regular-高優惠敏感         北區 trip_cnt_per_user 0.043546 0.043546  1\n",
      "養成Regular-高優惠敏感         南區 trip_cnt_per_user 0.042282 0.042282  1\n",
      "養成Regular-高優惠敏感         中區 trip_cnt_per_user 0.034743 0.034743  1\n",
      "       喚回-高優惠敏感         北區 trip_cnt_per_user 0.033159 0.033159  1\n",
      "    14天在晚尖峰預估車資         北區 trip_cnt_per_user 0.028753 0.028753  1\n",
      "    既有regular鞏固         南區 trip_cnt_per_user 0.028642 0.028642  1\n",
      "    14天在晚尖峰預估車資         中區 trip_cnt_per_user 0.024514 0.024514  1\n",
      "    既有regular鞏固         中區 trip_cnt_per_user 0.023885 0.023885  1\n",
      "   14天在其他尖峰預估車資         中區 trip_cnt_per_user 0.020006 0.020006  1\n",
      "   14天在其他尖峰預估車資         南區 trip_cnt_per_user 0.019238 0.019238  1\n",
      "    既有regular鞏固         北區 trip_cnt_per_user 0.018577 0.018577  1\n",
      "   14天在其他尖峰預估車資         北區 trip_cnt_per_user 0.017775 0.017775  1\n",
      "    14天在晚尖峰預估車資         南區 trip_cnt_per_user 0.015757 0.015757  1\n",
      "Saved: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\control_worst15_group_rmse_lgbm_trip.csv\n",
      "\n",
      "=== Best 15 group RMSE (LGBM) - nonrepeat_cnt_per_user (CONTROL) ===\n",
      "ops_type_merged city_group                 target     RMSE      MAE  n\n",
      "    14天在晚尖峰預估車資         中區 nonrepeat_cnt_per_user 0.002563 0.002563  1\n",
      "          喚回-其他         中區 nonrepeat_cnt_per_user 0.004282 0.004282  1\n",
      "    既有regular鞏固         中區 nonrepeat_cnt_per_user 0.004821 0.004821  1\n",
      "          喚回-其他         南區 nonrepeat_cnt_per_user 0.005659 0.005659  1\n",
      "          喚回-其他         北區 nonrepeat_cnt_per_user 0.012764 0.012764  1\n",
      "     90天在尖峰預估車資         南區 nonrepeat_cnt_per_user 0.013292 0.013292  1\n",
      "     90天在尖峰預估車資         中區 nonrepeat_cnt_per_user 0.014210 0.014210  1\n",
      "   養成Regular-其他         北區 nonrepeat_cnt_per_user 0.017854 0.017854  1\n",
      "   14天在其他尖峰預估車資         南區 nonrepeat_cnt_per_user 0.018188 0.018188  1\n",
      "     90天在尖峰預估車資         北區 nonrepeat_cnt_per_user 0.021871 0.021871  1\n",
      "    14天在晚尖峰預估車資         南區 nonrepeat_cnt_per_user 0.025402 0.025402  1\n",
      "   14天在其他尖峰預估車資         北區 nonrepeat_cnt_per_user 0.025738 0.025738  1\n",
      "   養成Regular-其他         南區 nonrepeat_cnt_per_user 0.031203 0.031203  1\n",
      "   14天在其他尖峰預估車資         中區 nonrepeat_cnt_per_user 0.034398 0.034398  1\n",
      "    既有regular鞏固         北區 nonrepeat_cnt_per_user 0.037102 0.037102  1\n",
      "Saved: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\control_best15_group_rmse_lgbm_nonrepeat.csv\n",
      "\n",
      "=== Worst 15 group RMSE (LGBM) - nonrepeat_cnt_per_user (CONTROL) ===\n",
      "ops_type_merged city_group                 target     RMSE      MAE  n\n",
      "養成Regular-高優惠敏感         中區 nonrepeat_cnt_per_user 0.062025 0.062025  1\n",
      "   養成Regular-其他         中區 nonrepeat_cnt_per_user 0.057502 0.057502  1\n",
      "    14天在晚尖峰預估車資         北區 nonrepeat_cnt_per_user 0.057115 0.057115  1\n",
      "養成Regular-高優惠敏感         南區 nonrepeat_cnt_per_user 0.053695 0.053695  1\n",
      "       喚回-高優惠敏感         南區 nonrepeat_cnt_per_user 0.050273 0.050273  1\n",
      "    既有regular鞏固         南區 nonrepeat_cnt_per_user 0.049277 0.049277  1\n",
      "養成Regular-高優惠敏感         北區 nonrepeat_cnt_per_user 0.047394 0.047394  1\n",
      "       喚回-高優惠敏感         北區 nonrepeat_cnt_per_user 0.040166 0.040166  1\n",
      "       喚回-高優惠敏感         中區 nonrepeat_cnt_per_user 0.037665 0.037665  1\n",
      "    既有regular鞏固         北區 nonrepeat_cnt_per_user 0.037102 0.037102  1\n",
      "   14天在其他尖峰預估車資         中區 nonrepeat_cnt_per_user 0.034398 0.034398  1\n",
      "   養成Regular-其他         南區 nonrepeat_cnt_per_user 0.031203 0.031203  1\n",
      "   14天在其他尖峰預估車資         北區 nonrepeat_cnt_per_user 0.025738 0.025738  1\n",
      "    14天在晚尖峰預估車資         南區 nonrepeat_cnt_per_user 0.025402 0.025402  1\n",
      "     90天在尖峰預估車資         北區 nonrepeat_cnt_per_user 0.021871 0.021871  1\n",
      "Saved: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\control_worst15_group_rmse_lgbm_nonrepeat.csv\n",
      "\n",
      "=== Final tuned model params (CONTROL) ===\n",
      "                target                                                                                                                                                                                                                                                                                                     params                              note                                                    value\n",
      "     trip_cnt_per_user  {'objective': 'regression', 'n_estimators': 3000, 'learning_rate': 0.05, 'random_state': 42, 'force_row_wise': True, 'num_leaves': 31, 'max_depth': 6, 'min_child_samples': 80, 'subsample': 0.9, 'colsample_bytree': 0.8, 'bagging_freq': 1, 'reg_alpha': 0.0, 'reg_lambda': 2.0, 'min_split_gain': 0.0}                               NaN                                                      NaN\n",
      "nonrepeat_cnt_per_user {'objective': 'regression', 'n_estimators': 3000, 'learning_rate': 0.05, 'random_state': 42, 'force_row_wise': True, 'num_leaves': 31, 'max_depth': 6, 'min_child_samples': 50, 'subsample': 0.9, 'colsample_bytree': 0.8, 'bagging_freq': 1, 'reg_alpha': 0.0, 'reg_lambda': 5.0, 'min_split_gain': 0.01}                               NaN                                                      NaN\n",
      "                   NaN                                                                                                                                                                                                                                                                                                        NaN          categorical_feature used ['treatment', 'source', 'ops_type_merged', 'city_group']\n",
      "                   NaN                                                                                                                                                                                                                                                                                                        NaN object_cols auto-cast to category ['treatment', 'source', 'ops_type_merged', 'city_group']\n",
      "Saved: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\control_final_model_params.csv\n",
      "\n",
      "✅ Saved all outputs to: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "DATA_PATH = \"../../final_data/data_260119_control.csv\"\n",
    "TARGET_WEEK = \"2026-01-05\"\n",
    "\n",
    "DATE_COL = \"experiment_date\"\n",
    "\n",
    "Y1 = \"trip_cnt_per_user\"\n",
    "Y2 = \"nonrepeat_cnt_per_user\"\n",
    "\n",
    "Y1_LAG1 = \"trip_cnt_per_user_lag1\"\n",
    "Y1_ROLL4 = \"trip_cnt_per_user_roll4\"\n",
    "Y2_LAG1 = \"nonrepeat_cnt_per_user_lag1\"\n",
    "Y2_ROLL4 = \"nonrepeat_cnt_per_user_roll4\"\n",
    "\n",
    "# 你想 drop 的（跟 random 一致）\n",
    "DROP_FEATURES = {\"log1p_user_cnt\", \"week_idx\"}\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "OUTPUT_DIR = os.path.join(\"..\", \"..\", \"result\", \"control\")\n",
    "ENCODING_EXCEL = \"utf-8-sig\"\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n",
    "# --------------------------\n",
    "# Helpers\n",
    "# --------------------------\n",
    "def rmse(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    return float(np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "def make_time_val_split(train_df, date_col, n_val_weeks=4):\n",
    "    uniq_dates = np.array(sorted(pd.to_datetime(train_df[date_col]).dt.date.unique()))\n",
    "    if len(uniq_dates) <= 2:\n",
    "        return train_df.copy(), None\n",
    "\n",
    "    n_val_weeks = min(n_val_weeks, max(1, len(uniq_dates) // 5))\n",
    "    val_dates = set(uniq_dates[-n_val_weeks:])\n",
    "\n",
    "    tr = train_df[~pd.to_datetime(train_df[date_col]).dt.date.isin(val_dates)].copy()\n",
    "    va = train_df[pd.to_datetime(train_df[date_col]).dt.date.isin(val_dates)].copy()\n",
    "\n",
    "    if tr.empty or va.empty:\n",
    "        cut = int(len(uniq_dates) * 0.8)\n",
    "        cut = max(1, min(cut, len(uniq_dates) - 1))\n",
    "        tr_dates = set(uniq_dates[:cut])\n",
    "        va_dates = set(uniq_dates[cut:])\n",
    "        tr = train_df[pd.to_datetime(train_df[date_col]).dt.date.isin(tr_dates)].copy()\n",
    "        va = train_df[pd.to_datetime(train_df[date_col]).dt.date.isin(va_dates)].copy()\n",
    "        if tr.empty or va.empty:\n",
    "            return train_df.copy(), None\n",
    "\n",
    "    return tr, va\n",
    "\n",
    "def safe_mkdir(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def save_csv(df_, filename):\n",
    "    path = os.path.join(OUTPUT_DIR, filename)\n",
    "    df_.to_csv(path, index=False, encoding=ENCODING_EXCEL)\n",
    "    return path\n",
    "\n",
    "def print_and_save(df_, title, filename, head_n=None):\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    if head_n is None:\n",
    "        print(df_.to_string(index=False))\n",
    "    else:\n",
    "        print(df_.head(head_n).to_string(index=False))\n",
    "        if len(df_) > head_n:\n",
    "            print(f\"... (printed first {head_n} rows; total rows={len(df_)})\")\n",
    "    path = save_csv(df_, filename)\n",
    "    print(f\"Saved: {os.path.abspath(path)}\")\n",
    "    return path\n",
    "\n",
    "def metrics_by_group(test_df, group_col, y_col, pred_cols):\n",
    "    rows = []\n",
    "    for key, g in test_df.groupby(group_col):\n",
    "        y = g[y_col].astype(float).values\n",
    "        n = len(g)\n",
    "        for model_name, pred_col in pred_cols.items():\n",
    "            rows.append({\n",
    "                group_col: key,\n",
    "                \"target\": y_col,\n",
    "                \"model\": model_name,\n",
    "                \"MAE\": mae(y, g[pred_col].astype(float).values),\n",
    "                \"RMSE\": rmse(y, g[pred_col].astype(float).values),\n",
    "                \"n\": n,\n",
    "            })\n",
    "    return pd.DataFrame(rows).sort_values([\"target\", group_col, \"model\"]).reset_index(drop=True)\n",
    "\n",
    "def group_rmse_table(test_df, y_col, pred_col, group_cols):\n",
    "    rows = []\n",
    "    for key, g in test_df.groupby(group_cols):\n",
    "        y = g[y_col].astype(float).values\n",
    "        p = g[pred_col].astype(float).values\n",
    "        rows.append({\n",
    "            **{col: val for col, val in zip(group_cols, key)},\n",
    "            \"target\": y_col,\n",
    "            \"RMSE\": rmse(y, p),\n",
    "            \"MAE\": mae(y, p),\n",
    "            \"n\": len(g),\n",
    "        })\n",
    "    return pd.DataFrame(rows).sort_values([\"target\", \"RMSE\"]).reset_index(drop=True)\n",
    "\n",
    "def assert_no_object_dtypes(X, name=\"X\"):\n",
    "    bad = X.dtypes[X.dtypes == \"object\"]\n",
    "    if len(bad) > 0:\n",
    "        raise ValueError(f\"{name} still has object dtypes: {list(bad.index)}\")\n",
    "\n",
    "# --------------------------\n",
    "# Load & preprocess\n",
    "# --------------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "if df[DATE_COL].isna().any():\n",
    "    bad = df[df[DATE_COL].isna()].head(10)\n",
    "    raise ValueError(f\"Found unparsable dates in {DATE_COL}. Examples:\\n{bad}\")\n",
    "\n",
    "required_cols = [DATE_COL, Y1, Y2, Y1_LAG1, Y1_ROLL4, Y2_LAG1, Y2_ROLL4]\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "# week_idx\n",
    "min_date = df[DATE_COL].min()\n",
    "df[\"week_idx\"] = ((df[DATE_COL] - min_date).dt.days // 7).astype(int)\n",
    "\n",
    "target_week = pd.to_datetime(TARGET_WEEK)\n",
    "train = df[df[DATE_COL] < target_week].copy()\n",
    "test  = df[df[DATE_COL] == target_week].copy()\n",
    "if train.empty:\n",
    "    raise ValueError(\"Train set is empty. TARGET_WEEK may be too early.\")\n",
    "if test.empty:\n",
    "    raise ValueError(f\"Test set is empty for week {TARGET_WEEK}. Check experiment_date values.\")\n",
    "\n",
    "print(\"=== Target week ===\", TARGET_WEEK)\n",
    "print(\"Train rows:\", len(train), \"| Test rows:\", len(test))\n",
    "\n",
    "# --------------------------\n",
    "# Baselines\n",
    "# --------------------------\n",
    "test[\"pred_y1_lag1\"]  = test[Y1_LAG1].astype(float)\n",
    "test[\"pred_y1_roll4\"] = test[Y1_ROLL4].astype(float)\n",
    "test[\"pred_y2_lag1\"]  = test[Y2_LAG1].astype(float)\n",
    "test[\"pred_y2_roll4\"] = test[Y2_ROLL4].astype(float)\n",
    "\n",
    "# --------------------------\n",
    "# Features: exclude y + drops\n",
    "# --------------------------\n",
    "exclude = {Y1, Y2} | set(DROP_FEATURES)\n",
    "feature_cols = [c for c in df.columns if c not in exclude]\n",
    "if DATE_COL in feature_cols:\n",
    "    feature_cols.remove(DATE_COL)\n",
    "\n",
    "# --------------------------\n",
    "# ✅ Key fix: convert ALL object cols in features to category\n",
    "# and align categories between train/test/val\n",
    "# --------------------------\n",
    "# Build X on the full df first so we can align category levels easily\n",
    "X_all = df[feature_cols].copy()\n",
    "\n",
    "obj_cols = X_all.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "for c in obj_cols:\n",
    "    X_all[c] = X_all[c].astype(\"category\")\n",
    "\n",
    "# also allow user-defined categorical already in category\n",
    "cat_features = X_all.select_dtypes(include=[\"category\"]).columns.tolist()\n",
    "\n",
    "# split back\n",
    "X_train = X_all.loc[train.index].copy()\n",
    "X_test  = X_all.loc[test.index].copy()\n",
    "\n",
    "# make sure categories are consistent (especially important if some categories only appear in test)\n",
    "for c in cat_features:\n",
    "    union_cats = X_all[c].cat.categories\n",
    "    X_train[c] = X_train[c].cat.set_categories(union_cats)\n",
    "    X_test[c]  = X_test[c].cat.set_categories(union_cats)\n",
    "\n",
    "# sanity check: no object dtype remains\n",
    "assert_no_object_dtypes(X_train, \"X_train\")\n",
    "assert_no_object_dtypes(X_test, \"X_test\")\n",
    "\n",
    "y1_train = train[Y1].astype(float).values\n",
    "y2_train = train[Y2].astype(float).values\n",
    "y1_true  = test[Y1].astype(float).values\n",
    "y2_true  = test[Y2].astype(float).values\n",
    "\n",
    "# --------------------------\n",
    "# Val split for early stopping\n",
    "# --------------------------\n",
    "tr_df, va_df = make_time_val_split(train, DATE_COL, n_val_weeks=4)\n",
    "X_tr = X_all.loc[tr_df.index].copy()\n",
    "X_va = None\n",
    "y1_va = y2_va = None\n",
    "\n",
    "# align categories in val too\n",
    "for c in cat_features:\n",
    "    X_tr[c] = X_tr[c].cat.set_categories(X_all[c].cat.categories)\n",
    "\n",
    "y1_tr = tr_df[Y1].astype(float).values\n",
    "y2_tr = tr_df[Y2].astype(float).values\n",
    "\n",
    "if va_df is not None and (not va_df.empty):\n",
    "    X_va = X_all.loc[va_df.index].copy()\n",
    "    for c in cat_features:\n",
    "        X_va[c] = X_va[c].cat.set_categories(X_all[c].cat.categories)\n",
    "    y1_va = va_df[Y1].astype(float).values\n",
    "    y2_va = va_df[Y2].astype(float).values\n",
    "\n",
    "fit_kwargs = dict(categorical_feature=cat_features)\n",
    "\n",
    "# --------------------------\n",
    "# Tuned params (from your 10-week backtest, CONTROL)\n",
    "# --------------------------\n",
    "base_params = dict(\n",
    "    objective=\"regression\",\n",
    "    n_estimators=3000,\n",
    "    learning_rate=0.05,\n",
    "    random_state=RANDOM_STATE,\n",
    "    force_row_wise=True,\n",
    ")\n",
    "\n",
    "# Y1 trip: TR_best(Stage2_id54)\n",
    "params_y1 = dict(\n",
    "    num_leaves=31,\n",
    "    max_depth=6,\n",
    "    min_child_samples=80,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.8,\n",
    "    bagging_freq=1,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=2.0,\n",
    "    min_split_gain=0.0,\n",
    ")\n",
    "\n",
    "# Y2 nonrepeat: NR_best(Stage2_id21)\n",
    "params_y2 = dict(\n",
    "    num_leaves=31,\n",
    "    max_depth=6,\n",
    "    min_child_samples=50,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.8,\n",
    "    bagging_freq=1,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=5.0,\n",
    "    min_split_gain=0.01,\n",
    ")\n",
    "\n",
    "m_y1 = lgb.LGBMRegressor(**{**base_params, **params_y1})\n",
    "m_y2 = lgb.LGBMRegressor(**{**base_params, **params_y2})\n",
    "\n",
    "EARLY_STOP = 100\n",
    "\n",
    "if X_va is not None:\n",
    "    m_y1.fit(\n",
    "        X_tr, y1_tr,\n",
    "        eval_set=[(X_va, y1_va)],\n",
    "        eval_metric=\"l2\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=EARLY_STOP, verbose=False)],\n",
    "        **fit_kwargs\n",
    "    )\n",
    "    m_y2.fit(\n",
    "        X_tr, y2_tr,\n",
    "        eval_set=[(X_va, y2_va)],\n",
    "        eval_metric=\"l2\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=EARLY_STOP, verbose=False)],\n",
    "        **fit_kwargs\n",
    "    )\n",
    "else:\n",
    "    m_y1.fit(X_train, y1_train, **fit_kwargs)\n",
    "    m_y2.fit(X_train, y2_train, **fit_kwargs)\n",
    "\n",
    "print(\"\\n=== Best iterations (if early stopping used) ===\")\n",
    "print(\"Y1 best_iter:\", getattr(m_y1, \"best_iteration_\", None))\n",
    "print(\"Y2 best_iter:\", getattr(m_y2, \"best_iteration_\", None))\n",
    "\n",
    "test[\"pred_y1_lgbm\"] = m_y1.predict(X_test)\n",
    "test[\"pred_y2_lgbm\"] = m_y2.predict(X_test)\n",
    "\n",
    "# --------------------------\n",
    "# Overall metrics\n",
    "# --------------------------\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\"target\": Y1, \"model\": \"baseline_lag1\",  \"MAE\": mae(y1_true, test[\"pred_y1_lag1\"]),  \"RMSE\": rmse(y1_true, test[\"pred_y1_lag1\"])},\n",
    "    {\"target\": Y1, \"model\": \"baseline_roll4\", \"MAE\": mae(y1_true, test[\"pred_y1_roll4\"]), \"RMSE\": rmse(y1_true, test[\"pred_y1_roll4\"])},\n",
    "    {\"target\": Y1, \"model\": \"lgbm_tuned\",     \"MAE\": mae(y1_true, test[\"pred_y1_lgbm\"]),  \"RMSE\": rmse(y1_true, test[\"pred_y1_lgbm\"])},\n",
    "    {\"target\": Y2, \"model\": \"baseline_lag1\",  \"MAE\": mae(y2_true, test[\"pred_y2_lag1\"]),  \"RMSE\": rmse(y2_true, test[\"pred_y2_lag1\"])},\n",
    "    {\"target\": Y2, \"model\": \"baseline_roll4\", \"MAE\": mae(y2_true, test[\"pred_y2_roll4\"]), \"RMSE\": rmse(y2_true, test[\"pred_y2_roll4\"])},\n",
    "    {\"target\": Y2, \"model\": \"lgbm_tuned\",     \"MAE\": mae(y2_true, test[\"pred_y2_lgbm\"]),  \"RMSE\": rmse(y2_true, test[\"pred_y2_lgbm\"])},\n",
    "]).sort_values([\"target\", \"model\"]).reset_index(drop=True)\n",
    "\n",
    "pred_map_y1 = {\"baseline_lag1\": \"pred_y1_lag1\", \"baseline_roll4\": \"pred_y1_roll4\", \"lgbm_tuned\": \"pred_y1_lgbm\"}\n",
    "pred_map_y2 = {\"baseline_lag1\": \"pred_y2_lag1\", \"baseline_roll4\": \"pred_y2_roll4\", \"lgbm_tuned\": \"pred_y2_lgbm\"}\n",
    "\n",
    "metrics_by_ops_df = pd.concat([\n",
    "    metrics_by_group(test, \"ops_type_merged\", Y1, pred_map_y1),\n",
    "    metrics_by_group(test, \"ops_type_merged\", Y2, pred_map_y2),\n",
    "], ignore_index=True).sort_values([\"target\", \"ops_type_merged\", \"model\"]).reset_index(drop=True)\n",
    "\n",
    "metrics_by_city_df = pd.concat([\n",
    "    metrics_by_group(test, \"city_group\", Y1, pred_map_y1),\n",
    "    metrics_by_group(test, \"city_group\", Y2, pred_map_y2),\n",
    "], ignore_index=True).sort_values([\"target\", \"city_group\", \"model\"]).reset_index(drop=True)\n",
    "\n",
    "# Predictions full\n",
    "id_cols = [DATE_COL, \"ops_type_merged\", \"city_group\"]\n",
    "pred_cols = [\n",
    "    Y1, \"pred_y1_lag1\", \"pred_y1_roll4\", \"pred_y1_lgbm\",\n",
    "    Y2, \"pred_y2_lag1\", \"pred_y2_roll4\", \"pred_y2_lgbm\",\n",
    "]\n",
    "full_pred_df = test[id_cols + pred_cols].copy()\n",
    "\n",
    "# Aggregates\n",
    "agg = pd.DataFrame({\n",
    "    \"metric\": [\"mean_true\", \"mean_pred_lag1\", \"mean_pred_roll4\", \"mean_pred_lgbm\"],\n",
    "    \"trip_cnt_per_user\": [\n",
    "        float(np.mean(test[Y1])),\n",
    "        float(np.mean(test[\"pred_y1_lag1\"])),\n",
    "        float(np.mean(test[\"pred_y1_roll4\"])),\n",
    "        float(np.mean(test[\"pred_y1_lgbm\"])),\n",
    "    ],\n",
    "    \"nonrepeat_cnt_per_user\": [\n",
    "        float(np.mean(test[Y2])),\n",
    "        float(np.mean(test[\"pred_y2_lag1\"])),\n",
    "        float(np.mean(test[\"pred_y2_roll4\"])),\n",
    "        float(np.mean(test[\"pred_y2_lgbm\"])),\n",
    "    ],\n",
    "})\n",
    "\n",
    "# Group RMSE tables (only by ops_type_merged, city_group)\n",
    "GROUP_KEY_COLS = [\"ops_type_merged\", \"city_group\"]\n",
    "group_rmse_y1 = group_rmse_table(test, Y1, \"pred_y1_lgbm\", GROUP_KEY_COLS)\n",
    "group_rmse_y2 = group_rmse_table(test, Y2, \"pred_y2_lgbm\", GROUP_KEY_COLS)\n",
    "\n",
    "best15_y1 = group_rmse_y1.nsmallest(15, \"RMSE\").copy()\n",
    "worst15_y1 = group_rmse_y1.nlargest(15, \"RMSE\").copy()\n",
    "best15_y2 = group_rmse_y2.nsmallest(15, \"RMSE\").copy()\n",
    "worst15_y2 = group_rmse_y2.nlargest(15, \"RMSE\").copy()\n",
    "\n",
    "# Save\n",
    "safe_mkdir(OUTPUT_DIR)\n",
    "\n",
    "print_and_save(metrics_df, \"Overall metrics (CONTROL, target week)\", \"control_overall_metrics.csv\", head_n=None)\n",
    "print_and_save(metrics_by_ops_df, \"Metrics by ops_type_merged (CONTROL, target week)\", \"control_metrics_by_ops_type_merged.csv\", head_n=80)\n",
    "print_and_save(metrics_by_city_df, \"Metrics by city_group (CONTROL, target week)\", \"control_metrics_by_city_group.csv\", head_n=80)\n",
    "\n",
    "print_and_save(full_pred_df, \"Predictions FULL (CONTROL; saved full; preview shown)\", \"control_predictions_full.csv\", head_n=30)\n",
    "print_and_save(agg, \"Quick aggregates (means) (CONTROL)\", \"control_quick_aggregates_means.csv\", head_n=None)\n",
    "\n",
    "print_and_save(group_rmse_y1, \"Group RMSE (LGBM) - trip_cnt_per_user (CONTROL, all groups)\", \"control_group_rmse_lgbm_trip_all_groups.csv\", head_n=30)\n",
    "print_and_save(group_rmse_y2, \"Group RMSE (LGBM) - nonrepeat_cnt_per_user (CONTROL, all groups)\", \"control_group_rmse_lgbm_nonrepeat_all_groups.csv\", head_n=30)\n",
    "\n",
    "print_and_save(best15_y1, \"Best 15 group RMSE (LGBM) - trip_cnt_per_user (CONTROL)\", \"control_best15_group_rmse_lgbm_trip.csv\", head_n=None)\n",
    "print_and_save(worst15_y1, \"Worst 15 group RMSE (LGBM) - trip_cnt_per_user (CONTROL)\", \"control_worst15_group_rmse_lgbm_trip.csv\", head_n=None)\n",
    "print_and_save(best15_y2, \"Best 15 group RMSE (LGBM) - nonrepeat_cnt_per_user (CONTROL)\", \"control_best15_group_rmse_lgbm_nonrepeat.csv\", head_n=None)\n",
    "print_and_save(worst15_y2, \"Worst 15 group RMSE (LGBM) - nonrepeat_cnt_per_user (CONTROL)\", \"control_worst15_group_rmse_lgbm_nonrepeat.csv\", head_n=None)\n",
    "\n",
    "params_log = pd.DataFrame([\n",
    "    {\"target\": Y1, \"params\": str({**base_params, **params_y1})},\n",
    "    {\"target\": Y2, \"params\": str({**base_params, **params_y2})},\n",
    "    {\"note\": \"categorical_feature used\", \"value\": str(cat_features)},\n",
    "    {\"note\": \"object_cols auto-cast to category\", \"value\": str(obj_cols)},\n",
    "])\n",
    "print_and_save(params_log, \"Final tuned model params (CONTROL)\", \"control_final_model_params.csv\", head_n=None)\n",
    "\n",
    "print(f\"\\n✅ Saved all outputs to: {os.path.abspath(OUTPUT_DIR)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faa1d36",
   "metadata": {},
   "source": [
    "# compare lgbm and baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "473ba971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "BEST model by group (target=trip_cnt_per_user, criterion=RMSE) | split by city_group\n",
      "==========================================================================================\n",
      "\n",
      "[Overall winner counts]\n",
      "best_model  n_groups   pct\n",
      "      lgbm        10 41.67\n",
      "     roll4         7 29.17\n",
      "      lag1         7 29.17\n",
      "\n",
      "[Winner counts + mean errors by city_group]\n",
      "city_group  total_groups  lag1  lgbm  roll4  lag1_pct  lgbm_pct  roll4_pct  lag1_RMSE_mean  lag1_MAE_mean  roll4_RMSE_mean  roll4_MAE_mean  lgbm_RMSE_mean  lgbm_MAE_mean\n",
      "        中區             8     1     3      4      12.5      37.5       50.0        0.024562       0.024562         0.018999        0.018999        0.021178       0.021178\n",
      "        北區             8     2     4      2      25.0      50.0       25.0        0.027227       0.027227         0.028028        0.028028        0.021416       0.021416\n",
      "        南區             8     4     3      1      50.0      37.5       12.5        0.019846       0.019846         0.019701        0.019701        0.021785       0.021785\n",
      "\n",
      "==========================================================================================\n",
      "BEST model by group (target=nonrepeat_cnt_per_user, criterion=RMSE) | split by city_group\n",
      "==========================================================================================\n",
      "\n",
      "[Overall winner counts]\n",
      "best_model  n_groups  pct\n",
      "      lgbm        15 62.5\n",
      "     roll4         6 25.0\n",
      "      lag1         3 12.5\n",
      "\n",
      "[Winner counts + mean errors by city_group]\n",
      "city_group  total_groups  lag1  lgbm  roll4  lag1_pct  lgbm_pct  roll4_pct  lag1_RMSE_mean  lag1_MAE_mean  roll4_RMSE_mean  roll4_MAE_mean  lgbm_RMSE_mean  lgbm_MAE_mean\n",
      "        中區             8     0     5      3       0.0      62.5       37.5        0.042349       0.042349         0.036780        0.036780        0.027183       0.027183\n",
      "        北區             8     0     8      0       0.0     100.0        0.0        0.072779       0.072779         0.060654        0.060654        0.032501       0.032501\n",
      "        南區             8     3     2      3      37.5      25.0       37.5        0.036835       0.036835         0.033027        0.033027        0.030874       0.030874\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "GROUP_KEY_COLS = [\"treatment\", \"source\", \"ops_type_merged\", \"city_group\"]\n",
    "\n",
    "def _rmse(a, b):\n",
    "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
    "    return float(np.sqrt(np.mean((a - b) ** 2)))\n",
    "\n",
    "def _mae(a, b):\n",
    "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
    "    return float(np.mean(np.abs(a - b)))\n",
    "\n",
    "def best_model_by_group(\n",
    "    df, y_col, pred_cols, group_cols,\n",
    "    metric=\"RMSE\",\n",
    "    tie_rtol=1e-6, tie_atol=1e-9,\n",
    "):\n",
    "    \"\"\"\n",
    "    Per-group winner table.\n",
    "    - Drops rows with NaN in y or any pred col (per group) to avoid NaN metrics.\n",
    "    - tie_rtol / tie_atol controls \"close enough\" tie.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    need_cols = [y_col] + list(pred_cols.values()) + group_cols\n",
    "\n",
    "    # safety: only keep required columns to save memory\n",
    "    d = df[need_cols].copy()\n",
    "\n",
    "    for key, g in d.groupby(group_cols, dropna=False):\n",
    "        # drop NaN rows (important!)\n",
    "        g2 = g.dropna(subset=[y_col] + list(pred_cols.values()))\n",
    "        if g2.empty:\n",
    "            continue\n",
    "\n",
    "        y = g2[y_col].astype(float).values\n",
    "        out = {col: val for col, val in zip(group_cols, key)}\n",
    "        out[\"target\"] = y_col\n",
    "        out[\"n\"] = len(g2)\n",
    "        out[\"y_true_mean\"] = float(np.mean(y))\n",
    "\n",
    "        errs = {}\n",
    "        for m, pcol in pred_cols.items():\n",
    "            p = g2[pcol].astype(float).values\n",
    "            out[f\"{m}_pred_mean\"] = float(np.mean(p))\n",
    "            out[f\"{m}_RMSE\"] = _rmse(y, p)\n",
    "            out[f\"{m}_MAE\"]  = _mae(y, p)\n",
    "            errs[m] = out[f\"{m}_{metric}\"]\n",
    "\n",
    "        min_err = min(errs.values())\n",
    "        winners = [m for m, e in errs.items() if np.isclose(e, min_err, rtol=tie_rtol, atol=tie_atol)]\n",
    "\n",
    "        out[\"best_metric\"] = metric\n",
    "        out[\"best_error\"] = float(min_err)\n",
    "        out[\"best_model\"] = winners[0] if len(winners) == 1 else \"TIE:\" + \",\".join(sorted(winners))\n",
    "        rows.append(out)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def summary_by_city(win_df, pred_cols):\n",
    "    \"\"\"\n",
    "    Summary by city_group:\n",
    "    - total_groups\n",
    "    - winner counts + pct\n",
    "    - mean RMSE/MAE per method across groups\n",
    "    \"\"\"\n",
    "    if win_df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # winner counts\n",
    "    counts = win_df.groupby([\"city_group\", \"best_model\"]).size().reset_index(name=\"n_groups\")\n",
    "    totals = win_df.groupby(\"city_group\").size().reset_index(name=\"total_groups\")\n",
    "    counts = counts.merge(totals, on=\"city_group\", how=\"left\")\n",
    "    counts[\"pct\"] = (counts[\"n_groups\"] / counts[\"total_groups\"] * 100).round(2)\n",
    "\n",
    "    counts_pivot = (\n",
    "        counts.pivot(index=\"city_group\", columns=\"best_model\", values=\"n_groups\")\n",
    "        .fillna(0).astype(int).reset_index()\n",
    "    )\n",
    "    pct_pivot = (\n",
    "        counts.pivot(index=\"city_group\", columns=\"best_model\", values=\"pct\")\n",
    "        .fillna(0.0).reset_index()\n",
    "    )\n",
    "    pct_cols = [c for c in pct_pivot.columns if c != \"city_group\"]\n",
    "    pct_pivot = pct_pivot.rename(columns={c: f\"{c}_pct\" for c in pct_cols})\n",
    "\n",
    "    win_summary = counts_pivot.merge(pct_pivot, on=\"city_group\", how=\"left\")\n",
    "    win_summary = win_summary.merge(totals, on=\"city_group\", how=\"left\")\n",
    "\n",
    "    # mean errors by city\n",
    "    agg_rows = []\n",
    "    for city, g in win_df.groupby(\"city_group\"):\n",
    "        row = {\"city_group\": city, \"total_groups\": len(g)}\n",
    "        for m in pred_cols.keys():\n",
    "            row[f\"{m}_RMSE_mean\"] = float(g[f\"{m}_RMSE\"].mean())\n",
    "            row[f\"{m}_MAE_mean\"]  = float(g[f\"{m}_MAE\"].mean())\n",
    "        agg_rows.append(row)\n",
    "    err_summary = pd.DataFrame(agg_rows)\n",
    "\n",
    "    out = win_summary.merge(err_summary, on=[\"city_group\", \"total_groups\"], how=\"left\")\n",
    "    base_cols = [\"city_group\", \"total_groups\"]\n",
    "    out = out[base_cols + [c for c in out.columns if c not in base_cols]].sort_values(\"city_group\").reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def print_city_summary(win_df, pred_cols, title):\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(title)\n",
    "    print(\"=\"*90)\n",
    "\n",
    "    overall = win_df[\"best_model\"].value_counts().rename_axis(\"best_model\").reset_index(name=\"n_groups\")\n",
    "    overall[\"pct\"] = (overall[\"n_groups\"] / overall[\"n_groups\"].sum() * 100).round(2)\n",
    "    print(\"\\n[Overall winner counts]\")\n",
    "    print(overall.to_string(index=False))\n",
    "\n",
    "    city_tbl = summary_by_city(win_df, pred_cols)\n",
    "    print(\"\\n[Winner counts + mean errors by city_group]\")\n",
    "    print(city_tbl.to_string(index=False))\n",
    "\n",
    "# ---- maps ----\n",
    "pred_map_y1 = {\"lag1\": \"pred_y1_lag1\", \"roll4\": \"pred_y1_roll4\", \"lgbm\": \"pred_y1_lgbm\"}\n",
    "pred_map_y2 = {\"lag1\": \"pred_y2_lag1\", \"roll4\": \"pred_y2_roll4\", \"lgbm\": \"pred_y2_lgbm\"}\n",
    "\n",
    "# ---- compute winners ----\n",
    "win_y1 = best_model_by_group(test, Y1, pred_map_y1, GROUP_KEY_COLS, metric=\"RMSE\")\n",
    "win_y2 = best_model_by_group(test, Y2, pred_map_y2, GROUP_KEY_COLS, metric=\"RMSE\")\n",
    "\n",
    "# ---- print summary ----\n",
    "print_city_summary(win_y1, pred_map_y1, f\"BEST model by group (target={Y1}, criterion=RMSE) | split by city_group\")\n",
    "print_city_summary(win_y2, pred_map_y2, f\"BEST model by group (target={Y2}, criterion=RMSE) | split by city_group\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bccb2b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "[CONTROL] BEST model by group (target=trip_cnt_per_user, criterion=RMSE) | split by city_group\n",
      "==========================================================================================\n",
      "\n",
      "[Overall winner counts]\n",
      "best_model  n_groups   pct\n",
      "      lgbm        10 41.67\n",
      "     roll4         7 29.17\n",
      "      lag1         7 29.17\n",
      "\n",
      "[Winner counts + mean errors by city_group]\n",
      "city_group  total_groups  lag1  lgbm  roll4  lag1_pct  lgbm_pct  roll4_pct  lag1_RMSE_mean  lag1_MAE_mean  roll4_RMSE_mean  roll4_MAE_mean  lgbm_RMSE_mean  lgbm_MAE_mean\n",
      "        中區             8     1     3      4      12.5      37.5       50.0        0.024562       0.024562         0.018999        0.018999        0.021178       0.021178\n",
      "        北區             8     2     4      2      25.0      50.0       25.0        0.027227       0.027227         0.028028        0.028028        0.021416       0.021416\n",
      "        南區             8     4     3      1      50.0      37.5       12.5        0.019846       0.019846         0.019701        0.019701        0.021785       0.021785\n",
      "\n",
      "==========================================================================================\n",
      "[CONTROL] BEST model by group (target=nonrepeat_cnt_per_user, criterion=RMSE) | split by city_group\n",
      "==========================================================================================\n",
      "\n",
      "[Overall winner counts]\n",
      "best_model  n_groups  pct\n",
      "      lgbm        15 62.5\n",
      "     roll4         6 25.0\n",
      "      lag1         3 12.5\n",
      "\n",
      "[Winner counts + mean errors by city_group]\n",
      "city_group  total_groups  lag1  lgbm  roll4  lag1_pct  lgbm_pct  roll4_pct  lag1_RMSE_mean  lag1_MAE_mean  roll4_RMSE_mean  roll4_MAE_mean  lgbm_RMSE_mean  lgbm_MAE_mean\n",
      "        中區             8     0     5      3       0.0      62.5       37.5        0.042349       0.042349         0.036780        0.036780        0.027183       0.027183\n",
      "        北區             8     0     8      0       0.0     100.0        0.0        0.072779       0.072779         0.060654        0.060654        0.032501       0.032501\n",
      "        南區             8     3     2      3      37.5      25.0       37.5        0.036835       0.036835         0.033027        0.033027        0.030874       0.030874\n",
      "Saved: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\control_winner_by_city_trip_cnt_per_user.csv\n",
      "Saved: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\control_winner_by_city_nonrepeat_cnt_per_user.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'..\\\\..\\\\result\\\\control\\\\control_winner_by_city_nonrepeat_cnt_per_user.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ==========================\n",
    "# CONTROL — Save winner summary by city_group (RMSE)  (one file per target)\n",
    "# - Will auto-fix column names: city_group/city, ops_type_merged/ops_type\n",
    "# ==========================\n",
    "\n",
    "OUTPUT_DIR = os.path.join(\"..\", \"..\", \"result\", \"control\")\n",
    "ENCODING_EXCEL = \"utf-8-sig\"\n",
    "\n",
    "# --------------------------\n",
    "# Column normalization helpers\n",
    "# --------------------------\n",
    "def pick_col(df, candidates, required=True, label=\"\"):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    if required:\n",
    "        raise KeyError(f\"Missing required column for {label}. Tried: {candidates}\\n\"\n",
    "                       f\"Existing columns (sample): {list(df.columns)[:80]}\")\n",
    "    return None\n",
    "\n",
    "def normalize_columns(df):\n",
    "    \"\"\"\n",
    "    Return a copy with normalized column names:\n",
    "    treatment, source, ops_type_merged, city_group\n",
    "    \"\"\"\n",
    "    d = df.copy()\n",
    "\n",
    "    col_treat = pick_col(d, [\"treatment\"], label=\"treatment\")\n",
    "    col_src   = pick_col(d, [\"source\"], label=\"source\")\n",
    "\n",
    "    col_ops   = pick_col(d, [\"ops_type_merged\", \"ops_type\"], label=\"ops_type_merged/ops_type\")\n",
    "    col_city  = pick_col(d, [\"city_group\", \"city\"], label=\"city_group/city\")\n",
    "\n",
    "    rename_map = {}\n",
    "    if col_ops != \"ops_type_merged\":\n",
    "        rename_map[col_ops] = \"ops_type_merged\"\n",
    "    if col_city != \"city_group\":\n",
    "        rename_map[col_city] = \"city_group\"\n",
    "    # treatment/source usually already correct, but keep safe\n",
    "    if col_treat != \"treatment\":\n",
    "        rename_map[col_treat] = \"treatment\"\n",
    "    if col_src != \"source\":\n",
    "        rename_map[col_src] = \"source\"\n",
    "\n",
    "    if rename_map:\n",
    "        d = d.rename(columns=rename_map)\n",
    "\n",
    "    return d\n",
    "\n",
    "# --------------------------\n",
    "# Metrics\n",
    "# --------------------------\n",
    "def _rmse(a, b):\n",
    "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
    "    return float(np.sqrt(np.mean((a - b) ** 2)))\n",
    "\n",
    "def _mae(a, b):\n",
    "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
    "    return float(np.mean(np.abs(a - b)))\n",
    "\n",
    "def best_model_by_group(\n",
    "    df, y_col, pred_cols, group_cols,\n",
    "    metric=\"RMSE\",\n",
    "    tie_rtol=1e-6, tie_atol=1e-9,\n",
    "):\n",
    "    \"\"\"\n",
    "    Per-group winner table.\n",
    "    - Drops rows with NaN in y or any pred col (per group) to avoid NaN metrics.\n",
    "    - tie_rtol / tie_atol controls \"close enough\" tie.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    need_cols = [y_col] + list(pred_cols.values()) + group_cols\n",
    "\n",
    "    d = df[need_cols].copy()\n",
    "\n",
    "    for key, g in d.groupby(group_cols, dropna=False):\n",
    "        g2 = g.dropna(subset=[y_col] + list(pred_cols.values()))\n",
    "        if g2.empty:\n",
    "            continue\n",
    "\n",
    "        y = g2[y_col].astype(float).values\n",
    "        out = {col: val for col, val in zip(group_cols, key)}\n",
    "        out[\"target\"] = y_col\n",
    "        out[\"n\"] = len(g2)\n",
    "        out[\"y_true_mean\"] = float(np.mean(y))\n",
    "\n",
    "        errs = {}\n",
    "        for m, pcol in pred_cols.items():\n",
    "            p = g2[pcol].astype(float).values\n",
    "            out[f\"{m}_pred_mean\"] = float(np.mean(p))\n",
    "            out[f\"{m}_RMSE\"] = _rmse(y, p)\n",
    "            out[f\"{m}_MAE\"]  = _mae(y, p)\n",
    "            errs[m] = out[f\"{m}_{metric}\"]\n",
    "\n",
    "        min_err = min(errs.values())\n",
    "        winners = [m for m, e in errs.items() if np.isclose(e, min_err, rtol=tie_rtol, atol=tie_atol)]\n",
    "\n",
    "        out[\"best_metric\"] = metric\n",
    "        out[\"best_error\"] = float(min_err)\n",
    "        out[\"best_model\"] = winners[0] if len(winners) == 1 else \"TIE:\" + \",\".join(sorted(winners))\n",
    "        rows.append(out)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def summary_by_city(win_df, pred_cols):\n",
    "    if win_df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    counts = win_df.groupby([\"city_group\", \"best_model\"]).size().reset_index(name=\"n_groups\")\n",
    "    totals = win_df.groupby(\"city_group\").size().reset_index(name=\"total_groups\")\n",
    "    counts = counts.merge(totals, on=\"city_group\", how=\"left\")\n",
    "    counts[\"pct\"] = (counts[\"n_groups\"] / counts[\"total_groups\"] * 100).round(2)\n",
    "\n",
    "    counts_pivot = (\n",
    "        counts.pivot(index=\"city_group\", columns=\"best_model\", values=\"n_groups\")\n",
    "        .fillna(0).astype(int).reset_index()\n",
    "    )\n",
    "    pct_pivot = (\n",
    "        counts.pivot(index=\"city_group\", columns=\"best_model\", values=\"pct\")\n",
    "        .fillna(0.0).reset_index()\n",
    "    )\n",
    "    pct_cols = [c for c in pct_pivot.columns if c != \"city_group\"]\n",
    "    pct_pivot = pct_pivot.rename(columns={c: f\"{c}_pct\" for c in pct_cols})\n",
    "\n",
    "    win_summary = counts_pivot.merge(pct_pivot, on=\"city_group\", how=\"left\")\n",
    "    win_summary = win_summary.merge(totals, on=\"city_group\", how=\"left\")\n",
    "\n",
    "    agg_rows = []\n",
    "    for city, g in win_df.groupby(\"city_group\"):\n",
    "        row = {\"city_group\": city, \"total_groups\": len(g)}\n",
    "        for m in pred_cols.keys():\n",
    "            row[f\"{m}_RMSE_mean\"] = float(g[f\"{m}_RMSE\"].mean())\n",
    "            row[f\"{m}_MAE_mean\"]  = float(g[f\"{m}_MAE\"].mean())\n",
    "        agg_rows.append(row)\n",
    "    err_summary = pd.DataFrame(agg_rows)\n",
    "\n",
    "    out = win_summary.merge(err_summary, on=[\"city_group\", \"total_groups\"], how=\"left\")\n",
    "    base_cols = [\"city_group\", \"total_groups\"]\n",
    "    out = out[base_cols + [c for c in out.columns if c not in base_cols]].sort_values(\"city_group\").reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def print_city_summary(win_df, pred_cols, title):\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(title)\n",
    "    print(\"=\"*90)\n",
    "\n",
    "    overall = win_df[\"best_model\"].value_counts().rename_axis(\"best_model\").reset_index(name=\"n_groups\")\n",
    "    overall[\"pct\"] = (overall[\"n_groups\"] / overall[\"n_groups\"].sum() * 100).round(2)\n",
    "    print(\"\\n[Overall winner counts]\")\n",
    "    print(overall.to_string(index=False))\n",
    "\n",
    "    city_tbl = summary_by_city(win_df, pred_cols)\n",
    "    print(\"\\n[Winner counts + mean errors by city_group]\")\n",
    "    print(city_tbl.to_string(index=False))\n",
    "\n",
    "def save_csv(df_, filename):\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    path = os.path.join(OUTPUT_DIR, filename)\n",
    "    df_.to_csv(path, index=False, encoding=ENCODING_EXCEL)\n",
    "    print(\"Saved:\", os.path.abspath(path))\n",
    "    return path\n",
    "\n",
    "# ==========================\n",
    "# Run (CONTROL)\n",
    "# ==========================\n",
    "\n",
    "# 1) normalize column names (city_group / ops_type_merged)\n",
    "test2 = normalize_columns(test)\n",
    "\n",
    "GROUP_KEY_COLS = [\"treatment\", \"source\", \"ops_type_merged\", \"city_group\"]\n",
    "\n",
    "# 2) check prediction cols exist\n",
    "need_pred_cols = [\n",
    "    \"pred_y1_lag1\", \"pred_y1_roll4\", \"pred_y1_lgbm\",\n",
    "    \"pred_y2_lag1\", \"pred_y2_roll4\", \"pred_y2_lgbm\",\n",
    "]\n",
    "missing_preds = [c for c in need_pred_cols if c not in test2.columns]\n",
    "if missing_preds:\n",
    "    raise KeyError(\n",
    "        f\"Missing prediction columns in `test`:\\n{missing_preds}\\n\\n\"\n",
    "        f\"Existing columns (sample): {list(test2.columns)[:80]}\\n\"\n",
    "        f\"👉 If your pred col names are different, rename them to the above names first.\"\n",
    "    )\n",
    "\n",
    "# 3) maps\n",
    "pred_map_y1 = {\"lag1\": \"pred_y1_lag1\", \"roll4\": \"pred_y1_roll4\", \"lgbm\": \"pred_y1_lgbm\"}\n",
    "pred_map_y2 = {\"lag1\": \"pred_y2_lag1\", \"roll4\": \"pred_y2_roll4\", \"lgbm\": \"pred_y2_lgbm\"}\n",
    "\n",
    "# 4) compute winners + summary\n",
    "win_y1 = best_model_by_group(test2, Y1, pred_map_y1, GROUP_KEY_COLS, metric=\"RMSE\")\n",
    "win_y2 = best_model_by_group(test2, Y2, pred_map_y2, GROUP_KEY_COLS, metric=\"RMSE\")\n",
    "\n",
    "sum_y1 = summary_by_city(win_y1, pred_map_y1)\n",
    "sum_y2 = summary_by_city(win_y2, pred_map_y2)\n",
    "\n",
    "# 5) print + save (one file per target)\n",
    "print_city_summary(win_y1, pred_map_y1, f\"[CONTROL] BEST model by group (target={Y1}, criterion=RMSE) | split by city_group\")\n",
    "print_city_summary(win_y2, pred_map_y2, f\"[CONTROL] BEST model by group (target={Y2}, criterion=RMSE) | split by city_group\")\n",
    "\n",
    "save_csv(sum_y1, \"control_winner_by_city_trip_cnt_per_user.csv\")\n",
    "save_csv(sum_y2, \"control_winner_by_city_nonrepeat_cnt_per_user.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadc9a2e",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a4842f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\minhsiang.chang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SHAP for CONTROL | Y1 (trip_cnt_per_user) =====\n",
      "[SHAP] Saved importance: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\SHAP\\control_y1_trip_shap_importance_mean_abs.csv\n",
      "[SHAP] Saved shap values: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\SHAP\\control_y1_trip_shap_values.csv\n",
      "[SHAP] Saved beeswarm plot: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\SHAP\\control_y1_trip_shap_summary_beeswarm.png\n",
      "[SHAP] Saved bar plot: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\SHAP\\control_y1_trip_shap_summary_bar.png\n",
      "\n",
      "===== SHAP for CONTROL | Y2 (nonrepeat_cnt_per_user) =====\n",
      "[SHAP] Saved importance: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\SHAP\\control_y2_nonrepeat_shap_importance_mean_abs.csv\n",
      "[SHAP] Saved shap values: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\SHAP\\control_y2_nonrepeat_shap_values.csv\n",
      "[SHAP] Saved beeswarm plot: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\SHAP\\control_y2_nonrepeat_shap_summary_beeswarm.png\n",
      "[SHAP] Saved bar plot: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\SHAP\\control_y2_nonrepeat_shap_summary_bar.png\n",
      "\n",
      "[SHAP] Top 20 features by mean(|SHAP|) - CONTROL Y1:\n",
      "                                 feature  mean_abs_shap\n",
      "                         ops_type_merged       0.012765\n",
      "                 trip_cnt_per_user_roll4       0.009536\n",
      "    weekday_nonrepeat_cnt_per_user_roll4       0.008055\n",
      "          weekend_trip_cnt_per_user_lag1       0.004045\n",
      "         weekday_trip_cnt_per_user_roll4       0.003218\n",
      "log_coupon_register_total_per_user_roll4       0.002242\n",
      "                              city_group       0.002184\n",
      "      log_coupon_MGM_total_per_user_lag2       0.002103\n",
      "    weekend_nonrepeat_cnt_per_user_roll4       0.001814\n",
      "                           avg_rainy_day       0.001561\n",
      "            nonrepeat_cnt_per_user_roll4       0.001555\n",
      "       log_coupon_BD_total_per_user_lag2       0.001491\n",
      "                       avg_rainy_weekday       0.001340\n",
      "      log_coupon_BD_total_per_user_roll4       0.001246\n",
      "     weekend_nonrepeat_cnt_per_user_lag2       0.001210\n",
      "      log_coupon_CDP_total_per_user_lag2       0.001081\n",
      "                weekend_match_rate_roll4       0.001011\n",
      " log_coupon_register_total_per_user_lag1       0.000973\n",
      " log_coupon_register_total_per_user_lag2       0.000715\n",
      "     log_coupon_CDP_total_per_user_roll4       0.000599\n",
      "\n",
      "[SHAP] Top 20 features by mean(|SHAP|) - CONTROL Y2:\n",
      "                                 feature  mean_abs_shap\n",
      "                         ops_type_merged       0.015444\n",
      "                 trip_cnt_per_user_roll4       0.013981\n",
      "    weekday_nonrepeat_cnt_per_user_roll4       0.006395\n",
      "            nonrepeat_cnt_per_user_roll4       0.006042\n",
      "         weekday_trip_cnt_per_user_roll4       0.003808\n",
      "                       avg_rainy_weekday       0.003797\n",
      "    weekend_nonrepeat_cnt_per_user_roll4       0.003114\n",
      "                              city_group       0.003036\n",
      "          weekend_trip_cnt_per_user_lag1       0.002190\n",
      "      log_coupon_BD_total_per_user_roll4       0.001916\n",
      "     weekend_nonrepeat_cnt_per_user_lag1       0.001767\n",
      "      log_coupon_MGM_total_per_user_lag2       0.001655\n",
      "       log_coupon_BD_total_per_user_lag2       0.001569\n",
      "                       avg_rainy_weekend       0.001559\n",
      "                delta_nonrepeat_per_user       0.001330\n",
      "      log_coupon_CDP_total_per_user_lag2       0.000991\n",
      "log_coupon_register_total_per_user_roll4       0.000981\n",
      "     log_coupon_MGM_total_per_user_roll4       0.000962\n",
      "      log_coupon_MGM_total_per_user_lag1       0.000951\n",
      "                           avg_rainy_day       0.000931\n",
      "\n",
      "✅ SHAP outputs saved to: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\SHAP\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# SHAP section (CONTROL) — add after predictions/models are ready\n",
    "# Assumes you already have:\n",
    "#   - m_y1, m_y2 (trained LGBM models for control)\n",
    "#   - X_train, X_test (feature matrices for control)\n",
    "#   - feature_cols (list of feature names)\n",
    "# ==========================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OUTPUT_DIR = os.path.join(\"..\", \"..\", \"result\", \"control\", \"SHAP\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---- optional: if you had font issues before ----\n",
    "# plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "# plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "def shap_run_and_save(\n",
    "    model,\n",
    "    X_ref,                 # background/reference dataset (use train sample)\n",
    "    X_explain,             # dataset to explain (use test week sample or full)\n",
    "    feature_names,\n",
    "    out_prefix,\n",
    "    output_dir,\n",
    "    max_explain_rows=2000, # limit for explanation set\n",
    "    max_ref_rows=3000      # limit for background\n",
    "):\n",
    "    \"\"\"\n",
    "    Saves:\n",
    "      1) SHAP mean(|value|) importance csv\n",
    "      2) SHAP values per row csv\n",
    "      3) summary beeswarm png\n",
    "      4) summary bar png\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # sample to control runtime\n",
    "    X_ref_s = X_ref.sample(max_ref_rows, random_state=42) if len(X_ref) > max_ref_rows else X_ref\n",
    "    X_exp_s = X_explain.sample(max_explain_rows, random_state=42) if len(X_explain) > max_explain_rows else X_explain\n",
    "\n",
    "    # TreeExplainer (LightGBM)\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "\n",
    "    # compute SHAP values (regression => (n_rows, n_features))\n",
    "    shap_values = explainer.shap_values(X_exp_s)\n",
    "\n",
    "    # ---- 1) Global importance: mean(|SHAP|) ----\n",
    "    mean_abs = np.abs(shap_values).mean(axis=0)\n",
    "    imp_df = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"mean_abs_shap\": mean_abs\n",
    "    }).sort_values(\"mean_abs_shap\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    imp_path = os.path.join(output_dir, f\"{out_prefix}_shap_importance_mean_abs.csv\")\n",
    "    imp_df.to_csv(imp_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[SHAP] Saved importance: {os.path.abspath(imp_path)}\")\n",
    "\n",
    "    # ---- 2) Per-row SHAP values ----\n",
    "    shap_df = pd.DataFrame(shap_values, columns=feature_names)\n",
    "    shap_df.insert(0, \"row_id\", X_exp_s.index.astype(str))\n",
    "\n",
    "    shap_val_path = os.path.join(output_dir, f\"{out_prefix}_shap_values.csv\")\n",
    "    shap_df.to_csv(shap_val_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[SHAP] Saved shap values: {os.path.abspath(shap_val_path)}\")\n",
    "\n",
    "    # ---- 3) Plots ----\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values, X_exp_s, feature_names=feature_names, show=False)\n",
    "    fig_path = os.path.join(output_dir, f\"{out_prefix}_shap_summary_beeswarm.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"[SHAP] Saved beeswarm plot: {os.path.abspath(fig_path)}\")\n",
    "\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values, X_exp_s, feature_names=feature_names, plot_type=\"bar\", show=False)\n",
    "    fig_path2 = os.path.join(output_dir, f\"{out_prefix}_shap_summary_bar.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_path2, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"[SHAP] Saved bar plot: {os.path.abspath(fig_path2)}\")\n",
    "\n",
    "    return imp_df\n",
    "\n",
    "# ---- choose what to explain ----\n",
    "# Background: train, Explain: target week test\n",
    "X_ref_y1 = X_train\n",
    "X_ref_y2 = X_train\n",
    "\n",
    "X_explain_y1 = X_test\n",
    "X_explain_y2 = X_test\n",
    "\n",
    "# ---- run ----\n",
    "print(\"\\n===== SHAP for CONTROL | Y1 (trip_cnt_per_user) =====\")\n",
    "imp_y1 = shap_run_and_save(\n",
    "    model=m_y1,\n",
    "    X_ref=X_ref_y1,\n",
    "    X_explain=X_explain_y1,\n",
    "    feature_names=feature_cols,\n",
    "    out_prefix=\"control_y1_trip\",\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_explain_rows=2000,\n",
    "    max_ref_rows=3000\n",
    ")\n",
    "\n",
    "print(\"\\n===== SHAP for CONTROL | Y2 (nonrepeat_cnt_per_user) =====\")\n",
    "imp_y2 = shap_run_and_save(\n",
    "    model=m_y2,\n",
    "    X_ref=X_ref_y2,\n",
    "    X_explain=X_explain_y2,\n",
    "    feature_names=feature_cols,\n",
    "    out_prefix=\"control_y2_nonrepeat\",\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_explain_rows=2000,\n",
    "    max_ref_rows=3000\n",
    ")\n",
    "\n",
    "# ---- print top 20 to cell for quick inspection ----\n",
    "print(\"\\n[SHAP] Top 20 features by mean(|SHAP|) - CONTROL Y1:\")\n",
    "print(imp_y1.head(20).to_string(index=False))\n",
    "\n",
    "print(\"\\n[SHAP] Top 20 features by mean(|SHAP|) - CONTROL Y2:\")\n",
    "print(imp_y2.head(20).to_string(index=False))\n",
    "\n",
    "print(f\"\\n✅ SHAP outputs saved to: {os.path.abspath(OUTPUT_DIR)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bffc36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONTROL CAT-MEAN SHAP] Saved: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\SHAP\\control_y1_trip_catmean_ops_city.csv\n",
      "[CONTROL CAT-MEAN SHAP] Saved: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\SHAP\\control_y2_nonrepeat_catmean_ops_city.csv\n",
      "\n",
      "Saved files:\n",
      "..\\..\\result\\control\\SHAP\\control_y1_trip_catmean_ops_city.csv\n",
      "..\\..\\result\\control\\SHAP\\control_y2_nonrepeat_catmean_ops_city.csv\n",
      "\n",
      "[CONTROL Y1] ops_type_merged top 10 push up:\n",
      "    cat_feature  category_value  n  mean_shap  mean_abs_shap\n",
      "ops_type_merged     既有regular鞏固  3   0.025481       0.025481\n",
      "ops_type_merged    14天在其他尖峰預估車資  3   0.019059       0.019059\n",
      "ops_type_merged     14天在晚尖峰預估車資  3   0.007336       0.007336\n",
      "ops_type_merged 養成Regular-高優惠敏感  3  -0.000731       0.000731\n",
      "ops_type_merged        喚回-高優惠敏感  3  -0.006725       0.006725\n",
      "ops_type_merged    養成Regular-其他  3  -0.011642       0.011642\n",
      "ops_type_merged           喚回-其他  3  -0.013538       0.013538\n",
      "ops_type_merged      90天在尖峰預估車資  3  -0.017604       0.017604\n",
      "\n",
      "[CONTROL Y1] city_group top 10 push up:\n",
      "cat_feature category_value  n  mean_shap  mean_abs_shap\n",
      " city_group             北區  8   0.001687       0.001687\n",
      " city_group             中區  8   0.001662       0.001662\n",
      " city_group             南區  8  -0.003204       0.003204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minhsiang.chang\\AppData\\Local\\Temp\\ipykernel_26560\\1716207517.py:151: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  grp = merged.groupby(c_cat, dropna=False)[c_shap]\n",
      "C:\\Users\\minhsiang.chang\\AppData\\Local\\Temp\\ipykernel_26560\\1716207517.py:151: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  grp = merged.groupby(c_cat, dropna=False)[c_shap]\n",
      "C:\\Users\\minhsiang.chang\\AppData\\Local\\Temp\\ipykernel_26560\\1716207517.py:151: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  grp = merged.groupby(c_cat, dropna=False)[c_shap]\n",
      "C:\\Users\\minhsiang.chang\\AppData\\Local\\Temp\\ipykernel_26560\\1716207517.py:151: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  grp = merged.groupby(c_cat, dropna=False)[c_shap]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ==========================================\n",
    "# CONTROL: category mean SHAP for ops_type_merged & city_group\n",
    "# Output: ONE file per Y\n",
    "# ==========================================\n",
    "\n",
    "ENCODING_EXCEL = \"utf-8-sig\"\n",
    "\n",
    "# 你要的兩個切法\n",
    "CAT_COLS_NEED = [\"ops_type_merged\", \"city_group\"]\n",
    "\n",
    "# control 的 SHAP 資料夾 / 檔名（請確認你的 prefix）\n",
    "SHAP_DIR = os.path.join(\"..\", \"..\", \"result\", \"control\", \"SHAP\")\n",
    "y1_shap_values_csv = os.path.join(SHAP_DIR, \"control_y1_trip_shap_values.csv\")\n",
    "y2_shap_values_csv = os.path.join(SHAP_DIR, \"control_y2_nonrepeat_shap_values.csv\")\n",
    "\n",
    "def _ensure_row_id_from_index(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    row_id 一律用 index（對應你 shap_run_and_save 裡 row_id = X_exp_s.index.astype(str)）\n",
    "    千萬不要 reset_index() 變成 0..n-1，會對不起來。\n",
    "    \"\"\"\n",
    "    out = X.copy()\n",
    "    out[\"row_id\"] = out.index.astype(str)\n",
    "    return out\n",
    "\n",
    "def cat_mean_shap_ops_city_onefile_per_y(\n",
    "    X_explain: pd.DataFrame,\n",
    "    shap_values_csv_path: str,\n",
    "    output_dir: str,\n",
    "    out_prefix: str,\n",
    "    cat_cols: list = None,\n",
    "    encoding: str = ENCODING_EXCEL\n",
    "):\n",
    "    \"\"\"\n",
    "    輸出 ONE csv（針對 ONE Y）：\n",
    "      {out_prefix}_catmean_ops_city.csv\n",
    "\n",
    "    欄位：\n",
    "      cat_feature, category_value, n, mean_shap, mean_abs_shap\n",
    "    \"\"\"\n",
    "    if cat_cols is None:\n",
    "        cat_cols = [\"ops_type_merged\", \"city_group\"]\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # --- read shap values ---\n",
    "    shap_df = pd.read_csv(shap_values_csv_path, encoding=encoding)\n",
    "    if \"row_id\" not in shap_df.columns:\n",
    "        raise ValueError(\"shap_values_csv 必須包含 row_id 欄位（你 SHAP 程式會產生）\")\n",
    "    shap_df[\"row_id\"] = shap_df[\"row_id\"].astype(str)\n",
    "\n",
    "    # --- prepare category values with row_id from X_explain index ---\n",
    "    X_cat = _ensure_row_id_from_index(X_explain)\n",
    "    need_in_X = [\"row_id\"] + [c for c in cat_cols if c in X_cat.columns]\n",
    "    missing_cat = [c for c in cat_cols if c not in X_cat.columns]\n",
    "    if missing_cat:\n",
    "        print(f\"[WARN] X_explain missing cat cols, will skip: {missing_cat}\")\n",
    "\n",
    "    X_cat = X_cat[need_in_X].copy()\n",
    "    merged = shap_df.merge(X_cat, on=\"row_id\", how=\"left\")  # keep SHAP rows\n",
    "\n",
    "    if merged[[\"row_id\"]].shape[0] == 0:\n",
    "        raise ValueError(\"Merged rows = 0，請檢查 X_explain 的 index 是否與 shap row_id 相同\")\n",
    "\n",
    "    out_parts = []\n",
    "\n",
    "    for c in cat_cols:\n",
    "        if c not in merged.columns:\n",
    "            print(f\"[SKIP] {c}: category column not found in merged (X_explain 沒有這欄或 merge 失敗)\")\n",
    "            continue\n",
    "\n",
    "        # shap 值欄位：通常就是同名 feature（如果你用 LGBM categorical）\n",
    "        if c not in shap_df.columns:\n",
    "            # 如果你這裡遇到，代表 SHAP 檔裡沒有這個 feature 的欄位（可能被 one-hot / 沒放進 feature_cols）\n",
    "            print(f\"[SKIP] {c}: SHAP column '{c}' not found in shap_values.csv (可能 one-hot 或不在 feature_cols)\")\n",
    "            continue\n",
    "\n",
    "        grp = merged.groupby(c, dropna=False)[c]\n",
    "        # 上面這行只是為了拿 group key，實際用 shap col 聚合\n",
    "        grp_shap = merged.groupby(c, dropna=False)[c]  # placeholder\n",
    "\n",
    "        # 正確：以 shap 值欄位聚合\n",
    "        grp_shap = merged.groupby(c, dropna=False)[c].apply(lambda s: s)  # not used\n",
    "\n",
    "        g = merged.groupby(c, dropna=False)[c]  # not used\n",
    "\n",
    "        # 實作聚合（用 shap 欄位 c）\n",
    "        grp2 = merged.groupby(c, dropna=False)[c]  # not used\n",
    "\n",
    "        # 直接用 Series groupby on shap column\n",
    "        s = merged[c]  # category series\n",
    "        shap_s = merged[c]  # placeholder\n",
    "\n",
    "        # ---- compute table ----\n",
    "        by = merged.groupby(c, dropna=False)[c]  # placeholder\n",
    "        # 用同名 shap 欄位 c\n",
    "        by_shap = merged.groupby(c, dropna=False)[c]  # placeholder\n",
    "\n",
    "        # 其實 shap 欄位也是 merged[c]（同名），但為避免混淆這裡明確指定\n",
    "        by_shap = merged.groupby(c, dropna=False)[c]\n",
    "\n",
    "        # 上面會混到 category 自己…我們要的是 shap 值欄位：merged[c]（同名）但它被 category 覆蓋了\n",
    "        # 所以改成從 shap_df 帶進來的欄位取值：在 merged 裡同名欄位會是「category」，不是 shap\n",
    "        # 解法：merge 前先把 X_cat 的欄位加後綴，避免同名覆蓋\n",
    "    # ---- 重新做一次：避免同名覆蓋（最安全） ----\n",
    "\n",
    "def cat_mean_shap_ops_city_onefile_per_y_SAFE(\n",
    "    X_explain: pd.DataFrame,\n",
    "    shap_values_csv_path: str,\n",
    "    output_dir: str,\n",
    "    out_prefix: str,\n",
    "    cat_cols: list = None,\n",
    "    encoding: str = ENCODING_EXCEL\n",
    "):\n",
    "    if cat_cols is None:\n",
    "        cat_cols = [\"ops_type_merged\", \"city_group\"]\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    shap_df = pd.read_csv(shap_values_csv_path, encoding=encoding)\n",
    "    if \"row_id\" not in shap_df.columns:\n",
    "        raise ValueError(\"shap_values_csv 必須包含 row_id 欄位\")\n",
    "    shap_df[\"row_id\"] = shap_df[\"row_id\"].astype(str)\n",
    "\n",
    "    X_cat = _ensure_row_id_from_index(X_explain)\n",
    "    missing_cat = [c for c in cat_cols if c not in X_cat.columns]\n",
    "    if missing_cat:\n",
    "        print(f\"[WARN] X_explain missing cat cols, will skip: {missing_cat}\")\n",
    "\n",
    "    # 避免跟 shap_df 的同名欄位衝突：cat 欄位加 _cat\n",
    "    X_cat2 = X_cat[[\"row_id\"] + [c for c in cat_cols if c in X_cat.columns]].copy()\n",
    "    X_cat2 = X_cat2.rename(columns={c: f\"{c}_cat\" for c in cat_cols if c in X_cat2.columns})\n",
    "\n",
    "    merged = shap_df.merge(X_cat2, on=\"row_id\", how=\"left\")  # 保留 SHAP rows\n",
    "\n",
    "    out_parts = []\n",
    "    for c in cat_cols:\n",
    "        c_cat = f\"{c}_cat\"\n",
    "        c_shap = c\n",
    "\n",
    "        if c_cat not in merged.columns:\n",
    "            print(f\"[SKIP] {c}: category col not found after merge ({c_cat})\")\n",
    "            continue\n",
    "        if c_shap not in merged.columns:\n",
    "            print(f\"[SKIP] {c}: SHAP col not found in shap_values.csv ({c_shap})\")\n",
    "            continue\n",
    "\n",
    "        grp = merged.groupby(c_cat, dropna=False)[c_shap]\n",
    "        tbl = pd.DataFrame({\n",
    "            \"n\": grp.size(),\n",
    "            \"mean_shap\": grp.mean(),\n",
    "            \"mean_abs_shap\": grp.apply(lambda s: s.abs().mean()),\n",
    "        }).reset_index().rename(columns={c_cat: \"category_value\"})\n",
    "\n",
    "        tbl.insert(0, \"cat_feature\", c)\n",
    "        # 同一個 cat_feature 內：推高的排前面（mean_shap 大）\n",
    "        tbl = tbl.sort_values([\"cat_feature\", \"mean_shap\"], ascending=[True, False]).reset_index(drop=True)\n",
    "        out_parts.append(tbl)\n",
    "\n",
    "    if not out_parts:\n",
    "        raise ValueError(\"No output produced. 可能是 ops_type_merged/city_group 不在 SHAP feature 欄位中（或 one-hot）\")\n",
    "\n",
    "    out_df = pd.concat(out_parts, ignore_index=True)\n",
    "\n",
    "    out_path = os.path.join(output_dir, f\"{out_prefix}_catmean_ops_city.csv\")\n",
    "    out_df.to_csv(out_path, index=False, encoding=encoding)\n",
    "    print(f\"[CONTROL CAT-MEAN SHAP] Saved: {os.path.abspath(out_path)}\")\n",
    "    return out_df, out_path\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Example run (CONTROL)\n",
    "# ==========================\n",
    "# X_test 請用你 control pipeline 的 X_test（對應你 SHAP explain set 的 index）\n",
    "# 例如：X_explain_y1 = X_test；X_explain_y2 = X_test（跟你 random 一樣）\n",
    "\n",
    "# Y1\n",
    "df_y1, path_y1 = cat_mean_shap_ops_city_onefile_per_y_SAFE(\n",
    "    X_explain=X_test,\n",
    "    shap_values_csv_path=y1_shap_values_csv,\n",
    "    output_dir=SHAP_DIR,\n",
    "    out_prefix=\"control_y1_trip\",\n",
    "    cat_cols=CAT_COLS_NEED\n",
    ")\n",
    "\n",
    "# Y2\n",
    "df_y2, path_y2 = cat_mean_shap_ops_city_onefile_per_y_SAFE(\n",
    "    X_explain=X_test,\n",
    "    shap_values_csv_path=y2_shap_values_csv,\n",
    "    output_dir=SHAP_DIR,\n",
    "    out_prefix=\"control_y2_nonrepeat\",\n",
    "    cat_cols=CAT_COLS_NEED\n",
    ")\n",
    "\n",
    "print(\"\\nSaved files:\")\n",
    "print(path_y1)\n",
    "print(path_y2)\n",
    "\n",
    "# quick peek\n",
    "print(\"\\n[CONTROL Y1] ops_type_merged top 10 push up:\")\n",
    "print(df_y1[df_y1[\"cat_feature\"]==\"ops_type_merged\"].head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n[CONTROL Y1] city_group top 10 push up:\")\n",
    "print(df_y1[df_y1[\"cat_feature\"]==\"city_group\"].head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8083fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONTROL mean(SHAP)] Saved: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\SHAP\\control_y1_trip_shap_importance_mean_abs_and_signed.csv\n",
      "[CONTROL mean(SHAP)] Saved (signed desc): d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\SHAP\\control_y1_trip_shap_importance_mean_signed_desc.csv\n",
      "[CONTROL mean(SHAP)] Saved (signed asc): d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\SHAP\\control_y1_trip_shap_importance_mean_signed_asc.csv\n",
      "[CONTROL mean(SHAP)] Saved: d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\SHAP\\control_y2_nonrepeat_shap_importance_mean_abs_and_signed.csv\n",
      "[CONTROL mean(SHAP)] Saved (signed desc): d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\SHAP\\control_y2_nonrepeat_shap_importance_mean_signed_desc.csv\n",
      "[CONTROL mean(SHAP)] Saved (signed asc): d:\\minhsiang.chang\\Desktop\\2026winter_project\\result\\control\\SHAP\\control_y2_nonrepeat_shap_importance_mean_signed_asc.csv\n",
      "\n",
      "[CONTROL] Top 20 by mean(|SHAP|) with mean(SHAP) - Y1:\n",
      "                                 feature  mean_abs_shap  mean_shap\n",
      "                         ops_type_merged       0.012765   0.000205\n",
      "                 trip_cnt_per_user_roll4       0.009536  -0.000086\n",
      "    weekday_nonrepeat_cnt_per_user_roll4       0.008055   0.000303\n",
      "          weekend_trip_cnt_per_user_lag1       0.004045   0.001198\n",
      "         weekday_trip_cnt_per_user_roll4       0.003218   0.000507\n",
      "log_coupon_register_total_per_user_roll4       0.002242  -0.000171\n",
      "                              city_group       0.002184   0.000048\n",
      "      log_coupon_MGM_total_per_user_lag2       0.002103   0.002080\n",
      "    weekend_nonrepeat_cnt_per_user_roll4       0.001814   0.001018\n",
      "                           avg_rainy_day       0.001561  -0.001561\n",
      "            nonrepeat_cnt_per_user_roll4       0.001555   0.000150\n",
      "       log_coupon_BD_total_per_user_lag2       0.001491   0.000857\n",
      "                       avg_rainy_weekday       0.001340  -0.001340\n",
      "      log_coupon_BD_total_per_user_roll4       0.001246   0.000780\n",
      "     weekend_nonrepeat_cnt_per_user_lag2       0.001210  -0.000797\n",
      "      log_coupon_CDP_total_per_user_lag2       0.001081   0.001081\n",
      "                weekend_match_rate_roll4       0.001011  -0.000934\n",
      " log_coupon_register_total_per_user_lag1       0.000973  -0.000252\n",
      " log_coupon_register_total_per_user_lag2       0.000715  -0.000289\n",
      "     log_coupon_CDP_total_per_user_roll4       0.000599   0.000599\n",
      "\n",
      "[CONTROL] Top 20 by mean(|SHAP|) with mean(SHAP) - Y2:\n",
      "                                 feature  mean_abs_shap  mean_shap\n",
      "                         ops_type_merged       0.015444   0.000041\n",
      "                 trip_cnt_per_user_roll4       0.013981   0.001427\n",
      "    weekday_nonrepeat_cnt_per_user_roll4       0.006395   0.000001\n",
      "            nonrepeat_cnt_per_user_roll4       0.006042   0.001011\n",
      "         weekday_trip_cnt_per_user_roll4       0.003808   0.000579\n",
      "                       avg_rainy_weekday       0.003797  -0.003797\n",
      "    weekend_nonrepeat_cnt_per_user_roll4       0.003114   0.001814\n",
      "                              city_group       0.003036  -0.000026\n",
      "          weekend_trip_cnt_per_user_lag1       0.002190   0.001278\n",
      "      log_coupon_BD_total_per_user_roll4       0.001916   0.001130\n",
      "     weekend_nonrepeat_cnt_per_user_lag1       0.001767   0.000832\n",
      "      log_coupon_MGM_total_per_user_lag2       0.001655   0.001655\n",
      "       log_coupon_BD_total_per_user_lag2       0.001569   0.000971\n",
      "                       avg_rainy_weekend       0.001559  -0.001559\n",
      "                delta_nonrepeat_per_user       0.001330   0.000007\n",
      "      log_coupon_CDP_total_per_user_lag2       0.000991   0.000966\n",
      "log_coupon_register_total_per_user_roll4       0.000981   0.000240\n",
      "     log_coupon_MGM_total_per_user_roll4       0.000962  -0.000109\n",
      "      log_coupon_MGM_total_per_user_lag1       0.000951  -0.000850\n",
      "                           avg_rainy_day       0.000931  -0.000931\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ==========================\n",
    "# CONTROL: Compute mean(SHAP) from saved shap_values.csv\n",
    "# (NO change to your SHAP code; only read saved *_shap_values.csv)\n",
    "# ==========================\n",
    "\n",
    "def compute_mean_shap_from_shap_values_csv(\n",
    "    shap_values_csv_path: str,\n",
    "    output_dir: str = None,\n",
    "    out_prefix: str = None,\n",
    "    encoding: str = \"utf-8-sig\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      - shap_values_csv_path: path to \"{out_prefix}_shap_values.csv\"\n",
    "        (created by your shap_run_and_save; first col row_id, others are features)\n",
    "\n",
    "    Output files (saved to output_dir):\n",
    "      1) {out_prefix}_shap_importance_mean_abs_and_signed.csv\n",
    "         columns: feature, mean_abs_shap, mean_shap\n",
    "         sorted by mean_abs_shap desc\n",
    "      2) {out_prefix}_shap_importance_mean_signed_desc.csv\n",
    "         sorted by mean_shap desc  (push prediction up)\n",
    "      3) {out_prefix}_shap_importance_mean_signed_asc.csv\n",
    "         sorted by mean_shap asc   (pull prediction down)\n",
    "\n",
    "    Returns:\n",
    "      - importance_df: DataFrame with both mean_abs_shap and mean_shap\n",
    "    \"\"\"\n",
    "    if not os.path.exists(shap_values_csv_path):\n",
    "        raise FileNotFoundError(f\"SHAP values csv not found: {shap_values_csv_path}\")\n",
    "\n",
    "    df = pd.read_csv(shap_values_csv_path, encoding=encoding)\n",
    "\n",
    "    # Drop row_id if exists; keep only feature columns\n",
    "    shap_only = df.drop(columns=[\"row_id\"], errors=\"ignore\")\n",
    "\n",
    "    if shap_only.shape[1] == 0:\n",
    "        raise ValueError(\"No feature columns found in SHAP values csv (after dropping row_id).\")\n",
    "\n",
    "    # mean(SHAP) and mean(|SHAP|)\n",
    "    mean_signed = shap_only.mean(axis=0)     # Series indexed by feature\n",
    "    mean_abs = shap_only.abs().mean(axis=0)\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        \"feature\": shap_only.columns,\n",
    "        \"mean_abs_shap\": mean_abs.values,\n",
    "        \"mean_shap\": mean_signed.values\n",
    "    }).sort_values(\"mean_abs_shap\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Decide output_dir & out_prefix\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(shap_values_csv_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if out_prefix is None:\n",
    "        base = os.path.basename(shap_values_csv_path)\n",
    "        if base.endswith(\"_shap_values.csv\"):\n",
    "            out_prefix = base.replace(\"_shap_values.csv\", \"\")\n",
    "        else:\n",
    "            out_prefix = os.path.splitext(base)[0]\n",
    "\n",
    "    # (1) abs + signed together\n",
    "    out_path_all = os.path.join(output_dir, f\"{out_prefix}_shap_importance_mean_abs_and_signed.csv\")\n",
    "    importance_df.to_csv(out_path_all, index=False, encoding=encoding)\n",
    "    print(f\"[CONTROL mean(SHAP)] Saved: {os.path.abspath(out_path_all)}\")\n",
    "\n",
    "    # (2) signed desc (push up)\n",
    "    out_path_pos = os.path.join(output_dir, f\"{out_prefix}_shap_importance_mean_signed_desc.csv\")\n",
    "    importance_df[[\"feature\", \"mean_shap\"]].sort_values(\"mean_shap\", ascending=False).to_csv(\n",
    "        out_path_pos, index=False, encoding=encoding\n",
    "    )\n",
    "    print(f\"[CONTROL mean(SHAP)] Saved (signed desc): {os.path.abspath(out_path_pos)}\")\n",
    "\n",
    "    # (3) signed asc (pull down)\n",
    "    out_path_neg = os.path.join(output_dir, f\"{out_prefix}_shap_importance_mean_signed_asc.csv\")\n",
    "    importance_df[[\"feature\", \"mean_shap\"]].sort_values(\"mean_shap\", ascending=True).to_csv(\n",
    "        out_path_neg, index=False, encoding=encoding\n",
    "    )\n",
    "    print(f\"[CONTROL mean(SHAP)] Saved (signed asc): {os.path.abspath(out_path_neg)}\")\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# CONTROL: Example usage\n",
    "# ==========================\n",
    "\n",
    "# 1) 這裡請改成「控制組」SHAP 輸出資料夾\n",
    "#    例如你 SHAP code 用的是: ../../result/control/SHAP\n",
    "OUTPUT_DIR = os.path.join(\"..\", \"..\", \"result\", \"control\", \"SHAP\")\n",
    "\n",
    "# 2) 這裡的檔名要對到你控制組 shap_run_and_save 的 out_prefix\n",
    "#    你如果當初用 out_prefix=\"control_y1_trip\"，那它就會產生：\n",
    "#    control_y1_trip_shap_values.csv\n",
    "y1_shap_values_csv = os.path.join(OUTPUT_DIR, \"control_y1_trip_shap_values.csv\")\n",
    "y2_shap_values_csv = os.path.join(OUTPUT_DIR, \"control_y2_nonrepeat_shap_values.csv\")\n",
    "\n",
    "mean_df_y1 = compute_mean_shap_from_shap_values_csv(y1_shap_values_csv, output_dir=OUTPUT_DIR)\n",
    "mean_df_y2 = compute_mean_shap_from_shap_values_csv(y2_shap_values_csv, output_dir=OUTPUT_DIR)\n",
    "\n",
    "print(\"\\n[CONTROL] Top 20 by mean(|SHAP|) with mean(SHAP) - Y1:\")\n",
    "print(mean_df_y1.head(20).to_string(index=False))\n",
    "\n",
    "print(\"\\n[CONTROL] Top 20 by mean(|SHAP|) with mean(SHAP) - Y2:\")\n",
    "print(mean_df_y2.head(20).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
